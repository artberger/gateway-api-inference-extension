{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Gateway API Inference Extension is an official Kubernetes project focused on extending Gateway API with inference specific routing extensions.</p> <p>The overall resource model focuses on 2 new inference-focused personas and corresponding resources that they are expected to manage:</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<p>Gateway API Inference Extension, along with a reference implementation in Envoy Proxy, provides the following key features: </p> <ul> <li> <p>Model-aware routing: Instead of simply routing based on the path of the request, Gateway API Inference Extension allows you to route to models based on the model names. This is enabled by support for GenAI Inference API specifications (such as OpenAI API) in the gateway implementations such as in Envoy Proxy. This model-aware routing also extends to Low-Rank Adaptation (LoRA) fine-tuned models.</p> </li> <li> <p>Serving priority: Gateway API Inference Extension allows you to specify the serving priority of your models. For example, you can specify that your models for online inference of chat tasks (which is more latency sensitive) have a higher Criticality than a model for latency tolerant tasks such as a summarization. </p> </li> <li> <p>Model rollouts:  Gateway API Inference Extension allows you to incrementally roll out new model versions by traffic splitting definitions based on the model names. </p> </li> <li> <p>Extensibility for Inference Services: Gateway API Inference Extension defines extensibility pattern for additional Inference services to create bespoke routing capabilities should out of the box solutions not fit your needs.</p> </li> <li> <p>Customizable Load Balancing for Inference: Gateway API Inference Extension defines a pattern for customizable load balancing and request routing that is optimized for Inference. Gateway API Inference Extension provides a reference implementation of model endpoint picking leveraging metrics emitted from the model servers. This endpoint picking mechanism can be used in lieu of traditional load balancing mechanisms. Model Server-aware load balancing (\"smart\" load balancing as its sometimes referred to in this repo) has been proven to reduce the serving latency and improve utilization of accelerators in your clusters.</p> </li> </ul>"},{"location":"#api-resources","title":"API Resources","text":"<p>Head to our API overview to start exploring our APIs!</p>"},{"location":"#composable-layers","title":"Composable Layers","text":"<p>This project aims to define specifications to enable a compatible ecosystem for extending the Gateway API with custom endpoint selection algorithms. This project defines a set of patterns across three distinct layers of components that are relevant to this project:</p>"},{"location":"#gateway-api-implementations","title":"Gateway API Implementations","text":"<p>Gateway API has more than 25 implementations. As this pattern stabilizes, we expect a wide set of these implementations to support this project.</p>"},{"location":"#endpoint-selection-extension","title":"Endpoint Selection Extension","text":"<p>As part of this project, we're building an initial reference extension. Over time, we hope to see a wide variety of extensions emerge that follow this pattern and provide a wide range of choices.</p>"},{"location":"#model-server-frameworks","title":"Model Server Frameworks","text":"<p>This project will work closely with model server frameworks to establish a shared standard for interacting with these extensions, particularly focused on metrics and observability so extensions will be able to make informed routing decisions. The project is currently focused on integrations with vLLM and Triton, and will be open to other integrations as they are requested.</p>"},{"location":"#request-flow","title":"Request Flow","text":"<p>To illustrate how this all comes together, it may be helpful to walk through a sample request.</p> <ol> <li> <p>The first step involves the Gateway selecting the correct InferencePool (set of endpoints running a model server framework) or Service to route to. This logic is based on the existing Gateway and HTTPRoute APIs, and will be familiar to any Gateway API users or implementers.</p> </li> <li> <p>If the request should be routed to an InferencePool, the Gateway will forward the request information to the endpoint selection extension for that pool.</p> </li> <li> <p>The extension will fetch metrics from whichever portion of the InferencePool endpoints can best achieve the configured objectives. Note that this kind of metrics probing may happen asynchronously, depending on the extension.</p> </li> <li> <p>The extension will instruct the Gateway which endpoint the request should be routed to.</p> </li> <li> <p>The Gateway will route the request to the desired endpoint.</p> </li> </ol> <p></p>"},{"location":"#who-is-working-on-gateway-api-inference-extension","title":"Who is working on Gateway API Inference Extension?","text":"<p>This project is being driven by WG-Serving SIG-Network to improve and standardize routing to inference workloads in Kubernetes. Check out the implementations reference to see the latest projects &amp; products that support this project. If you are interested in contributing to or building an implementation using Gateway API then don\u2019t hesitate to get involved!</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-can-i-get-involved-with-this-project","title":"How can I get involved with this project?","text":"<p>The contributing page keeps track of how to get involved with the project.</p>"},{"location":"faq/#why-isnt-this-project-in-the-main-gateway-api-repo","title":"Why isn't this project in the main Gateway API repo?","text":"<p>This project is an extension of Gateway API, and may eventually be merged into the main Gateway API repo. As we're starting, this project represents a close collaboration between WG-Serving, SIG-Network, and the Gateway API subproject. These groups are all well represented within the ownership of this project, and the separate repo enables this group to iterate more quickly as this project is getting started. As the project stabilizes, we'll revisit if it should become part of the main Gateway API project.</p>"},{"location":"faq/#will-there-be-a-default-controller-implementation","title":"Will there be a default controller implementation?","text":"<p>No. Although this project will provide a default/reference implementation of an extension, each individual Gateway controller can support this pattern. The scope of this project is to define the API extension model, a reference extension, conformance tests, and overall documentation.</p>"},{"location":"faq/#can-you-add-support-for-my-use-case-to-the-reference-extension","title":"Can you add support for my use case to the reference extension?","text":"<p>Maybe. We're trying to keep the scope of the reference extension fairly narrow and instead hoping to see an ecosystem of compatible extensions developed in this space. Unless a use case fits neatly into the existing scope of our reference extension, it would likely be better to develop a separate extension focused on your use case.</p>"},{"location":"implementations/","title":"Implementations","text":"<p>This project has several implementations that are planned or in progress:</p> <ul> <li>Envoy Gateway</li> <li>Kgateway</li> <li>Google Kubernetes Engine</li> </ul>"},{"location":"implementations/#envoy-gateway","title":"Envoy Gateway","text":"<p>Envoy Gateway is an Envoy subproject for managing Envoy-based application gateways. The supported APIs and fields of the Gateway API are outlined here. Use the quickstart to get Envoy Gateway running with Gateway API in a few simple steps.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/#kgateway","title":"Kgateway","text":"<p>Kgateway is a feature-rich, Kubernetes-native ingress controller and next-generation API gateway. Kgateway brings the full power and community support of Gateway API to its existing control-plane implementation.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>Google Kubernetes Engine (GKE) is a managed Kubernetes platform offered by Google Cloud. GKE's implementation of the Gateway API is through the GKE Gateway controller which provisions Google Cloud Load Balancers for Pods in GKE clusters.</p> <p>The GKE Gateway controller supports weighted traffic splitting, mirroring, advanced routing, multi-cluster load balancing and more. See the docs to deploy private or public Gateways and also multi-cluster Gateways.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"api-types/inferencemodel/","title":"Inference Model","text":"Alpha since v0.1.0 <p>The <code>InferenceModel</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencemodel/#background","title":"Background","text":"<p>An InferenceModel allows the Inference Workload Owner to define:</p> <ul> <li>Which Model/LoRA adapter(s) to consume.</li> <li>Mapping from a client facing model name to the target model name in the InferencePool.</li> <li>InferenceModel allows for traffic splitting between adapters in the same InferencePool to allow for new LoRA adapter versions to be easily rolled out.</li> <li>Criticality of the requests to the InferenceModel.</li> </ul>"},{"location":"api-types/inferencemodel/#spec","title":"Spec","text":"<p>The full spec of the InferenceModel is defined here.</p>"},{"location":"api-types/inferencepool/","title":"Inference Pool","text":"Alpha since v0.1.0 <p>The <code>InferencePool</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencepool/#background","title":"Background","text":"<p>The InferencePool resource is a logical grouping of compute resources, e.g. Pods, that run model servers. The InferencePool would deploy its own routing, and offer administrative configuration to the Platform Admin. </p> <p>It is expected for the InferencePool to:</p> <ul> <li>Enforce fair consumption of resources across competing workloads</li> <li>Efficiently route requests across shared compute (as displayed by the PoC)</li> </ul> <p>It is not expected for the InferencePool to:</p> <ul> <li>Enforce any common set of adapters or base models are available on the Pods</li> <li>Manage Deployments of Pods within the Pool</li> <li>Manage Pod lifecycle of pods within the pool </li> </ul> <p>Additionally, any Pod that seeks to join an InferencePool would need to support a protocol, defined by this project, to ensure the Pool has adequate information to intelligently route requests.</p> <p><code>InferencePool</code> has some small overlap with <code>Service</code>, displayed here:</p> <p></p> <p>The InferencePool is not intended to be a mask of the Service object, simply exposing the absolute bare minimum required to allow the Platform Admin to focus less on networking, and more on Pool management. </p>"},{"location":"api-types/inferencepool/#spec","title":"Spec","text":"<p>The full spec of the InferencePool is defined here.</p>"},{"location":"concepts/api-overview/","title":"API Overview","text":""},{"location":"concepts/api-overview/#background","title":"Background","text":"<p>The Gateway API Inference Extension project is an extension of the Kubernetes Gateway API for serving Generative AI models on Kubernetes. Gateway API Inference Extension facilitates standardization of APIs for Kubernetes cluster operators and developers running generative AI inference, while allowing flexibility for underlying gateway implementations (such as Envoy Proxy) to iterate on mechanisms for optimized serving of models.</p> <p></p>"},{"location":"concepts/api-overview/#api-resources","title":"API Resources","text":""},{"location":"concepts/api-overview/#inferencepool","title":"InferencePool","text":"<p>InferencePool represents a set of Inference-focused Pods and an extension that will be used to route to them. Within the broader Gateway API resource model, this resource is considered a \"backend\". In practice, that means that you'd replace a Kubernetes Service with an InferencePool. This resource has some similarities to Service (a way to select Pods and specify a port), but has some unique capabilities. With InferenceModel, you can configure a routing extension as well as inference-specific routing optimizations. For more information on this resource, refer to our InferencePool documentation or go directly to the InferencePool spec.</p>"},{"location":"concepts/api-overview/#inferencemodel","title":"InferenceModel","text":"<p>An InferenceModel represents a model or adapter, and configuration associated with that model. This resource enables you to configure the relative criticality of a model, and allows you to seamlessly translate the requested model name to one or more backend model names. Multiple InferenceModels can be attached to an InferencePool. For more information on this resource, refer to our InferenceModel documentation or go directly to the InferenceModel spec.</p>"},{"location":"concepts/conformance/","title":"Conformance","text":"<p>Similar to Gateway API, this project will rely on conformance tests to ensure compatibility across implementations. This will be focused on three different layers:</p>"},{"location":"concepts/conformance/#1-gateway-api-implementations","title":"1. Gateway API Implementations","text":"<p>Conformance tests will verify that:</p> <ul> <li>InferencePool is supported as a backend type</li> <li>Implementations forward requests to the configured extension for an   InferencePool following the specification defined by this project</li> <li>Implementations honor the routing guidance provided by the extension</li> <li>Implementations behave appropriately when an extension is either not present   or fails to respond</li> </ul>"},{"location":"concepts/conformance/#2-inference-routing-extensions","title":"2. Inference Routing Extensions","text":"<p>Conformance tests will verify that:</p> <ul> <li>Extensions accept requests that match the protocol specified by this project</li> <li>Extensions respond with routing guidance that matches the protocol specified   by this project</li> </ul>"},{"location":"concepts/conformance/#3-model-server-frameworks","title":"3. Model Server Frameworks","text":"<p>Conformance tests will verify that:</p> <ul> <li>Frameworks serve the expected set of metrics using a format and path specified   by this project</li> </ul>"},{"location":"concepts/roles-and-personas/","title":"Roles and Personas","text":"<p>Before diving into the details of the API, descriptions of the personas these APIs were designed for will help convey the thought process of the API design.</p>"},{"location":"concepts/roles-and-personas/#inference-platform-admin","title":"Inference Platform Admin","text":"<p>The Inference Platform Admin creates and manages the infrastructure necessary to run LLM workloads, including handling Ops for:</p> <ul> <li>Hardware</li> <li>Model Server</li> <li>Base Model</li> <li>Resource Allocation for Workloads</li> <li>Gateway configuration</li> <li>etc</li> </ul>"},{"location":"concepts/roles-and-personas/#inference-workload-owner","title":"Inference Workload Owner","text":"<p>An Inference Workload Owner persona owns and manages one or many Generative AI Workloads (LLM focused currently). This includes:</p> <ul> <li>Defining criticality</li> <li>Managing fine-tunes</li> <li>LoRA Adapters</li> <li>System Prompts</li> <li>Prompt Cache</li> <li>etc.</li> <li>Managing rollout of adapters</li> </ul>"},{"location":"contributing/","title":"How to Get Involved","text":"<p>This page contains links to all of the meeting notes, design docs and related discussions around the APIs.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Bug reports should be filed as GitHub Issues on this repo.</p> <p>NOTE: If you're reporting a bug that applies to a specific implementation of this project, please check our implementations page to find links to the repositories where you can get help with your specific implementation.</p>"},{"location":"contributing/#communications","title":"Communications","text":"<p>Major discussions and notifications will be sent on both the WG-Serving and SIG-Network mailing lists.</p> <p>Although we may end up creating a new Slack channel in the future, our conversations are currently split between the following Kubernetes Slack channels:</p> <ul> <li>#sig-network-gateway-api</li> <li>#wg-serving</li> </ul>"},{"location":"contributing/#meetings","title":"Meetings","text":"<p>Gateway API community meetings happen every Thursday at 10am Pacific Time (convert to your timezone). To receive an invite to this and other WG-Serving community meetings, join the WG-Serving mailing list.</p> <ul> <li>Zoom link (passcode in meeting notes doc)</li> </ul>"},{"location":"contributing/#meeting-notes-and-recordings","title":"Meeting Notes and Recordings","text":"<p>Meeting agendas and notes are maintained in the meeting notes doc. Feel free to add topics for discussion at an upcoming meeting.</p> <p>All meetings are recorded and automatically uploaded to the [WG-Serving meetings YouTube playlist][https://www.youtube.com/playlist?list=PL69nYSiGNLP30qNanabU75ayPK7OPNAAS].</p>"},{"location":"contributing/devguide/","title":"Developer Guide","text":"<p>TODO</p>"},{"location":"gieps/overview/","title":"Gateway Inference Enhancement Proposal (GIEP)","text":"<p>Gateway Inference Enhancement Proposals (GIEPs) serve a similar purpose to the GIEP process for the main Gateway API project:</p> <ol> <li>Ensure that changes to the API follow a known process and discussion in the   OSS community.</li> <li>Make changes and proposals discoverable (current and future).</li> <li>Document design ideas, tradeoffs, decisions that were made for historical   reference.</li> <li>Record the results of larger community discussions.</li> <li>Record changes to the GIEP process itself.</li> </ol>"},{"location":"gieps/overview/#process","title":"Process","text":"<p>This diagram shows the state diagram of the GIEP process at a high level, but the details are below.</p> <pre><code>flowchart TD\n    D([Discuss with&lt;br /&gt;the community]) --&gt; C\n    C([Issue Created]) -------&gt; Memorandum\n    C([Issue Created]) --&gt; Provisional\n    Provisional --&gt;|If practical &lt;br /&gt; work needed| Prototyping\n    Provisional --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Prototyping --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Implementable --&gt;|Gateway API&lt;br /&gt;work completed| Experimental\n    Experimental --&gt;|Supported in&lt;br /&gt;multiple implementations&lt;br /&gt;+ Conformance tests| Standard\n    Standard --&gt;|Entire change is GA or implemented| Completed</code></pre>"},{"location":"gieps/overview/#giep-definitions","title":"GIEP Definitions","text":""},{"location":"gieps/overview/#giep-states","title":"GIEP States","text":"<p>Each GIEP has a state, which tracks where it is in the GIEP process.</p> <p>GIEPs can move to some states from any other state:</p> <ul> <li>Declined: The GIEP has been declined and further work will not occur.</li> <li>Deferred: We do not currently have bandwidth to handle this GIEP, it may     be revisited in the future.</li> <li>Declined: This proposal was considered by the community but ultimately   rejected.</li> <li>Withdrawn: This proposal was considered by the community but ultimately   withdrawn by the author.</li> </ul> <p>There is a special state to cover Memorandum GIEPs:</p> <ul> <li>Memorandum: These GIEPs either:<ul> <li>Document an agreement for further work, creating no spec changes   themselves, or</li> <li>Update the GIEP process.</li> </ul> </li> </ul> <p>API GIEPs flow through a number of states, which generally correspond to the level of stability of the change described in the GIEP:</p> <ul> <li>Provisional: The goals described by this GIEP have consensus but     implementation details have not been agreed to yet.</li> <li>Prototyping: An extension of <code>Provisional</code> which can be opted in to in     order to indicate to the community that there are some active practical     tests and experiments going on which are intended to be a part of the     development of this GIEP. This may include APIs or code, but that content     must not be distributed with releases.</li> <li>Implementable: The goals and implementation details described by this     GIEP have consensus but have not been fully implemented yet.</li> <li>Experimental: This GIEP has been implemented and is part of the     \"Experimental\" release channel. Breaking changes are still possible, up to     and including complete removal and moving to <code>Rejected</code>.</li> <li>Standard: This GIEP has been implemented and is part of the \"Standard\"     release channel. It should be quite stable.</li> <li>Completed: All implementation work on this API GIEP has been completed.</li> </ul>"},{"location":"gieps/overview/#relationships-between-gieps","title":"Relationships between GIEPs","text":"<p>GIEPs can have relationships between them. At this time, there are three possible relationships:</p> <ul> <li>Obsoletes and its backreference ObsoletedBy: when a GIEP is made   obsolete by another GIEP, and has its functionality completely replaced. The   Obsoleted GIEP is moved to the Declined state.</li> <li>Extends and its backreference ExtendedBy: when a GIEP has additional   details or implementation added in another GIEP.</li> <li>SeeAlso: when a GIEP is relevant to another GIEP, but is not affected in   any other defined way.</li> </ul> <p>Relationships are tracked in the YAML metadata files accompanying each GIEP.</p>"},{"location":"gieps/overview/#giep-metadata-file","title":"GIEP metadata file","text":"<p>Each GIEP has a YAML file containing metadata alongside it, please keep it up to date as changes to the GIEP occur.</p> <p>In particular, note the <code>authors</code>, and <code>changelog</code> fields, please keep those up to date.</p>"},{"location":"gieps/overview/#process_1","title":"Process","text":""},{"location":"gieps/overview/#1-discuss-with-the-community","title":"1. Discuss with the community","text":"<p>Before creating a GIEP, share your high level idea with the community. There are several places this may be done:</p> <ul> <li>A new GitHub   Discussion</li> <li>On our Slack Channel</li> <li>On one of our community   meetings</li> </ul> <p>Please default to GitHub discussions: they work a lot like GitHub issues which makes them easy to search.</p>"},{"location":"gieps/overview/#2-create-an-issue","title":"2. Create an Issue","text":"<p>Create a GIEP issue in the repo describing your change. At this point, you should copy the outcome of any other conversations or documents into this document.</p>"},{"location":"gieps/overview/#3-agree-on-the-goals","title":"3. Agree on the Goals","text":"<p>Although it can be tempting to start writing out all the details of your proposal, it's important to first ensure we all agree on the goals.</p> <p>For API GIEPs, the first version of your GIEP should aim for a \"Provisional\" status and leave out any implementation details, focusing primarily on \"Goals\" and \"Non-Goals\".</p> <p>For Memorandum GIEPs, the first version of your GIEP will be the only one, as Memorandums have only a single stage - <code>Accepted</code>.</p>"},{"location":"gieps/overview/#3-document-implementation-details","title":"3. Document Implementation Details","text":"<p>Now that everyone agrees on the goals, it is time to start writing out your proposed implementation details. These implementation details should be very thorough, including the proposed API spec, and covering any relevant edge cases. Note that it may be helpful to use a shared doc for part of this phase to enable faster iteration on potential designs.</p> <p>It is likely that throughout this process, you will discuss a variety of alternatives. Be sure to document all of these in the GIEP, and why we decided against them. At this stage, the GIEP should be targeting the \"Implementable\" stage.</p>"},{"location":"gieps/overview/#4-implement-the-giep-as-experimental","title":"4. Implement the GIEP as \"Experimental\"","text":"<p>With the GIEP marked as \"Implementable\", it is time to actually make those proposed changes in our API. In some cases, these changes will be documentation only, but in most cases, some API changes will also be required. It is important that every new feature of the API is marked as \"Experimental\" when it is introduced. Within the API, we use <code>&lt;gateway:experimental&gt;</code> tags to denote experimental fields. Within Golang packages (conformance tests, CLIs, e.t.c.) we use the <code>experimental</code> Golang build tag to denote experimental functionality.</p> <p>Some other requirements must be met before marking a GIEP <code>Experimental</code>:</p> <ul> <li>the graduation criteria to reach <code>Standard</code> MUST be filled out</li> <li>a proposed probationary period (see next section) must be included in the GIEP   and approved by maintainers.</li> </ul> <p>Before changes are released they MUST be documented. GIEPs that have not been both implemented and documented before a release cut off will be excluded from the release.</p>"},{"location":"gieps/overview/#probationary-period","title":"Probationary Period","text":"<p>Any GIEP in the <code>Experimental</code> phase is automatically under a \"probationary period\" where it will come up for re-assessment if its graduation criteria are not met within a given time period. GIEPs that wish to move into <code>Experimental</code> status MUST document a proposed period (6 months is the suggested default) that MUST be approved by maintainers. Maintainers MAY select an alternative time duration for a probationary period if deemed appropriate, and will document their reasoning.</p> <p>Rationale: This probationary period exists to avoid GIEPs getting \"stale\" and to provide guidance to implementations about how relevant features should be used, given that they are not guaranteed to become supported.</p> <p>At the end of a probationary period if the GIEP has not been able to resolve its graduation criteria it will move to \"Rejected\" status. In extenuating circumstances an extension of that period may be accepted by approval from maintainers. GIEPs which are <code>Rejected</code> in this way are removed from the experimental CRDs and more or less put on hold. GIEPs may be allowed to move back into <code>Experimental</code> status from <code>Rejected</code> for another probationary period if a new strategy for achieving their graduation criteria can be established. Any such plan to take a GIEP \"off the shelf\" must be reviewed and accepted by the maintainers.</p> <p>Warning: It is extremely important** that projects which implement <code>Experimental</code> features clearly document that these features may be removed in future releases.</p>"},{"location":"gieps/overview/#5-graduate-the-giep-to-standard","title":"5. Graduate the GIEP to \"Standard\"","text":"<p>Once this feature has met the graduation criteria, it is time to graduate it to the \"Standard\" channel of the API. Depending on the feature, this may include any of the following:</p> <ol> <li>Graduating the resource to beta</li> <li>Graduating fields to \"standard\" by removing <code>&lt;gateway:experimental&gt;</code> tags</li> <li>Graduating a concept to \"standard\" by updating documentation</li> </ol>"},{"location":"gieps/overview/#6-close-out-the-giep-issue","title":"6. Close out the GIEP issue","text":"<p>The GIEP issue should only be closed once the feature has: - Moved to the standard channel for distribution (if necessary) - Moved to a \"v1\" <code>apiVersion</code> for CRDs - been completely implemented and has wide acceptance (for process changes).</p> <p>In short, the GIEP issue should only be closed when the work is \"done\" (whatever that means for that GIEP).</p>"},{"location":"gieps/overview/#format","title":"Format","text":"<p>GIEPs should match the format of the template found in GIEP-696.</p>"},{"location":"gieps/overview/#out-of-scope","title":"Out of scope","text":"<p>What is out of scope: see text from KEP. Examples:</p> <ul> <li>Bug fixes</li> <li>Small changes (API validation, documentation, fixups). It is always possible   that the reviewers will determine a \"small\" change ends up requiring a GIEP.</li> </ul>"},{"location":"gieps/overview/#faq","title":"FAQ","text":""},{"location":"gieps/overview/#why-is-it-named-giep","title":"Why is it named GIEP?","text":"<p>To avoid potential confusion if people start following the cross references to the full GEP or KEP process.</p>"},{"location":"gieps/overview/#why-have-a-different-process-than-mainline","title":"Why have a different process than mainline?","text":"<p>Gateway API has some differences with most upstream KEPs. Notably Gateway API intentionally avoids including any implementation with the project, so this process is focused entirely on the substance of the API. As this project is based on CRDs it also has an entirely separately release process, and has developed concepts like \"release channels\" that do not exist in upstream.</p>"},{"location":"gieps/overview/#is-it-ok-to-discuss-using-shared-docs-scratch-docs-etc","title":"Is it ok to discuss using shared docs, scratch docs etc?","text":"<p>Yes, this can be a helpful intermediate step when iterating on design details. It is important that all major feedback, discussions, and alternatives considered in that step are represented in the GIEP though. A key goal of GIEPs is to show why we made a decision and which alternatives were considered. If separate docs are used, it's important that we can still see all relevant context and decisions in the final GIEP.</p>"},{"location":"gieps/overview/#when-should-i-mark-a-giep-as-prototyping-as-opposed-to-provisional","title":"When should I mark a GIEP as <code>Prototyping</code> as opposed to <code>Provisional</code>?","text":"<p>The <code>Prototyping</code> status carries the same base meaning as <code>Provisional</code> in that consensus is not complete between stakeholders and we're not ready to move toward releasing content yet. You should use <code>Prototyping</code> to indicate to your fellow community members that we're in a state of active practical tests and experiments which are intended to help us learn and iterate on the GIEP. These can include distributing content, but not under any release channel.</p>"},{"location":"gieps/overview/#should-i-implement-support-for-experimental-channel-features","title":"Should I implement support for <code>Experimental</code> channel features?","text":"<p>Ultimately one of the main ways to get something into <code>Standard</code> is for it to mature through the <code>Experimental</code> phase, so we really need people to implement these features and provide feedback in order to have progress. That said, the graduation of a feature past <code>Experimental</code> is not a forgone conclusion. Before implementing an experimental feature, you should:</p> <ul> <li>Clearly document that support for the feature is experimental and may   disappear in the future.</li> <li>Have a plan in place for how you would handle the removal of this feature from   the API.</li> </ul>"},{"location":"gieps/giep-116/","title":"GIEP-116: GIEP template","text":"<ul> <li>Issue: #0</li> <li>Status: Provisional|Implementable|Experimental|Standard|Deferred|Rejected|Withdrawn|Replaced</li> </ul> <p>(See status definitions here.)</p>"},{"location":"gieps/giep-116/#tldr","title":"TLDR","text":"<p>(1-2 sentence summary of the proposal)</p>"},{"location":"gieps/giep-116/#goals","title":"Goals","text":"<p>(Primary goals of this proposal.)</p>"},{"location":"gieps/giep-116/#non-goals","title":"Non-Goals","text":"<p>(What is out of scope for this proposal.)</p>"},{"location":"gieps/giep-116/#introduction","title":"Introduction","text":"<p>(Can link to external doc -- but we should bias towards copying the content into the GEP as online documents are easier to lose -- e.g. owner messes up the permissions, accidental deletion)</p>"},{"location":"gieps/giep-116/#api","title":"API","text":"<p>(... details, can point to PR with changes)</p>"},{"location":"gieps/giep-116/#conformance-details","title":"Conformance Details","text":"<p>(This section describes the names to be used for the feature or features in conformance tests and profiles.</p> <p>These should be <code>CamelCase</code> names that specify the feature as precisely as possible, and are particularly important for Extended features, since they may be surfaced to users.)</p>"},{"location":"gieps/giep-116/#alternatives","title":"Alternatives","text":"<p>(List other design alternatives and why we did not go in that direction)</p>"},{"location":"gieps/giep-116/#references","title":"References","text":"<p>(Add any additional document links. Again, we should try to avoid too much content not in version control to avoid broken links)</p>"},{"location":"guides/","title":"Getting started with Gateway API Inference Extension","text":"<p>This quickstart guide is intended for engineers familiar with k8s and model servers (vLLM in this instance). The goal of this guide is to get a first, single InferencePool up and running! </p>"},{"location":"guides/#prerequisites","title":"Prerequisites","text":"<ul> <li>Envoy Gateway v1.2.1 or higher</li> <li>A cluster with:</li> <li>Support for Services of type <code>LoadBalancer</code>. (This can be validated by ensuring your Envoy Gateway is up and running). For example, with Kind,      you can follow these steps.</li> <li>3 GPUs to run the sample model server. Adjust the number of replicas in <code>./config/manifests/vllm/deployment.yaml</code> as needed.</li> </ul>"},{"location":"guides/#steps","title":"Steps","text":""},{"location":"guides/#deploy-sample-model-server","title":"Deploy Sample Model Server","text":"<p>Create a Hugging Face secret to download the model meta-llama/Llama-2-7b-hf. Ensure that the token grants access to this model.    Deploy a sample vLLM deployment with the proper protocol to work with the LLM Instance Gateway.    <pre><code>kubectl create secret generic hf-token --from-literal=token=$HF_TOKEN # Your Hugging Face Token with access to Llama2\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/deployment.yaml\n</code></pre></p>"},{"location":"guides/#install-the-inference-extension-crds","title":"Install the Inference Extension CRDs","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/crd/bases/inference.networking.x-k8s.io_inferencepools.yaml\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/crd/bases/inference.networking.x-k8s.io_inferencemodels.yaml\n</code></pre>"},{"location":"guides/#deploy-inferencemodel","title":"Deploy InferenceModel","text":"<p>Deploy the sample InferenceModel which is configured to load balance traffic between the <code>tweet-summary-0</code> and <code>tweet-summary-1</code> LoRA adapters of the sample model server.    <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml\n</code></pre></p>"},{"location":"guides/#update-envoy-gateway-config-to-enable-patch-policy","title":"Update Envoy Gateway Config to enable Patch Policy**","text":"<p>Our custom LLM Gateway ext-proc is patched into the existing envoy gateway via <code>EnvoyPatchPolicy</code>. To enable this feature, we must extend the Envoy Gateway config map. To do this, simply run:    <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/enable_patch_policy.yaml\nkubectl rollout restart deployment envoy-gateway -n envoy-gateway-system\n</code></pre>    Additionally, if you would like to enable the admin interface, you can uncomment the admin lines and run this again.</p>"},{"location":"guides/#deploy-gateway","title":"Deploy Gateway","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gateway.yaml\n</code></pre> <p>NOTE: This file couples together the gateway infra and the HTTPRoute infra for a convenient, quick startup. Creating additional/different InferencePools on the same gateway will require an additional set of: <code>Backend</code>, <code>HTTPRoute</code>, the resources included in the <code>./manifests/gateway/ext-proc.yaml</code> file, and an additional <code>./manifests/gateway/patch_policy.yaml</code> file. Should you choose to experiment, familiarity with xDS and Envoy are very useful.</p> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:    <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   inference-gateway   &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p>"},{"location":"guides/#deploy-the-inference-extension-and-inferencepool","title":"Deploy the Inference Extension and InferencePool","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/ext_proc.yaml\n</code></pre>"},{"location":"guides/#deploy-envoy-gateway-custom-policies","title":"Deploy Envoy Gateway Custom Policies","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/extension_policy.yaml\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/patch_policy.yaml\n</code></pre> <p>NOTE: This is also per InferencePool, and will need to be configured to support the new pool should you wish to experiment further.</p>"},{"location":"guides/#optionally-apply-traffic-policy","title":"OPTIONALLY: Apply Traffic Policy","text":"<p>For high-traffic benchmarking you can apply this manifest to avoid any defaults that can cause timeouts/errors.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/traffic_policy.yaml\n</code></pre>"},{"location":"guides/#try-it-out","title":"Try it out","text":"<p>Wait until the gateway is ready.</p> <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=8081\n\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"tweet-summary\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre>"},{"location":"guides/adapter-rollout/","title":"Adapter Rollout","text":"<p>The goal of this guide is to demonstrate how to rollout a new adapter version.</p>"},{"location":"guides/adapter-rollout/#prerequisites","title":"Prerequisites","text":"<p>Follow the steps in the main guide</p>"},{"location":"guides/adapter-rollout/#safely-rollout-v2-adapter","title":"Safely rollout v2 adapter","text":""},{"location":"guides/adapter-rollout/#load-the-new-adapter-version-to-the-model-servers","title":"Load the new adapter version to the model servers","text":"<p>This guide leverages the LoRA syncer sidecar to dynamically manage adapters within a vLLM deployment, enabling users to add or remove them through a shared ConfigMap.</p> <p>Modify the LoRA syncer ConfigMap to initiate loading of the new adapter version.</p> <pre><code>   kubectl edit configmap vllm-llama2-7b-adapters\n</code></pre> <p>Change the ConfigMap to match the following (note the new entry under models):</p> <pre><code>        apiVersion: v1\n        kind: ConfigMap\n        metadata:\n        name: vllm-llama2-7b-adapters\n        data:\n        configmap.yaml: |\n             vLLMLoRAConfig:\n                name: vllm-llama2-7b-adapters\n                port: 8000\n                ensureExist:\n                    models:\n                    - base-model: meta-llama/Llama-2-7b-hf\n                      id: tweet-summary-1\n                      source: vineetsharma/qlora-adapter-Llama-2-7b-hf-TweetSumm\n                    - base-model: meta-llama/Llama-2-7b-hf\n                      id: tweet-summary-2\n                      source: mahimairaja/tweet-summarization-llama-2-finetuned\n</code></pre> <p>The new adapter version is applied to the model servers live, without requiring a restart.</p>"},{"location":"guides/adapter-rollout/#direct-traffic-to-the-new-adapter-version","title":"Direct traffic to the new adapter version","text":"<p>Modify the InferenceModel to configure a canary rollout with traffic splitting. In this example, 10% of traffic for tweet-summary model will be sent to the new tweet-summary-2 adapter.</p> <pre><code>   kubectl edit inferencemodel tweet-summary\n</code></pre> <p>Change the targetModels list in InferenceModel to match the following:</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha1\nkind: InferenceModel\nmetadata:\n  name: inferencemodel-sample\nspec:\n  modelName: tweet-summary\n  criticality: Critical\n  poolRef:\n    name: vllm-llama2-7b-pool\n  targetModels:\n  - name: tweet-summary-1\n    weight: 90\n  - name: tweet-summary-2\n    weight: 10\n</code></pre> <p>The above configuration means one in every ten requests should be sent to the new version. Try it out:</p> <ol> <li> <p>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=8081\n</code></pre></p> </li> <li> <p>Send a few requests as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"tweet-summary\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></p> </li> </ol>"},{"location":"guides/adapter-rollout/#finish-the-rollout","title":"Finish the rollout","text":"<p>Modify the InferenceModel to direct 100% of the traffic to the latest version of the adapter.</p> <pre><code>model:\n    name: tweet-summary\n    targetModels:\n    targetModelName: tweet-summary-2\n            weight: 100\n</code></pre> <p>Unload the older versions from the servers by updating the LoRA syncer ConfigMap to list the older version under the <code>ensureNotExist</code> list:</p> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n    name: dynamic-lora-config\n    data:\n    configmap.yaml: |\n            vLLMLoRAConfig:\n                name: sql-loras-llama\n                port: 8000\n                ensureExist:\n                    models:\n                    - base-model: meta-llama/Llama-2-7b-hf\n                      id: tweet-summary-2\n                      source: mahimairaja/tweet-summarization-llama-2-finetuned\n                ensureNotExist:\n                    models:\n                    - base-model: meta-llama/Llama-2-7b-hf\n                      id: tweet-summary-1\n                      source: vineetsharma/qlora-adapter-Llama-2-7b-hf-TweetSumm\n</code></pre> <p>With this, all requests should be served by the new adapter version.</p>"},{"location":"guides/implementers/","title":"Implementer's Guide","text":"<p>TODO</p>"},{"location":"guides/metrics/","title":"Metrics","text":"<p>This guide describes the current state of exposed metrics and how to scrape them.</p>"},{"location":"guides/metrics/#requirements","title":"Requirements","text":"<p>Response metrics are only supported in non-streaming mode, with the follow up issue to address streaming mode.</p> <p>Currently there are two options: - If requests don't use response streaming, then you can enable <code>Buffered</code> mode for response in <code>EnvoyExtensionPolicy</code>, this will buffer the response body at the proxy and forward it to the endpoint picker, which allows the endpoint picker to report response metrics.</p> <ul> <li>If requests use response streaming, then it is not recommended to enable <code>Buffered</code> mode, the response body processing mode should be left empty in the <code>EnvoyExtensionPolicy</code> (default). In this case response bodies will not be forwarded to the endpoint picker, and therefore response metrics will not be reported.</li> </ul> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyExtensionPolicy\nmetadata:\n  name: ext-proc-policy\n  namespace: default\nspec:\n  extProc:\n    - backendRefs:\n      - group: \"\"\n        kind: Service\n        name: inference-gateway-ext-proc\n        port: 9002\n      processingMode:\n        request:\n          body: Buffered\n        response:\n          body: Buffered\n</code></pre>"},{"location":"guides/metrics/#exposed-metrics","title":"Exposed metrics","text":"Metric name Metric Type Description Labels Status inference_model_request_total Counter The counter of requests broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_error_total Counter The counter of requests errors broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_duration_seconds Distribution Distribution of response latency. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_sizes Distribution Distribution of request size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_response_sizes Distribution Distribution of response size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_input_tokens Distribution Distribution of input token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_output_tokens Distribution Distribution of output token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_pool_average_kv_cache_utilization Gauge The average kv cache utilization for an inference server pool. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_average_queue_size Gauge The average number of requests pending in the model server queue. <code>name</code>=&lt;inference-pool-name&gt; ALPHA"},{"location":"guides/metrics/#scrape-metrics","title":"Scrape Metrics","text":"<p>Metrics endpoint is exposed at port 9090 by default. To scrape metrics, the client needs a ClusterRole with the following rule: <code>nonResourceURLs: \"/metrics\", verbs: get</code>.</p> <p>Here is one example if the client needs to mound the secret to act as the service account <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: inference-gateway-metrics-reader\nrules:\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: inference-gateway-sa-metrics-reader-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: inference-gateway-metrics-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: inference-gateway-sa-metrics-reader-secret\n  namespace: default\n  annotations:\n    kubernetes.io/service-account.name: inference-gateway-sa-metrics-reader\ntype: kubernetes.io/service-account-token\n</code></pre> Then, you can curl the 9090 port like following <pre><code>TOKEN=$(kubectl -n default get secret inference-gateway-sa-metrics-reader-secret  -o jsonpath='{.secrets[0].name}' -o jsonpath='{.data.token}' | base64 --decode)\n\nkubectl -n default port-forward inference-gateway-ext-proc-pod-name  9090\n\ncurl -H \"Authorization: Bearer $TOKEN\" localhost:9090/metrics\n</code></pre></p>"},{"location":"reference/spec/","title":"API Reference","text":""},{"location":"reference/spec/#packages","title":"Packages","text":"<ul> <li>inference.networking.x-k8s.io/v1alpha1</li> </ul>"},{"location":"reference/spec/#inferencenetworkingx-k8siov1alpha1","title":"inference.networking.x-k8s.io/v1alpha1","text":"<p>Package v1alpha1 contains API Schema definitions for the gateway v1alpha1 API group</p>"},{"location":"reference/spec/#resource-types","title":"Resource Types","text":"<ul> <li>InferenceModel</li> <li>InferencePool</li> </ul>"},{"location":"reference/spec/#criticality","title":"Criticality","text":"<p>Underlying type: string</p> <p>Defines how important it is to serve the model compared to other models.</p> <p>Validation: - Enum: [Critical Default Sheddable]</p> <p>Appears in: - InferenceModelSpec</p> Field Description <code>Critical</code> Most important. Requests to this band will be shed last. <code>Default</code> More important than Sheddable, less important than Critical.Requests in this band will be shed before critical traffic.+kubebuilder:default=Default <code>Sheddable</code> Least important. Requests to this band will be shed before all other bands."},{"location":"reference/spec/#inferencemodel","title":"InferenceModel","text":"<p>InferenceModel is the Schema for the InferenceModels API</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha1</code> <code>kind</code> string <code>InferenceModel</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferenceModelSpec <code>status</code> InferenceModelStatus"},{"location":"reference/spec/#inferencemodelspec","title":"InferenceModelSpec","text":"<p>InferenceModelSpec represents a specific model use case. This resource is managed by the \"Inference Workload Owner\" persona.</p> <p>The Inference Workload Owner persona is: a team that trains, verifies, and leverages a large language model from a model frontend, drives the lifecycle and rollout of new versions of those models, and defines the specific performance and latency goals for the model. These workloads are expected to operate within an InferencePool sharing compute capacity with other InferenceModels, defined by the Inference Platform Admin.</p> <p>InferenceModel's modelName (not the ObjectMeta name) is unique for a given InferencePool, if the name is reused, an error will be shown on the status of a InferenceModel that attempted to reuse. The oldest InferenceModel, based on creation timestamp, will be selected to remain valid. In the event of a race condition, one will be selected at random.</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>modelName</code> string The name of the model as the users set in the \"model\" parameter in the requests.The name should be unique among the workloads that reference the same backend pool.This is the parameter that will be used to match the request with. In the future, we mayallow to match on other request parameters. The other approach to support matching onon other request parameters is to use a different ModelName per HTTPFilter.Names can be reserved without implementing an actual model in the pool.This can be done by specifying a target model and setting the weight to zero,an error will be returned specifying that no valid target model is found. MaxLength: 253  <code>criticality</code> Criticality Defines how important it is to serve the model compared to other models referencing the same pool. Default Enum: [Critical Default Sheddable]  <code>targetModels</code> TargetModel array Allow multiple versions of a model for traffic splitting.If not specified, the target model name is defaulted to the modelName parameter.modelName is often in reference to a LoRA adapter. MaxItems: 10  <code>poolRef</code> PoolObjectReference Reference to the inference pool, the pool must exist in the same namespace. Required: {}"},{"location":"reference/spec/#inferencemodelstatus","title":"InferenceModelStatus","text":"<p>InferenceModelStatus defines the observed state of InferenceModel</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>conditions</code> Condition array Conditions track the state of the InferencePool."},{"location":"reference/spec/#inferencepool","title":"InferencePool","text":"<p>InferencePool is the Schema for the Inferencepools API</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha1</code> <code>kind</code> string <code>InferencePool</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferencePoolSpec <code>status</code> InferencePoolStatus"},{"location":"reference/spec/#inferencepoolspec","title":"InferencePoolSpec","text":"<p>InferencePoolSpec defines the desired state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>selector</code> object (keys:LabelKey, values:LabelValue) Selector uses a map of label to watch model server podsthat should be included in the InferencePool. ModelServers should notbe with any other Service or InferencePool, that behavior is not supportedand will result in sub-optimal utilization.In some cases, implementations may translate this to a Service selector, so this matches the simplemap used for Service selectors instead of the full Kubernetes LabelSelector type. Required: {}  <code>targetPortNumber</code> integer TargetPortNumber is the port number that the model servers within the pool expectto receive traffic from.This maps to the TargetPort in: https://pkg.go.dev/k8s.io/api/core/v1#ServicePort Maximum: 65535 Minimum: 0 Required: {}"},{"location":"reference/spec/#inferencepoolstatus","title":"InferencePoolStatus","text":"<p>InferencePoolStatus defines the observed state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>conditions</code> Condition array Conditions track the state of the InferencePool."},{"location":"reference/spec/#labelkey","title":"LabelKey","text":"<p>Underlying type: string</p> <p>Originally copied from: https://github.com/kubernetes-sigs/gateway-api/blob/99a3934c6bc1ce0874f3a4c5f20cafd8977ffcb4/apis/v1/shared_types.go#L694-L731 Duplicated as to not take an unexpected dependency on gw's API.</p> <p>LabelKey is the key of a label. This is used for validation of maps. This matches the Kubernetes \"qualified name\" validation that is used for labels.</p> <p>Valid values include:</p> <ul> <li>example</li> <li>example.com</li> <li>example.com/path</li> <li>example.com/path.html</li> </ul> <p>Invalid values include:</p> <ul> <li>example~ - \"~\" is an invalid character</li> <li>example.com. - can not start or end with \".\"</li> </ul> <p>Validation: - MaxLength: 253 - MinLength: 1 - Pattern: <code>^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?([A-Za-z0-9][-A-Za-z0-9_.]{0,61})?[A-Za-z0-9]$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#labelvalue","title":"LabelValue","text":"<p>Underlying type: string</p> <p>LabelValue is the value of a label. This is used for validation of maps. This matches the Kubernetes label validation rules: * must be 63 characters or less (can be empty), * unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]), * could contain dashes (-), underscores (_), dots (.), and alphanumerics between.</p> <p>Valid values include:</p> <ul> <li>MyValue</li> <li>my.name</li> <li>123-my-value</li> </ul> <p>Validation: - MaxLength: 63 - MinLength: 0 - Pattern: <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#poolobjectreference","title":"PoolObjectReference","text":"<p>PoolObjectReference identifies an API object within the namespace of the referrer.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>group</code> string Group is the group of the referent. inference.networking.x-k8s.io MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> string Kind is kind of the referent. For example \"InferencePool\". InferencePool MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> string Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}"},{"location":"reference/spec/#targetmodel","title":"TargetModel","text":"<p>TargetModel represents a deployed model or a LoRA adapter. The Name field is expected to match the name of the LoRA adapter (or base model) as it is registered within the model server. Inference Gateway assumes that the model exists on the model server and is the responsibility of the user to validate a correct match. Should a model fail to exist at request time, the error is processed by the Instance Gateway, and then emitted on the appropriate InferenceModel object.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>name</code> string The name of the adapter as expected by the ModelServer. MaxLength: 253  <code>weight</code> integer Weight is used to determine the proportion of traffic that should besent to this target model when multiple versions of the model are specified. 1 Maximum: 1e+06 Minimum: 0"}]}