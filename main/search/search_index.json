{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Gateway API Inference Extension is an official Kubernetes project focused on extending Gateway API with inference specific routing extensions.</p> <p>The overall resource model focuses on 2 new inference-focused personas and corresponding resources that they are expected to manage:</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<p>Gateway API Inference Extension, along with a reference implementation in Envoy Proxy, provides the following key features: </p> <ul> <li> <p>Model-aware routing: Instead of simply routing based on the path of the request, Gateway API Inference Extension allows you to route to models based on the model names. This is enabled by support for GenAI Inference API specifications (such as OpenAI API) in the gateway implementations such as in Envoy Proxy. This model-aware routing also extends to Low-Rank Adaptation (LoRA) fine-tuned models.</p> </li> <li> <p>Serving priority: Gateway API Inference Extension allows you to specify the serving priority of your models. For example, you can specify that your models for online inference of chat tasks (which is more latency sensitive) have a higher Criticality than a model for latency tolerant tasks such as a summarization. </p> </li> <li> <p>Model rollouts:  Gateway API Inference Extension allows you to incrementally roll out new model versions by traffic splitting definitions based on the model names. </p> </li> <li> <p>Extensibility for Inference Services: Gateway API Inference Extension defines extensibility pattern for additional Inference services to create bespoke routing capabilities should out of the box solutions not fit your needs.</p> </li> <li> <p>Customizable Load Balancing for Inference: Gateway API Inference Extension defines a pattern for customizable load balancing and request routing that is optimized for Inference. Gateway API Inference Extension provides a reference implementation of model endpoint picking leveraging metrics emitted from the model servers. This endpoint picking mechanism can be used in lieu of traditional load balancing mechanisms. Model Server-aware load balancing (\"smart\" load balancing as its sometimes referred to in this repo) has been proven to reduce the serving latency and improve utilization of accelerators in your clusters.</p> </li> </ul>"},{"location":"#api-resources","title":"API Resources","text":"<p>Head to our API overview to start exploring our APIs!</p>"},{"location":"#composable-layers","title":"Composable Layers","text":"<p>This project aims to define specifications to enable a compatible ecosystem for extending the Gateway API with custom endpoint selection algorithms. This project defines a set of patterns across three distinct layers of components that are relevant to this project:</p>"},{"location":"#gateway-api-implementations","title":"Gateway API Implementations","text":"<p>Gateway API has more than 25 implementations. As this pattern stabilizes, we expect a wide set of these implementations to support this project.</p>"},{"location":"#endpoint-selection-extension","title":"Endpoint Selection Extension","text":"<p>As part of this project, we're building an initial reference extension. Over time, we hope to see a wide variety of extensions emerge that follow this pattern and provide a wide range of choices.</p>"},{"location":"#model-server-frameworks","title":"Model Server Frameworks","text":"<p>This project will work closely with model server frameworks to establish a shared standard for interacting with these extensions, particularly focused on metrics and observability so extensions will be able to make informed routing decisions. The project is currently focused on integrations with vLLM and Triton, and will be open to other integrations as they are requested.</p>"},{"location":"#request-flow","title":"Request Flow","text":"<p>To illustrate how this all comes together, it may be helpful to walk through a sample request.</p> <ol> <li> <p>The first step involves the Gateway selecting the correct InferencePool (set of endpoints running a model server framework) or Service to route to. This logic is based on the existing Gateway and HTTPRoute APIs, and will be familiar to any Gateway API users or implementers.</p> </li> <li> <p>If the request should be routed to an InferencePool, the Gateway will forward the request information to the endpoint selection extension for that pool.</p> </li> <li> <p>The extension will fetch metrics from whichever portion of the InferencePool endpoints can best achieve the configured objectives. Note that this kind of metrics probing may happen asynchronously, depending on the extension.</p> </li> <li> <p>The extension will instruct the Gateway which endpoint the request should be routed to.</p> </li> <li> <p>The Gateway will route the request to the desired endpoint.</p> </li> </ol> <p></p>"},{"location":"#who-is-working-on-gateway-api-inference-extension","title":"Who is working on Gateway API Inference Extension?","text":"<p>This project is being driven by WG-Serving SIG-Network to improve and standardize routing to inference workloads in Kubernetes. Check out the implementations reference to see the latest projects &amp; products that support this project. If you are interested in contributing to or building an implementation using Gateway API then don\u2019t hesitate to get involved!</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-can-i-get-involved-with-this-project","title":"How can I get involved with this project?","text":"<p>The contributing page keeps track of how to get involved with the project.</p>"},{"location":"faq/#why-isnt-this-project-in-the-main-gateway-api-repo","title":"Why isn't this project in the main Gateway API repo?","text":"<p>This project is an extension of Gateway API, and may eventually be merged into the main Gateway API repo. As we're starting, this project represents a close collaboration between WG-Serving, SIG-Network, and the Gateway API subproject. These groups are all well represented within the ownership of this project, and the separate repo enables this group to iterate more quickly as this project is getting started. As the project stabilizes, we'll revisit if it should become part of the main Gateway API project.</p>"},{"location":"faq/#will-there-be-a-default-controller-implementation","title":"Will there be a default controller implementation?","text":"<p>No. Although this project will provide a default/reference implementation of an extension, each individual Gateway controller can support this pattern. The scope of this project is to define the API extension model, a reference extension, conformance tests, and overall documentation.</p>"},{"location":"faq/#can-you-add-support-for-my-use-case-to-the-reference-extension","title":"Can you add support for my use case to the reference extension?","text":"<p>Maybe. We're trying to keep the scope of the reference extension fairly narrow and instead hoping to see an ecosystem of compatible extensions developed in this space. Unless a use case fits neatly into the existing scope of our reference extension, it would likely be better to develop a separate extension focused on your use case.</p>"},{"location":"api-types/inferencemodel/","title":"Inference Model","text":"Alpha since v0.1.0 <p>The <code>InferenceModel</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencemodel/#background","title":"Background","text":"<p>An InferenceModel allows the Inference Workload Owner to define:</p> <ul> <li>Which Model/LoRA adapter(s) to consume.</li> <li>Mapping from a client facing model name to the target model name in the InferencePool.</li> <li>InferenceModel allows for traffic splitting between adapters in the same InferencePool to allow for new LoRA adapter versions to be easily rolled out.</li> <li>Criticality of the requests to the InferenceModel.</li> </ul>"},{"location":"api-types/inferencemodel/#spec","title":"Spec","text":"<p>The full spec of the InferenceModel is defined here.</p>"},{"location":"api-types/inferencepool/","title":"Inference Pool","text":"Alpha since v0.1.0 <p>The <code>InferencePool</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencepool/#background","title":"Background","text":"<p>The InferencePool API defines a group of Pods (containers) dedicated to serving AI models. Pods within an InferencePool share the same compute configuration, accelerator type, base language model, and model server. This abstraction simplifies the management of AI model serving resources, providing a centralized point of administrative configuration for Platform Admins.</p> <p>An InferencePool is expected to be bundled with an Endpoint Picker extension. This extension is responsible for tracking key metrics on each model server (i.e. the KV-cache utilization, queue length of pending requests, active LoRA adapters, etc.) and routing incoming inference requests to the optimal model server replica based on these metrics. An EPP can only be associated with a single InferencePool. The associated InferencePool is specified by the poolName and poolNamespace flags. An HTTPRoute can have multiple backendRefs that reference the same InferencePool and therefore routes to the same EPP. An HTTPRoute can have multiple backendRefs that reference different InferencePools and therefore routes to different EPPs.</p> <p>Additionally, any Pod that seeks to join an InferencePool would need to support the model server protocol, defined by this project, to ensure the Endpoint Picker has adequate information to intelligently route requests.</p>"},{"location":"api-types/inferencepool/#how-to-configure-an-inferencepool","title":"How to Configure an InferencePool","text":"<p>The full spec of the InferencePool is defined here.</p> <p>In summary, the InferencePoolSpec consists of 3 major parts:</p> <ul> <li>The <code>selector</code> field specifies which Pods belong to this pool. The labels in this selector must exactly match the labels applied to your model server Pods. </li> <li>The <code>targetPortNumber</code> field defines the port number that the Inference Gateway should route to on model server Pods that belong to this pool. </li> <li>The <code>extensionRef</code> field references the endpoint picker extension (EPP) service that monitors key metrics from model servers within the InferencePool and provides intelligent routing decisions.</li> </ul>"},{"location":"api-types/inferencepool/#example-configuration","title":"Example Configuration","text":"<p>Here is an example InferencePool configuration:</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp\n    port: 9002\n    failureMode: FailClose\n</code></pre> <p>In this example: </p> <ul> <li>An InferencePool named <code>vllm-llama3-8b-instruct</code> is created in the <code>default</code> namespace.</li> <li>It will select Pods that have the label <code>app: vllm-llama3-8b-instruct</code>.</li> <li>Traffic routed to this InferencePool will call out to the EPP service <code>vllm-llama3-8b-instruct-epp</code> on port <code>9002</code> for making routing decisions. If EPP fails to pick an endpoint, or is not responsive, the request will be dropped.</li> <li>Traffic routed to this InferencePool will be forwarded to the port <code>8000</code> on the selected Pods.</li> </ul>"},{"location":"api-types/inferencepool/#overlap-with-service","title":"Overlap with Service","text":"<p>InferencePool has some small overlap with Service, displayed here:</p> <p></p> <p>The InferencePool is not intended to be a mask of the Service object. It provides a specialized abstraction tailored for managing and routing traffic to groups of LLM model servers, allowing Platform Admins to focus on pool-level management rather than low-level networking details.</p>"},{"location":"api-types/inferencepool/#replacing-an-inferencepool","title":"Replacing an InferencePool","text":"<p>Please refer to the Replacing an InferencePool guide for details on uses cases and how to replace an InferencePool.</p>"},{"location":"concepts/api-overview/","title":"API Overview","text":""},{"location":"concepts/api-overview/#background","title":"Background","text":"<p>The Gateway API Inference Extension project is an extension of the Kubernetes Gateway API for serving Generative AI models on Kubernetes. Gateway API Inference Extension facilitates standardization of APIs for Kubernetes cluster operators and developers running generative AI inference, while allowing flexibility for underlying gateway implementations (such as Envoy Proxy) to iterate on mechanisms for optimized serving of models.</p> <p></p>"},{"location":"concepts/api-overview/#api-resources","title":"API Resources","text":""},{"location":"concepts/api-overview/#inferencepool","title":"InferencePool","text":"<p>InferencePool represents a set of Inference-focused Pods and an extension that will be used to route to them. Within the broader Gateway API resource model, this resource is considered a \"backend\". In practice, that means that you'd replace a Kubernetes Service with an InferencePool. This resource has some similarities to Service (a way to select Pods and specify a port), but has some unique capabilities. With InferencePool, you can configure a routing extension as well as inference-specific routing optimizations. For more information on this resource, refer to our InferencePool documentation or go directly to the InferencePool spec.</p>"},{"location":"concepts/api-overview/#inferencemodel","title":"InferenceModel","text":"<p>An InferenceModel represents a model or adapter, and configuration associated with that model. This resource enables you to configure the relative criticality of a model, and allows you to seamlessly translate the requested model name to one or more backend model names. Multiple InferenceModels can be attached to an InferencePool. For more information on this resource, refer to our InferenceModel documentation or go directly to the InferenceModel spec.</p>"},{"location":"concepts/conformance/","title":"Conformance","text":"<p>Similar to Gateway API, this project will rely on conformance tests to ensure compatibility across implementations. This will be focused on three different layers:</p>"},{"location":"concepts/conformance/#1-gateway-api-implementations","title":"1. Gateway API Implementations","text":"<p>Conformance tests will verify that:</p> <ul> <li>InferencePool is supported as a backend type</li> <li>Implementations forward requests to the configured extension for an   InferencePool following the specification defined by this project</li> <li>Implementations honor the routing guidance provided by the extension</li> <li>Implementations behave appropriately when an extension is either not present   or fails to respond</li> </ul>"},{"location":"concepts/conformance/#2-inference-routing-extensions","title":"2. Inference Routing Extensions","text":"<p>Conformance tests will verify that:</p> <ul> <li>Extensions accept requests that match the protocol specified by this project</li> <li>Extensions respond with routing guidance that matches the protocol specified   by this project</li> </ul>"},{"location":"concepts/conformance/#3-model-server-frameworks","title":"3. Model Server Frameworks","text":"<p>Conformance tests will verify that:</p> <ul> <li>Frameworks serve the expected set of metrics using a format and path specified   by this project</li> </ul>"},{"location":"concepts/roles-and-personas/","title":"Roles and Personas","text":"<p>Before diving into the details of the API, descriptions of the personas these APIs were designed for will help convey the thought process of the API design.</p>"},{"location":"concepts/roles-and-personas/#inference-platform-admin","title":"Inference Platform Admin","text":"<p>The Inference Platform Admin creates and manages the infrastructure necessary to run LLM workloads, including handling Ops for:</p> <ul> <li>Hardware</li> <li>Model Server</li> <li>Base Model</li> <li>Resource Allocation for Workloads</li> <li>Gateway configuration</li> <li>etc</li> </ul>"},{"location":"concepts/roles-and-personas/#inference-workload-owner","title":"Inference Workload Owner","text":"<p>An Inference Workload Owner persona owns and manages one or many Generative AI Workloads (LLM focused currently). This includes:</p> <ul> <li>Defining criticality</li> <li>Managing fine-tunes</li> <li>LoRA Adapters</li> <li>System Prompts</li> <li>Prompt Cache</li> <li>etc.</li> <li>Managing rollout of adapters</li> </ul>"},{"location":"contributing/","title":"How to Get Involved","text":"<p>This page contains links to all of the meeting notes, design docs and related discussions around the APIs.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Bug reports should be filed as GitHub Issues on this repo.</p> <p>NOTE: If you're reporting a bug that applies to a specific implementation of this project, please check our implementations page to find links to the repositories where you can get help with your specific implementation.</p>"},{"location":"contributing/#communications","title":"Communications","text":"<p>Major discussions and notifications will be sent on both the WG-Serving and SIG-Network mailing lists.</p> <p>Although we may end up creating a new Slack channel in the future, our conversations are currently split between the following Kubernetes Slack channels:</p> <ul> <li>#sig-network-gateway-api</li> <li>#wg-serving</li> </ul>"},{"location":"contributing/#meetings","title":"Meetings","text":"<p>Gateway API community meetings happen every Thursday at 10am Pacific Time (convert to your timezone). To receive an invite to this and other WG-Serving community meetings, join the WG-Serving mailing list.</p> <ul> <li>Zoom link (passcode in meeting notes doc)</li> </ul>"},{"location":"contributing/#meeting-notes-and-recordings","title":"Meeting Notes and Recordings","text":"<p>Meeting agendas and notes are maintained in the meeting notes doc. Feel free to add topics for discussion at an upcoming meeting.</p> <p>All meetings are recorded and automatically uploaded to the [WG-Serving meetings YouTube playlist][https://www.youtube.com/playlist?list=PL69nYSiGNLP30qNanabU75ayPK7OPNAAS].</p>"},{"location":"contributing/devguide/","title":"Developer Guide","text":"<p>TODO</p>"},{"location":"contributing/docs/","title":"Contributing to the docs","text":"<p>This doc site is built using MkDocs. It includes a Docker image for you to preview local changes without needing to set up MkDocs and its related plug-ins.</p> <p>Branch sources of the docs content:</p> <ul> <li><code>main</code> branch for <code>main</code> version</li> <li><code>release-MAJOR.MINOR</code> branches such as <code>release-0.1</code> for <code>0.1</code> version</li> <li><code>docs</code> branch for the versioned directories that are published via Netlify to the website</li> </ul>"},{"location":"contributing/docs/#preview-local-changes","title":"Preview local changes","text":"<ol> <li> <p>In the <code>site-src</code> directory, make your changes to the Markdown files.</p> </li> <li> <p>If you add a new page, make sure to update the <code>nav</code> section in the <code>mkdocs.yml</code> file.</p> </li> <li> <p>From the root directory of this project, run the Docker image with the following command from the <code>Makefile</code>.    <pre><code>make live-docs\n</code></pre></p> </li> <li> <p>Open your browser to preview the local build, http://localhost:3000.</p> </li> </ol> <p>One preview at a time</p> <p>For better performance, open one localhost preview at a time. If you have multiple browsers rendering <code>localhost:3000</code>, you might notice a lag time in loading pages.</p>"},{"location":"contributing/docs/#style-guides","title":"Style guides","text":"<p>Refer to the following style guides:</p> <ul> <li>Gateway API</li> <li>Kubernetes</li> </ul> <p>If you need guidance on specific words that are not covered in one of those style guides, check a common cloud provider, such as Google developer docs.</p>"},{"location":"contributing/docs/#version-the-docs","title":"Version the docs","text":"<p>The Material theme uses <code>mike</code> to version the docs. </p>"},{"location":"contributing/docs/#automatic-versioning-for-releases","title":"Automatic versioning for releases","text":"<p>The <code>make docs</code> target in the Makefile runs the <code>hack/mkdocs/make-docs.sh</code> script. This script runs <code>mike</code> to version the docs based on the current branch. It works for <code>main</code> and major/minor release branches such as <code>release-0.1</code>.</p>"},{"location":"contributing/docs/#update-versioned-docs","title":"Update versioned docs","text":"<p>For main or release branches such as <code>release-0.1</code>, you can update doc content as follows:</p> <ol> <li>Check out the main or release branch.</li> <li>Make changes to the markdown files in the <code>site-src</code> directory.</li> <li>Run <code>make docs</code> to build the docs and push the changes to the <code>gh-pages</code> branch.</li> <li>Netlify gets triggered automatically and publishes the changes to the website.</li> </ol>"},{"location":"contributing/docs/#manual-versioning","title":"Manual versioning","text":"<p>Sometimes, you might need to manually update a doc version. For example, you might want to delete an old LTS version that is no longer needed.</p> <p>The following steps cover common workflows for versioning. For more information, see the following resources:</p> <ul> <li>Material theme versioning page</li> <li><code>mike</code> readme</li> </ul> <p>Example workflow for using <code>mike</code>:</p> <ol> <li> <p>List the current versions. Aliases are included in brackets.    <pre><code>mike list\n\n# Example output\n0.3 [main]\n0.2 [latest]\n0.1\n</code></pre></p> </li> <li> <p>Check out the branch that you want to build the docs from.</p> </li> <li> <p>In the <code>site-src</code> directory, make and save your doc changes.</p> </li> <li> <p>Add the changes to the versions that you want to publish them in. If the version has an alias such as latest, you can include that. Make sure to include the <code>--branch docs</code> flag, so as not to publish docs to the <code>mike</code> default <code>gh-pages</code> branch.    <pre><code>mike deploy --push --branch docs main\nmike deploy --push --update-aliases 0.4 --branch docs latest\n</code></pre></p> </li> <li> <p>Delete an old version of the docs that you no longer need. The following example adds a new version 0.4 as main based on the current content, renames 0.3 to latest with the current content, removes the latest alias from 0.2 but leaves the version content untouched, and deletes version 0.1.    <pre><code>mike delete 0.1\n</code></pre></p> </li> </ol>"},{"location":"contributing/docs/#how-versioning-works","title":"How versioning works","text":"<p>The <code>mike</code> commands add each version as a separate commit and directory on the <code>docs</code> branch. </p> <ul> <li>The versioned directories contain the output of the MkDocs build for each version. </li> <li>The <code>latest</code> and <code>main</code> aliases are copies of the versioned directories.</li> <li>The <code>versions.json</code> file has the information for each version and alias that <code>mike</code> tracks. You can check this if you use </li> </ul> <p>Example directory structure:</p> <pre><code>'docs' branch\n\u2502\u2500\u2500 0.1/\n\u2502\u2500\u2500 0.2/\n\u2502\u2500\u2500 0.3/\n\u2502\u2500\u2500 0.4/\n\u2502\u2500\u2500 latest/\n\u2502\u2500\u2500 main/\n\u2502\u2500\u2500 versions.json\n</code></pre> <p>The doc builds then publish the versioned content from this branch to the website.</p>"},{"location":"contributing/docs/#develop-the-mkdocs-theme","title":"Develop the MkDocs theme","text":"<p>As you contribute to the Kubernetes Gateway API Inference Extension project, you might want to add features to the MkDocs theme or build process.</p> <p>Helpful resources:</p> <ul> <li>Customization, extensions, and overrides</li> <li>Setup features</li> <li>Plugins</li> </ul> <p>General steps:</p> <ol> <li> <p>Set up a virtual environment with python, pip, mkdocs, and the plugins that this project uses.    <pre><code>make virtualenv\n</code></pre></p> </li> <li> <p>Try out the MkDocs Material theme features, plugins, or other customizations that you want to add locally.</p> </li> <li> <p>For plugins, add the plugin to the <code>/hack/mkdocs/image/requirements.txt</code> file.</p> </li> <li> <p>From the root directory, run the Docker image of the docs. Make sure that your changes build and works as you expect.    <pre><code>make live-docs\n</code></pre></p> </li> </ol>"},{"location":"contributing/docs/#publish-the-docs","title":"Publish the docs","text":"<p>The project uses Netlify to host the docs. </p> <p>You can locally build the files that become the doc site. For example, you might want to check the HTML output of changes that you make to the site.</p> <pre><code>make build-docs-netlify\n</code></pre> <p>The Gateway API Inference Extension team will publish the docs based on the latest changes in the <code>main</code> branch.</p>"},{"location":"gieps/overview/","title":"Gateway Inference Enhancement Proposal (GIEP)","text":"<p>Gateway Inference Enhancement Proposals (GIEPs) serve a similar purpose to the GIEP process for the main Gateway API project:</p> <ol> <li>Ensure that changes to the API follow a known process and discussion in the   OSS community.</li> <li>Make changes and proposals discoverable (current and future).</li> <li>Document design ideas, tradeoffs, decisions that were made for historical   reference.</li> <li>Record the results of larger community discussions.</li> <li>Record changes to the GIEP process itself.</li> </ol>"},{"location":"gieps/overview/#process","title":"Process","text":"<p>This diagram shows the state diagram of the GIEP process at a high level, but the details are below.</p> <pre><code>flowchart TD\n    D([Discuss with&lt;br /&gt;the community]) --&gt; C\n    C([Issue Created]) -------&gt; Memorandum\n    C([Issue Created]) --&gt; Provisional\n    Provisional --&gt;|If practical &lt;br /&gt; work needed| Prototyping\n    Provisional --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Prototyping --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Implementable --&gt;|Gateway API&lt;br /&gt;work completed| Experimental\n    Experimental --&gt;|Supported in&lt;br /&gt;multiple implementations&lt;br /&gt;+ Conformance tests| Standard\n    Standard --&gt;|Entire change is GA or implemented| Completed</code></pre>"},{"location":"gieps/overview/#giep-definitions","title":"GIEP Definitions","text":""},{"location":"gieps/overview/#giep-states","title":"GIEP States","text":"<p>Each GIEP has a state, which tracks where it is in the GIEP process.</p> <p>GIEPs can move to some states from any other state:</p> <ul> <li>Declined: The GIEP has been declined and further work will not occur.</li> <li>Deferred: We do not currently have bandwidth to handle this GIEP, it may     be revisited in the future.</li> <li>Declined: This proposal was considered by the community but ultimately   rejected.</li> <li>Withdrawn: This proposal was considered by the community but ultimately   withdrawn by the author.</li> </ul> <p>There is a special state to cover Memorandum GIEPs:</p> <ul> <li>Memorandum: These GIEPs either:<ul> <li>Document an agreement for further work, creating no spec changes   themselves, or</li> <li>Update the GIEP process.</li> </ul> </li> </ul> <p>API GIEPs flow through a number of states, which generally correspond to the level of stability of the change described in the GIEP:</p> <ul> <li>Provisional: The goals described by this GIEP have consensus but     implementation details have not been agreed to yet.</li> <li>Prototyping: An extension of <code>Provisional</code> which can be opted in to in     order to indicate to the community that there are some active practical     tests and experiments going on which are intended to be a part of the     development of this GIEP. This may include APIs or code, but that content     must not be distributed with releases.</li> <li>Implementable: The goals and implementation details described by this     GIEP have consensus but have not been fully implemented yet.</li> <li>Experimental: This GIEP has been implemented and is part of the     \"Experimental\" release channel. Breaking changes are still possible, up to     and including complete removal and moving to <code>Rejected</code>.</li> <li>Standard: This GIEP has been implemented and is part of the \"Standard\"     release channel. It should be quite stable.</li> <li>Completed: All implementation work on this API GIEP has been completed.</li> </ul>"},{"location":"gieps/overview/#relationships-between-gieps","title":"Relationships between GIEPs","text":"<p>GIEPs can have relationships between them. At this time, there are three possible relationships:</p> <ul> <li>Obsoletes and its backreference ObsoletedBy: when a GIEP is made   obsolete by another GIEP, and has its functionality completely replaced. The   Obsoleted GIEP is moved to the Declined state.</li> <li>Extends and its backreference ExtendedBy: when a GIEP has additional   details or implementation added in another GIEP.</li> <li>SeeAlso: when a GIEP is relevant to another GIEP, but is not affected in   any other defined way.</li> </ul> <p>Relationships are tracked in the YAML metadata files accompanying each GIEP.</p>"},{"location":"gieps/overview/#giep-metadata-file","title":"GIEP metadata file","text":"<p>Each GIEP has a YAML file containing metadata alongside it, please keep it up to date as changes to the GIEP occur.</p> <p>In particular, note the <code>authors</code>, and <code>changelog</code> fields, please keep those up to date.</p>"},{"location":"gieps/overview/#process_1","title":"Process","text":""},{"location":"gieps/overview/#1-discuss-with-the-community","title":"1. Discuss with the community","text":"<p>Before creating a GIEP, share your high level idea with the community. There are several places this may be done:</p> <ul> <li>A new GitHub   Discussion</li> <li>On our Slack Channel</li> <li>On one of our community   meetings</li> </ul> <p>Please default to GitHub discussions: they work a lot like GitHub issues which makes them easy to search.</p>"},{"location":"gieps/overview/#2-create-an-issue","title":"2. Create an Issue","text":"<p>Create a GIEP issue in the repo describing your change. At this point, you should copy the outcome of any other conversations or documents into this document.</p>"},{"location":"gieps/overview/#3-agree-on-the-goals","title":"3. Agree on the Goals","text":"<p>Although it can be tempting to start writing out all the details of your proposal, it's important to first ensure we all agree on the goals.</p> <p>For API GIEPs, the first version of your GIEP should aim for a \"Provisional\" status and leave out any implementation details, focusing primarily on \"Goals\" and \"Non-Goals\".</p> <p>For Memorandum GIEPs, the first version of your GIEP will be the only one, as Memorandums have only a single stage - <code>Accepted</code>.</p>"},{"location":"gieps/overview/#3-document-implementation-details","title":"3. Document Implementation Details","text":"<p>Now that everyone agrees on the goals, it is time to start writing out your proposed implementation details. These implementation details should be very thorough, including the proposed API spec, and covering any relevant edge cases. Note that it may be helpful to use a shared doc for part of this phase to enable faster iteration on potential designs.</p> <p>It is likely that throughout this process, you will discuss a variety of alternatives. Be sure to document all of these in the GIEP, and why we decided against them. At this stage, the GIEP should be targeting the \"Implementable\" stage.</p>"},{"location":"gieps/overview/#4-implement-the-giep-as-experimental","title":"4. Implement the GIEP as \"Experimental\"","text":"<p>With the GIEP marked as \"Implementable\", it is time to actually make those proposed changes in our API. In some cases, these changes will be documentation only, but in most cases, some API changes will also be required. It is important that every new feature of the API is marked as \"Experimental\" when it is introduced. Within the API, we use <code>&lt;gateway:experimental&gt;</code> tags to denote experimental fields. Within Golang packages (conformance tests, CLIs, e.t.c.) we use the <code>experimental</code> Golang build tag to denote experimental functionality.</p> <p>Some other requirements must be met before marking a GIEP <code>Experimental</code>:</p> <ul> <li>the graduation criteria to reach <code>Standard</code> MUST be filled out</li> <li>a proposed probationary period (see next section) must be included in the GIEP   and approved by maintainers.</li> </ul> <p>Before changes are released they MUST be documented. GIEPs that have not been both implemented and documented before a release cut off will be excluded from the release.</p>"},{"location":"gieps/overview/#probationary-period","title":"Probationary Period","text":"<p>Any GIEP in the <code>Experimental</code> phase is automatically under a \"probationary period\" where it will come up for re-assessment if its graduation criteria are not met within a given time period. GIEPs that wish to move into <code>Experimental</code> status MUST document a proposed period (6 months is the suggested default) that MUST be approved by maintainers. Maintainers MAY select an alternative time duration for a probationary period if deemed appropriate, and will document their reasoning.</p> <p>Rationale: This probationary period exists to avoid GIEPs getting \"stale\" and to provide guidance to implementations about how relevant features should be used, given that they are not guaranteed to become supported.</p> <p>At the end of a probationary period if the GIEP has not been able to resolve its graduation criteria it will move to \"Rejected\" status. In extenuating circumstances an extension of that period may be accepted by approval from maintainers. GIEPs which are <code>Rejected</code> in this way are removed from the experimental CRDs and more or less put on hold. GIEPs may be allowed to move back into <code>Experimental</code> status from <code>Rejected</code> for another probationary period if a new strategy for achieving their graduation criteria can be established. Any such plan to take a GIEP \"off the shelf\" must be reviewed and accepted by the maintainers.</p> <p>Warning: It is extremely important** that projects which implement <code>Experimental</code> features clearly document that these features may be removed in future releases.</p>"},{"location":"gieps/overview/#5-graduate-the-giep-to-standard","title":"5. Graduate the GIEP to \"Standard\"","text":"<p>Once this feature has met the graduation criteria, it is time to graduate it to the \"Standard\" channel of the API. Depending on the feature, this may include any of the following:</p> <ol> <li>Graduating the resource to beta</li> <li>Graduating fields to \"standard\" by removing <code>&lt;gateway:experimental&gt;</code> tags</li> <li>Graduating a concept to \"standard\" by updating documentation</li> </ol>"},{"location":"gieps/overview/#6-close-out-the-giep-issue","title":"6. Close out the GIEP issue","text":"<p>The GIEP issue should only be closed once the feature has: - Moved to the standard channel for distribution (if necessary) - Moved to a \"v1\" <code>apiVersion</code> for CRDs - been completely implemented and has wide acceptance (for process changes).</p> <p>In short, the GIEP issue should only be closed when the work is \"done\" (whatever that means for that GIEP).</p>"},{"location":"gieps/overview/#format","title":"Format","text":"<p>GIEPs should match the format of the template found in GIEP-696.</p>"},{"location":"gieps/overview/#out-of-scope","title":"Out of scope","text":"<p>What is out of scope: see text from KEP. Examples:</p> <ul> <li>Bug fixes</li> <li>Small changes (API validation, documentation, fixups). It is always possible   that the reviewers will determine a \"small\" change ends up requiring a GIEP.</li> </ul>"},{"location":"gieps/overview/#faq","title":"FAQ","text":""},{"location":"gieps/overview/#why-is-it-named-giep","title":"Why is it named GIEP?","text":"<p>To avoid potential confusion if people start following the cross references to the full GEP or KEP process.</p>"},{"location":"gieps/overview/#why-have-a-different-process-than-mainline","title":"Why have a different process than mainline?","text":"<p>Gateway API has some differences with most upstream KEPs. Notably Gateway API intentionally avoids including any implementation with the project, so this process is focused entirely on the substance of the API. As this project is based on CRDs it also has an entirely separately release process, and has developed concepts like \"release channels\" that do not exist in upstream.</p>"},{"location":"gieps/overview/#is-it-ok-to-discuss-using-shared-docs-scratch-docs-etc","title":"Is it ok to discuss using shared docs, scratch docs etc?","text":"<p>Yes, this can be a helpful intermediate step when iterating on design details. It is important that all major feedback, discussions, and alternatives considered in that step are represented in the GIEP though. A key goal of GIEPs is to show why we made a decision and which alternatives were considered. If separate docs are used, it's important that we can still see all relevant context and decisions in the final GIEP.</p>"},{"location":"gieps/overview/#when-should-i-mark-a-giep-as-prototyping-as-opposed-to-provisional","title":"When should I mark a GIEP as <code>Prototyping</code> as opposed to <code>Provisional</code>?","text":"<p>The <code>Prototyping</code> status carries the same base meaning as <code>Provisional</code> in that consensus is not complete between stakeholders and we're not ready to move toward releasing content yet. You should use <code>Prototyping</code> to indicate to your fellow community members that we're in a state of active practical tests and experiments which are intended to help us learn and iterate on the GIEP. These can include distributing content, but not under any release channel.</p>"},{"location":"gieps/overview/#should-i-implement-support-for-experimental-channel-features","title":"Should I implement support for <code>Experimental</code> channel features?","text":"<p>Ultimately one of the main ways to get something into <code>Standard</code> is for it to mature through the <code>Experimental</code> phase, so we really need people to implement these features and provide feedback in order to have progress. That said, the graduation of a feature past <code>Experimental</code> is not a forgone conclusion. Before implementing an experimental feature, you should:</p> <ul> <li>Clearly document that support for the feature is experimental and may   disappear in the future.</li> <li>Have a plan in place for how you would handle the removal of this feature from   the API.</li> </ul>"},{"location":"gieps/giep-116/","title":"GIEP-116: GIEP template","text":"<ul> <li>Issue: #0</li> <li>Status: Provisional|Implementable|Experimental|Standard|Deferred|Rejected|Withdrawn|Replaced</li> </ul> <p>(See status definitions here.)</p>"},{"location":"gieps/giep-116/#tldr","title":"TLDR","text":"<p>(1-2 sentence summary of the proposal)</p>"},{"location":"gieps/giep-116/#goals","title":"Goals","text":"<p>(Primary goals of this proposal.)</p>"},{"location":"gieps/giep-116/#non-goals","title":"Non-Goals","text":"<p>(What is out of scope for this proposal.)</p>"},{"location":"gieps/giep-116/#introduction","title":"Introduction","text":"<p>(Can link to external doc -- but we should bias towards copying the content into the GEP as online documents are easier to lose -- e.g. owner messes up the permissions, accidental deletion)</p>"},{"location":"gieps/giep-116/#api","title":"API","text":"<p>(... details, can point to PR with changes)</p>"},{"location":"gieps/giep-116/#conformance-details","title":"Conformance Details","text":"<p>(This section describes the names to be used for the feature or features in conformance tests and profiles.</p> <p>These should be <code>CamelCase</code> names that specify the feature as precisely as possible, and are particularly important for Extended features, since they may be surfaced to users.)</p>"},{"location":"gieps/giep-116/#alternatives","title":"Alternatives","text":"<p>(List other design alternatives and why we did not go in that direction)</p>"},{"location":"gieps/giep-116/#references","title":"References","text":"<p>(Add any additional document links. Again, we should try to avoid too much content not in version control to avoid broken links)</p>"},{"location":"guides/","title":"Getting started with Gateway API Inference Extension","text":"Experimental <p>This project is still in an alpha state and breaking changes may occur in the future.</p> <p>This quickstart guide is intended for engineers familiar with k8s and model servers (vLLM in this instance). The goal of this guide is to get an Inference Gateway up and running! </p>"},{"location":"guides/#prerequisites","title":"Prerequisites","text":"<ul> <li>A cluster with:</li> <li>Support for services of type <code>LoadBalancer</code>. For kind clusters, follow this guide   to get services of type LoadBalancer working.</li> <li>Support for sidecar containers (enabled by default since Kubernetes v1.29)   to run the model server deployment.</li> </ul>"},{"location":"guides/#steps","title":"Steps","text":""},{"location":"guides/#deploy-sample-model-server","title":"Deploy Sample Model Server","text":"<p>Two options are supported for running the model server:</p> <ol> <li> <p>GPU-based model server.       Requirements: a Hugging Face access token that grants access to the model meta-llama/Llama-3.1-8B-Instruct.</p> </li> <li> <p>CPU-based model server (not using GPUs).       The sample uses the model Qwen/Qwen2.5-1.5B-Instruct.  </p> </li> </ol> <p>Choose one of these options and follow the steps below. Please do not deploy both, as the deployments have the same name and will override each other.</p> GPU-Based Model ServerCPU-Based Model Server <p>For this setup, you will need 3 GPUs to run the sample model server. Adjust the number of replicas in <code>./config/manifests/vllm/gpu-deployment.yaml</code> as needed.   Create a Hugging Face secret to download the model meta-llama/Llama-3.1-8B-Instruct. Ensure that the token grants access to this model.</p> <p>Deploy a sample vLLM deployment with the proper protocol to work with the LLM Instance Gateway.   <pre><code>kubectl create secret generic hf-token --from-literal=token=$HF_TOKEN # Your Hugging Face Token with access to the set of Llama models\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml\n</code></pre></p> <p>This setup is using the formal <code>vllm-cpu</code> image, which according to the documentation can run vLLM on x86 CPU platform.   For this setup, we use approximately 9.5GB of memory and 12 CPUs for each replica.  </p> <p>While it is possible to deploy the model server with less resources, this is not recommended. For example, in our tests, loading the model using 8GB of memory and 1 CPU was possible but took almost 3.5 minutes and inference requests took unreasonable time. In general, there is a tradeoff between the memory and CPU we allocate to our pods and the performance. The more memory and CPU we allocate the better performance we can get.</p> <p>After running multiple configurations of these values we decided in this sample to use 9.5GB of memory and 12 CPUs for each replica, which gives reasonable response times. You can increase those numbers and potentially may even get better response times. For modifying the allocated resources, adjust the numbers in cpu-deployment.yaml as needed.  </p> <p>Deploy a sample vLLM deployment with the proper protocol to work with the LLM Instance Gateway.   <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml\n</code></pre></p>"},{"location":"guides/#install-the-inference-extension-crds","title":"Install the Inference Extension CRDs","text":"Latest ReleaseDev Version <pre><code>VERSION=v0.3.0\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/$VERSION/manifests.yaml\n</code></pre> <pre><code>kubectl apply -k https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd\n</code></pre>"},{"location":"guides/#deploy-inferencemodel","title":"Deploy InferenceModel","text":"<p>Deploy the sample InferenceModel which is configured to forward traffic to the <code>food-review-1</code> LoRA adapter of the sample model server.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml\n</code></pre>"},{"location":"guides/#deploy-the-inferencepool-and-endpoint-picker-extension","title":"Deploy the InferencePool and Endpoint Picker Extension","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml\n</code></pre>"},{"location":"guides/#deploy-inference-gateway","title":"Deploy Inference Gateway","text":"<p>Choose one of the following options to deploy an Inference Gateway.</p> GKEIstioKgateway <ol> <li> <p>Enable the Gateway API and configure proxy-only subnets when necessary. See Deploy Gateways    for detailed instructions.</p> </li> <li> <p>Deploy Gateway and HealthCheckPolicy resources</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gateway.yaml\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   inference-gateway   &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> <li> <p>Given that the default connection timeout may be insufficient for most inference workloads, it is recommended to configure a timeout appropriate for your intended use case.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml\n</code></pre> </li> </ol> <p>Please note that this feature is currently in an experimental phase and is not intended for production use.    The implementation and user experience are subject to changes as we continue to iterate on this project.</p> <ol> <li> <p>Requirements</p> <ul> <li>Gateway API CRDs installed.</li> </ul> </li> <li> <p>Install Istio</p> <pre><code>TAG=1.26-alpha.9befed2f1439d883120f8de70fd70d84ca0ebc3d\n# on Linux\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-linux-amd64.tar.gz\ntar -xvf istioctl-$TAG-linux-amd64.tar.gz\n# on macOS\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-osx.tar.gz\ntar -xvf istioctl-$TAG-osx.tar.gz\n# on Windows\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-win.zip\nunzip istioctl-$TAG-win.zip\n\n./istioctl install --set tag=$TAG --set hub=gcr.io/istio-testing\n</code></pre> </li> <li> <p>If you run the Endpoint Picker (EPP) with the <code>--secureServing</code> flag set to <code>true</code> (the default mode), it is currently using a self-signed certificate. As a security measure, Istio does not trust self-signed certificates by default. As a temporary workaround, you can apply the destination rule to bypass TLS verification for EPP. A more secure TLS implementation in EPP is being discussed in Issue 582.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/destination-rule.yaml\n</code></pre> </li> <li> <p>Deploy Gateway</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/gateway.yaml\n</code></pre> </li> <li> <p>Label the gateway</p> <pre><code>kubectl label gateway llm-gateway istio.io/enable-inference-extproc=true\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   inference-gateway   &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> </ol> <p>Kgateway recently added support for inference extension as a technical preview. This means do not   run Kgateway with inference extension in production environments. Refer to Issue 10411   for the list of caveats, supported features, etc.</p> <ol> <li> <p>Requirements</p> <ul> <li>Helm installed.</li> <li>Gateway API CRDs installed.</li> </ul> </li> <li> <p>Set the Kgateway version and install the Kgateway CRDs.</p> <pre><code>KGTW_VERSION=v2.0.0\nhelm upgrade -i --create-namespace --namespace kgateway-system --version $KGTW_VERSION kgateway-crds oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds\n</code></pre> </li> <li> <p>Install Kgateway</p> <pre><code>helm upgrade -i --namespace kgateway-system --version $KGTW_VERSION kgateway oci://cr.kgateway.dev/kgateway-dev/charts/kgateway --set inferenceExtension.enabled=true\n</code></pre> </li> <li> <p>Deploy the Gateway</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/gateway.yaml\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   kgateway            &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> </ol>"},{"location":"guides/#try-it-out","title":"Try it out","text":"<p>Wait until the gateway is ready.</p> GPU-Based Model ServerCPU-Based Model Server <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\n\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre> <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\n\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre>"},{"location":"guides/#cleanup","title":"Cleanup","text":"<p>The following instructions assume you would like to cleanup ALL resources that were created in this quickstart guide.    Please be careful not to delete resources you'd like to keep.</p> <ol> <li> <p>Uninstall the InferencePool, InferenceModel, and model server resources</p> <pre><code>kubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml --ignore-not-found\nkubectl delete secret hf-token --ignore-not-found\n</code></pre> </li> <li> <p>Uninstall the Gateway API resources</p> <pre><code>kubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/httproute.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/destination-rule.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/httproute.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/httproute.yaml --ignore-not-found\n</code></pre> </li> <li> <p>Uninstall the Gateway API Inference Extension CRDs</p> <pre><code>kubectl delete -k https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd --ignore-not-found\n</code></pre> </li> <li> <p>Choose one of the following options to cleanup the Inference Gateway.</p> </li> </ol> GKEIstioKgateway <p>TODO</p> <p>TODO</p> <p>The following instructions assume you would like to cleanup ALL Kgateway resources that were created in this quickstart guide.</p> <ol> <li> <p>Uninstall Kgateway</p> <pre><code>helm uninstall kgateway -n kgateway-system\n</code></pre> </li> <li> <p>Uninstall the Kgateway CRDs.</p> <pre><code>helm uninstall kgateway-crds -n kgateway-system\n</code></pre> </li> <li> <p>Remove the Kgateway namespace.</p> <pre><code>kubectl delete ns kgateway-system\n</code></pre> </li> </ol>"},{"location":"guides/adapter-rollout/","title":"Adapter Rollout","text":"<p>The goal of this guide is to demonstrate how to rollout a new adapter version.</p>"},{"location":"guides/adapter-rollout/#prerequisites","title":"Prerequisites","text":"<p>Follow the steps in the main guide</p>"},{"location":"guides/adapter-rollout/#safely-rollout-v2-adapter","title":"Safely rollout v2 adapter","text":""},{"location":"guides/adapter-rollout/#load-the-new-adapter-version-to-the-model-servers","title":"Load the new adapter version to the model servers","text":"<p>This guide leverages the LoRA syncer sidecar to dynamically manage adapters within a vLLM deployment, enabling users to add or remove them through a shared ConfigMap.</p> <p>Modify the LoRA syncer ConfigMap to initiate loading of the new adapter version.</p> <pre><code>kubectl edit configmap vllm-llama3-8b-instruct-adapters\n</code></pre> <p>Change the ConfigMap to match the following (note the new entry under models):</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n</code></pre> <p>The new adapter version is applied to the model servers live, without requiring a restart.</p>"},{"location":"guides/adapter-rollout/#direct-traffic-to-the-new-adapter-version","title":"Direct traffic to the new adapter version","text":"<p>Modify the InferenceModel to configure a canary rollout with traffic splitting. In this example, 10% of traffic for food-review model will be sent to the new food-review-2 adapter.</p> <pre><code>kubectl edit inferencemodel food-review\n</code></pre> <p>Change the targetModels list in InferenceModel to match the following:</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-1\n    weight: 90\n  - name: food-review-2\n    weight: 10\n</code></pre> <p>The above configuration means one in every ten requests should be sent to the new version. Try it out:</p> <ol> <li> <p>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\n</code></pre></p> </li> <li> <p>Send a few requests as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></p> </li> </ol>"},{"location":"guides/adapter-rollout/#finish-the-rollout","title":"Finish the rollout","text":"<p>Modify the InferenceModel to direct 100% of the traffic to the latest version of the adapter.</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-2\n    weight: 100\n</code></pre> <p>Unload the older versions from the servers by updating the LoRA syncer ConfigMap to list the older version under the <code>ensureNotExist</code> list:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n      ensureNotExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n</code></pre> <p>With this, all requests should be served by the new adapter version.</p>"},{"location":"guides/implementers/","title":"Implementer's Guide","text":"<p>This guide is intended for developers looking to implement support for the InferencePool custom resources within their Gateway API controller. It outlines how InferencePool fits into the existing resource model, discusses implementation options, explains how to interact with extensions, and provides guidance on testing.</p>"},{"location":"guides/implementers/#inferencepool-as-a-gateway-backend","title":"InferencePool as a Gateway Backend","text":"<p>Before we dive into the implementation, let\u2019s recap how an InferencePool works. </p> <p></p> <p>InferencePool represents a set of Inference-focused Pods and an extension that will be used to route to them. The InferencePool introduces a new type of backend within the Gateway API resource model. Instead of targeting Services, a Gateway can route traffic to an InferencePool. This InferencePool then becomes responsible for intelligent routing to the underlying model server pods based on the associated InferenceModel configurations. </p> <p>Here is an example of how to route traffic to an InferencePool using an HTTPRoute: <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    name: inference-gateway\n  rules:\n  - backendRefs:\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: base-model\n    matches:\n    - path:\n        type: PathPrefix\n        value: /\n</code></pre></p> <p>Note that the <code>rules.backendRefs</code> describes which InferencePool should receive the forwarded traffic when the path matches the corresponding path prefix. This is very similar to how we configure a Gateway with an HTTPRoute that directs traffic to a Service (a way to select Pods and specify a port). By using the InferencePool, it provides an abstraction over a set of compute resources (model server pods), and allows the controller to implement specialized routing strategies for these inference workloads.</p>"},{"location":"guides/implementers/#building-the-gateway-controller","title":"Building the Gateway controller","text":"<p>The general idea of implementing a Gateway controller supporting the InferencePool involves two major steps: </p> <ol> <li>Tracking the endpoints for InferencePool backends </li> <li>Callout to an extension to make intelligent routing decisions</li> </ol>"},{"location":"guides/implementers/#endpoint-tracking","title":"Endpoint Tracking","text":"<p>Consider a simple inference pool like this: <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp\n</code></pre></p> <p>There are mainly two options for how to treat the Inference Pool in your controller.</p> <p>Option 1: Shadow Service Creation</p> <p>If your Gateway controller already handles Service as a backend, you can choose to create a headless Service that mirrors the endpoints defined by the InferencePool, like this:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata: \n  name: vllm-llama3-8b-instruct-shadow-service\nspec:\n  ports:\n  - port: 54321\n    protocol: TCP\n    targetPort: 8000\n  selector:\n    app:  vllm-llama3-8b-instruct\n  type: ClusterIP\n  clusterIP: None\n</code></pre> <p>The gateway controller would then treat this shadow service just like any other backend service it routes traffic to. </p> <p>This approach likely allows you to leverage existing service discovery, healthcheck infrastructure, and load balancing mechanisms that your controller already supports. However, it does come with the overhead of managing additional Service objects, and hence may affect the overall latency of the reconciliation of the Gateways.</p> <p>Option 2: Tracking InferencePool Endpoints Separately</p> <p>You can also choose to directly select and monitor the endpoints belonging to the InferencePool. For the simple inference pool example we have above, the controller would use the label <code>app: vllm-llama3-8b-instruct</code> to discover the pods matching the criteria, and get their endpoints (i.e. IP and port number). It would then need to monitor these pods for health and availability. </p> <p>With this approach, you can tailor the endpoint tracking and routing logic specifically to the characteristics and requirements of your InferencePool.</p>"},{"location":"guides/implementers/#callout-extension","title":"Callout Extension","text":"<p>The Endpoint Picker, or EPP, is a core component of the inference extension. The primary interaction for routing requests is defined between the proxy (e.g., Envoy) and the EPP using the Envoy external processing service protocol. See the Endpoint Picker Protocol for more information.</p>"},{"location":"guides/implementers/#how-to-callout-to-epp","title":"How to Callout to EPP","text":"<p>For each HTTP request, the proxy CAN communicate the subset of endpoints the EPP MUST pick from by setting <code>x-gateway-destination-endpoint-subset</code> key in the filter metadata field of the ext-proc request. If this key is set, the EPP must select from this endpoint list. If the list is empty or no endpoints are eligible, it should return a 503 error. If the key isn't set, the EPP selects from the endpoints defined by the InferencePool selector.</p>"},{"location":"guides/implementers/#response-from-the-extension","title":"Response from the extension","text":"<p>The EPP communicates the chosen endpoint to the proxy via the <code>x-gateway-destination-endpoint</code> HTTP header and the <code>dynamic_metadata</code> field of the ext-proc response. Failure to communicate the endpoint using both methods results in a 503 error if no endpoints are ready, or a 429 error if the request should be dropped. The header and metadata values must match. In addition to the chosen endpoint, a single fallback endpoint CAN be set using the key <code>x-gateway-destination-endpoint-fallback</code> in the same metadata namespace as one used for <code>x-gateway-destination-endpoint</code>.</p>"},{"location":"guides/implementers/#testing-tips","title":"Testing Tips","text":"<p>Here are some tips for testing your controller end-to-end:</p> <ul> <li>Focus on Key Scenarios: Add common scenarios like creating, updating, and deleting InferencePool resources, as well as different routing rules that target InferencePool backends.</li> <li>Verify Routing Behaviors: Design more complex routing scenarios and verify that requests are correctly routed to the appropriate model server pods within the InferencePool based on the InferenceModel configuration.</li> <li>Test Error Handling: Verify that the controller correctly handles scenarios like unsupported model names or resource constraints (if criticality-based shedding is implemented). Test with state transitions (such as constant requests while Pods behind EPP are being replaced and Pods behind InferencePool are being replaced) to ensure that the system is resilient to failures and can automatically recover by redirecting traffic to healthy Pods.</li> <li>Using Reference EPP Implementation + Echoserver: You can use the reference EPP implementation for testing your controller end-to-end. Instead of a full-fledged model server, a simple mock server (like the echoserver) can be very useful for verifying routing to ensure the correct pod received the request. </li> <li>Performance Test: Run end-to-end benchmarks to make sure that your inference gateway can achieve the latency target that is desired.</li> </ul>"},{"location":"guides/implementers/#conformance-tests","title":"Conformance Tests","text":"<p>A set of conformance tests will be developed soon to help verify that a controller is working as expected. This guide will be updated once we have more information. Stay tuned!</p>"},{"location":"guides/metrics/","title":"Metrics","text":"<p>This guide describes the current state of exposed metrics and how to scrape them.</p>"},{"location":"guides/metrics/#requirements","title":"Requirements","text":"<p>To have response metrics, ensure the body mode is set to <code>Buffered</code> or <code>Streamed</code> (this should be the default behavior for all implementations).</p> <p>If you want to include usage metrics for vLLM model server streaming request, send the request with <code>include_usage</code>:</p> <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"whats your fav movie?\",\n\"max_tokens\": 10,\n\"temperature\": 0,\n\"stream\": true,\n\"stream_options\": {\"include_usage\": \"true\"}\n}'\n</code></pre>"},{"location":"guides/metrics/#exposed-metrics","title":"Exposed metrics","text":"Metric name Metric Type Description Labels Status inference_model_request_total Counter The counter of requests broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_error_total Counter The counter of requests errors broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_duration_seconds Distribution Distribution of response latency. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA normalized_time_per_output_token_seconds Distribution Distribution of ntpot (response latency per output token) <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_sizes Distribution Distribution of request size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_response_sizes Distribution Distribution of response size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_input_tokens Distribution Distribution of input token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_output_tokens Distribution Distribution of output token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_running_requests Gauge Number of running requests for each model. <code>model_name</code>=&lt;model-name&gt; ALPHA inference_pool_average_kv_cache_utilization Gauge The average kv cache utilization for an inference server pool. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_average_queue_size Gauge The average number of requests pending in the model server queue. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_per_pod_queue_size Gauge The total number of queue for each model server pod under the inference pool <code>model_server_pod</code>=&lt;model-server-pod-name&gt; <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_ready_pods Gauge The number of ready pods for an inference server pool. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_extension_info Gauge The general information of the current build. <code>commit</code>=&lt;hash-of-the-build&gt; ALPHA"},{"location":"guides/metrics/#scrape-metrics","title":"Scrape Metrics","text":"<p>Metrics endpoint is exposed at port 9090 by default. To scrape metrics, the client needs a ClusterRole with the following rule: <code>nonResourceURLs: \"/metrics\", verbs: get</code>.</p> <p>Here is one example if the client needs to mound the secret to act as the service account <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: inference-gateway-metrics-reader\nrules:\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: inference-gateway-sa-metrics-reader-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: inference-gateway-metrics-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: inference-gateway-sa-metrics-reader-secret\n  namespace: default\n  annotations:\n    kubernetes.io/service-account.name: inference-gateway-sa-metrics-reader\ntype: kubernetes.io/service-account-token\n</code></pre> Then, you can curl the 9090 port like following <pre><code>TOKEN=$(kubectl -n default get secret inference-gateway-sa-metrics-reader-secret  -o jsonpath='{.secrets[0].name}' -o jsonpath='{.data.token}' | base64 --decode)\n\nkubectl -n default port-forward inference-gateway-ext-proc-pod-name  9090\n\ncurl -H \"Authorization: Bearer $TOKEN\" localhost:9090/metrics\n</code></pre></p>"},{"location":"guides/replacing-inference-pool/","title":"Replacing an InferencePool","text":""},{"location":"guides/replacing-inference-pool/#background","title":"Background","text":"<p>Replacing an InferencePool is a powerful technique for performing various infrastructure and model updates with minimal disruption and built-in rollback capabilities. This method allows you to introduce changes incrementally, monitor their impact, and revert to the previous state if necessary. </p>"},{"location":"guides/replacing-inference-pool/#use-cases","title":"Use Cases","text":"<p>Use Cases for Replacing an InferencePool:</p> <ul> <li>Upgrading or replacing your model server framework</li> <li>Upgrading or replacing your base model</li> <li>Transitioning to new hardware</li> </ul>"},{"location":"guides/replacing-inference-pool/#how-to-replace-an-inferencepool","title":"How to replace an InferencePool","text":"<p>To replacing an InferencePool:</p> <ol> <li>Deploy new infrastructure: Create a new InferencePool configured with the new hardware / model server / base model that you chose.</li> <li>Configure traffic splitting: Use an HTTPRoute to split traffic between the existing InferencePool and the new InferencePool. The <code>backendRefs.weight</code> field controls the traffic percentage allocated to each pool.</li> <li>Maintain InferenceModel integrity: Keep your InferenceModel configuration unchanged. This ensures that the system applies the same LoRA adapters consistently across both base model versions.</li> <li>Preserve rollback capability: Retain the original nodes and InferencePool during the roll out to facilitate a rollback if necessary.</li> </ol>"},{"location":"guides/replacing-inference-pool/#example","title":"Example","text":"<p>You start with an existing lnferencePool named <code>llm-pool-v1</code>. To replace the original InferencePool, you create a new InferencePool named <code>llm-pool-v2</code>. By configuring an HTTPRoute, as shown below, you can incrementally split traffic between the original <code>llm-pool-v1</code> and new <code>llm-pool-v2</code>. </p> <ol> <li> <p>Save the following sample manifest as <code>httproute.yaml</code>:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    name: inference-gateway\n  rules:\n    backendRefs:\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: llm-pool-v1\n      weight: 90\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: llm-pool-v2\n      weight: 10\n</code></pre> </li> <li> <p>Apply the sample manifest to your cluster:</p> <pre><code>kubectl apply -f httproute.yaml\n</code></pre> <p>The original <code>llm-pool-v1</code> InferencePool receives most of the traffic, while the <code>llm-pool-v2</code> InferencePool receives the rest. </p> </li> <li> <p>Increase the traffic weight gradually for the <code>llm-pool-v2</code> InferencePool to complete the new InferencePool roll out.</p> </li> </ol>"},{"location":"implementations/gateways/","title":"Gateway Implementations","text":"<p>This project has several implementations that are planned or in progress:</p> <ul> <li>Envoy AI Gateway</li> <li>Kgateway</li> <li>Google Kubernetes Engine</li> <li>Istio</li> <li>Alibaba Cloud Container Service for Kubernetes</li> </ul>"},{"location":"implementations/gateways/#envoy-ai-gateway","title":"Envoy AI Gateway","text":"<p>Envoy AI Gateway is an open source project built on top of  Envoy and Envoy Gateway to handle request traffic  from application clients to GenAI services. The features and capabilities are outlined here. Use the quickstart to get Envoy AI Gateway running with Gateway API in a few simple steps.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#kgateway","title":"Kgateway","text":"<p>Kgateway is a feature-rich, Kubernetes-native ingress controller and next-generation API gateway. Kgateway brings the full power and community support of Gateway API to its existing control-plane implementation.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>Google Kubernetes Engine (GKE) is a managed Kubernetes platform offered by Google Cloud. GKE's implementation of the Gateway API is through the GKE Gateway controller which provisions Google Cloud Load Balancers for Pods in GKE clusters.</p> <p>The GKE Gateway controller supports weighted traffic splitting, mirroring, advanced routing, multi-cluster load balancing and more. See the docs to deploy private or public Gateways and also multi-cluster Gateways.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#istio","title":"Istio","text":"<p>Istio is an open source service mesh and gateway implementation. It provides a fully compliant implementation of the Kubernetes Gateway API for cluster ingress traffic control.  For service mesh users, Istio also fully supports east-west (including GAMMA) traffic management within the mesh.</p> <p>Gateway API Inference Extension support is being tracked by this GitHub Issue.</p>"},{"location":"implementations/gateways/#alibaba-cloud-container-service-for-kubernetes","title":"Alibaba Cloud Container Service for Kubernetes","text":"<p>Alibaba Cloud Container Service for Kubernetes (ACK) is a managed Kubernetes platform  offered by Alibaba Cloud. The implementation of the Gateway API in ACK is through the  ACK Gateway with Inference Extension component, which introduces model-aware,  GPU-efficient load balancing for AI workloads beyond basic HTTP routing.</p> <p>The ACK Gateway with Inference Extension implements the Gateway API Inference Extension  and provides optimized routing for serving generative AI workloads,  including weighted traffic splitting, mirroring, advanced routing, etc.  See the docs for the usage.</p> <p>Progress towards supporting Gateway API Inference Extension is being tracked  by this Issue.</p>"},{"location":"implementations/model-servers/","title":"Supported Model Servers","text":"<p>Any model server that conform to the model server protocol are supported by the inference extension.</p>"},{"location":"implementations/model-servers/#compatible-model-server-versions","title":"Compatible Model Server Versions","text":"Model Server Version Commit Notes vLLM V0 v0.6.4 and above commit 0ad216f vLLM V1 v0.8.0 and above commit bc32bc7 Triton(TensorRT-LLM) 25.03 and above commit 15cb989. LoRA affinity feature is not available as the required LoRA metrics haven't been implemented in Triton yet. Feature request"},{"location":"implementations/model-servers/#vllm","title":"vLLM","text":"<p>vLLM is configured as the default in the endpoint picker extension. No further configuration is required.</p>"},{"location":"implementations/model-servers/#triton-with-tensorrt-llm-backend","title":"Triton with TensorRT-LLM Backend","text":"<p>Triton specific metric names need to be specified when starting the EPP.</p>"},{"location":"implementations/model-servers/#option-1-use-helm","title":"Option 1: Use Helm","text":"<p>Use <code>--set inferencePool.modelServerType=triton-tensorrt-llm</code> to install the <code>inferencepool</code> via helm. See the <code>inferencepool</code> helm guide for more details.</p>"},{"location":"implementations/model-servers/#option-2-edit-epp-deployment-yaml","title":"Option 2: Edit EPP deployment yaml","text":"<p>Add the following to the <code>args</code> of the EPP deployment</p> <p><code>- -totalQueuedRequestsMetric - \"nv_trt_llm_request_metrics{request_type=waiting}\" - -kvCacheUsagePercentageMetric - \"nv_trt_llm_kv_cache_block_metrics{kv_cache_block_type=fraction}\" - -loraInfoMetric - \"\" # Set an empty metric to disable LoRA metric scraping as they are not supported by Triton yet.</code></p>"},{"location":"performance/benchmark/","title":"Benchmark","text":"<p>This user guide shows how to run benchmarks against a vLLM model server deployment by using both Gateway API Inference Extension, and a Kubernetes service as the load balancing strategy. The benchmark uses the Latency Profile Generator (LPG) tool to generate load and collect results.</p>"},{"location":"performance/benchmark/#prerequisites","title":"Prerequisites","text":""},{"location":"performance/benchmark/#deploy-the-inference-extension-and-sample-model-server","title":"Deploy the inference extension and sample model server","text":"<p>Follow the getting started guide to deploy the vLLM model server, CRDs, etc.</p> <p>Note: Only the GPU-based model server deployment option is supported for benchmark testing.</p>"},{"location":"performance/benchmark/#optional-scale-the-sample-vllm-deployment","title":"[Optional] Scale the sample vLLM deployment","text":"<p>You are more likely to see the benefits of the inference extension when there are a decent number of replicas to make the optimal routing decision.</p> <pre><code>kubectl scale deployment vllm-llama3-8b-instruct --replicas=8\n</code></pre>"},{"location":"performance/benchmark/#expose-the-model-server-via-a-k8s-service","title":"Expose the model server via a k8s service","text":"<p>To establish a baseline, expose the vLLM deployment as a k8s service:</p> <pre><code>kubectl expose deployment vllm-llama3-8b-instruct --port=80 --target-port=8000 --type=LoadBalancer\n</code></pre>"},{"location":"performance/benchmark/#run-benchmark","title":"Run benchmark","text":"<p>The LPG benchmark tool works by sending traffic to the specified target IP and port, and collecting the results. Follow the steps below to run a single benchmark. Multiple LPG instances can be deployed to run benchmarks in parallel against different targets.</p> <ol> <li> <p>Check out the repo.</p> <pre><code>git clone https://github.com/kubernetes-sigs/gateway-api-inference-extension\ncd gateway-api-inference-extension\n</code></pre> </li> <li> <p>Get the target IP. The examples below shows how to get the IP of a gateway or a k8s service.</p> <pre><code># Get gateway IP\nGW_IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\n# Get LoadBalancer k8s service IP\nSVC_IP=$(kubectl get service/vllm-llama2-7b -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n\necho $GW_IP\necho $SVC_IP\n</code></pre> </li> <li> <p>Then update the <code>&lt;target-ip&gt;</code> in <code>./config/manifests/benchmark/benchmark.yaml</code> to the value of <code>$SVC_IP</code> or <code>$GW_IP</code>.    Feel free to adjust other parameters such as <code>request_rates</code> as well. For a complete list of LPG configurations, refer to the    LPG user guide.</p> </li> <li> <p>Start the benchmark tool.</p> <pre><code>kubectl apply -f ./config/manifests/benchmark/benchmark.yaml\n</code></pre> </li> <li> <p>Wait for benchmark to finish and download the results. Use the <code>benchmark_id</code> environment variable to specify what this    benchmark is for. For instance, <code>inference-extension</code> or <code>k8s-svc</code>. When the LPG tool finishes benchmarking, it will print    a log line <code>LPG_FINISHED</code>. The script below will watch for that log line and then start downloading results.</p> <pre><code>benchmark_id='k8s-svc' ./tools/benchmark/download-benchmark-results.bash\n</code></pre> <p>After the script finishes, you should see benchmark results under <code>./tools/benchmark/output/default-run/k8s-svc/results/json</code> folder. Here is a sample json file. Replace <code>k8s-svc</code> with <code>inference-extension</code> when running an inference extension benchmark.</p> </li> </ol>"},{"location":"performance/benchmark/#tips","title":"Tips","text":"<ul> <li>When using a <code>benchmark_id</code> other than <code>k8s-svc</code> or <code>inference-extension</code>, the labels in <code>./tools/benchmark/benchmark.ipynb</code> must be   updated accordingly to analyze the results.</li> <li>You can specify <code>run_id=\"runX\"</code> environment variable when running the <code>./download-benchmark-results.bash</code> script. This is useful when you run benchmarks multiple times to get a more statistically meaningful results and group the results accordingly.</li> <li>Update the <code>request_rates</code> that best suit your benchmark environment.</li> </ul>"},{"location":"performance/benchmark/#advanced-benchmark-configurations","title":"Advanced Benchmark Configurations","text":"<p>Refer to the LPG user guide for a detailed list of configuration knobs.</p>"},{"location":"performance/benchmark/#analyze-the-results","title":"Analyze the results","text":"<p>This guide shows how to run the jupyter notebook using vscode after completing k8s service and inference extension benchmarks.</p> <ol> <li> <p>Create a python virtual environment.</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> </li> <li> <p>Install the dependencies.</p> <pre><code>pip install -r ./tools/benchmark/requirements.txt\n</code></pre> </li> <li> <p>Open the notebook <code>./tools/benchmark/benchmark.ipynb</code>, and run each cell. At the end you should    see a bar chart like below where \"ie\" represents inference extension. This chart is generated using this benchmarking tool with 6 vLLM (v1) model servers (H100 80 GB), llama2-7b and the ShareGPT dataset.</p> <p></p> </li> </ol>"},{"location":"reference/spec/","title":"API Reference","text":""},{"location":"reference/spec/#packages","title":"Packages","text":"<ul> <li>inference.networking.x-k8s.io/v1alpha2</li> </ul>"},{"location":"reference/spec/#inferencenetworkingx-k8siov1alpha2","title":"inference.networking.x-k8s.io/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the inference.networking.x-k8s.io API group.</p>"},{"location":"reference/spec/#resource-types","title":"Resource Types","text":"<ul> <li>InferenceModel</li> <li>InferencePool</li> </ul>"},{"location":"reference/spec/#criticality","title":"Criticality","text":"<p>Underlying type: string</p> <p>Criticality defines how important it is to serve the model compared to other models. Criticality is intentionally a bounded enum to contain the possibilities that need to be supported by the load balancing algorithm. Any reference to the Criticality field must be optional (use a pointer), and set no default. This allows us to union this with a oneOf field in the future should we wish to adjust/extend this behavior.</p> <p>Validation: - Enum: [Critical Standard Sheddable]</p> <p>Appears in: - InferenceModelSpec</p> Field Description <code>Critical</code> Critical defines the highest level of criticality. Requests to this band will be shed last. <code>Standard</code> Standard defines the base criticality level and is more important than Sheddable but lessimportant than Critical. Requests in this band will be shed before critical traffic.Most models are expected to fall within this band. <code>Sheddable</code> Sheddable defines the lowest level of criticality. Requests to this band will be shed beforeall other bands."},{"location":"reference/spec/#endpointpickerconfig","title":"EndpointPickerConfig","text":"<p>EndpointPickerConfig specifies the configuration needed by the proxy to discover and connect to the endpoint picker extension. This type is intended to be a union of mutually exclusive configuration options that we may add in the future.</p> <p>Appears in: - InferencePoolSpec</p> Field Description Default Validation <code>extensionRef</code> Extension Extension configures an endpoint picker as an extension service. Required: {}"},{"location":"reference/spec/#extension","title":"Extension","text":"<p>Extension specifies how to configure an extension that runs the endpoint picker.</p> <p>Appears in: - EndpointPickerConfig - InferencePoolSpec</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent.The default value is \"\", representing the Core API group. MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is the Kubernetes resource kind of the referent. For example\"Service\".Defaults to \"Service\" when not specified.ExternalName services can refer to CNAME DNS records that may liveoutside of the cluster and as such are difficult to reason about interms of conformance. They also may not be safe to forward to (seeCVE-2021-25740 for more information). Implementations MUST NOTsupport ExternalName Services. Service MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}  <code>portNumber</code> PortNumber The port number on the service running the extension. When unspecified,implementations SHOULD infer a default value of 9002 when the Kind isService. Maximum: 65535 Minimum: 1  <code>failureMode</code> ExtensionFailureMode Configures how the gateway handles the case when the extension is not responsive.Defaults to failClose. FailClose Enum: [FailOpen FailClose]"},{"location":"reference/spec/#extensionconnection","title":"ExtensionConnection","text":"<p>ExtensionConnection encapsulates options that configures the connection to the extension.</p> <p>Appears in: - Extension</p> Field Description Default Validation <code>failureMode</code> ExtensionFailureMode Configures how the gateway handles the case when the extension is not responsive.Defaults to failClose. FailClose Enum: [FailOpen FailClose]"},{"location":"reference/spec/#extensionfailuremode","title":"ExtensionFailureMode","text":"<p>Underlying type: string</p> <p>ExtensionFailureMode defines the options for how the gateway handles the case when the extension is not responsive.</p> <p>Validation: - Enum: [FailOpen FailClose]</p> <p>Appears in: - Extension - ExtensionConnection</p> Field Description <code>FailOpen</code> FailOpen specifies that the proxy should not drop the request and forward the request to and endpoint of its picking. <code>FailClose</code> FailClose specifies that the proxy should drop the request."},{"location":"reference/spec/#extensionreference","title":"ExtensionReference","text":"<p>ExtensionReference is a reference to the extension deployment.</p> <p>Appears in: - Extension</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent.The default value is \"\", representing the Core API group. MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is the Kubernetes resource kind of the referent. For example\"Service\".Defaults to \"Service\" when not specified.ExternalName services can refer to CNAME DNS records that may liveoutside of the cluster and as such are difficult to reason about interms of conformance. They also may not be safe to forward to (seeCVE-2021-25740 for more information). Implementations MUST NOTsupport ExternalName Services. Service MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}  <code>portNumber</code> PortNumber The port number on the service running the extension. When unspecified,implementations SHOULD infer a default value of 9002 when the Kind isService. Maximum: 65535 Minimum: 1"},{"location":"reference/spec/#group","title":"Group","text":"<p>Underlying type: string</p> <p>Group refers to a Kubernetes Group. It must either be an empty string or a RFC 1123 subdomain.</p> <p>This validation is based off of the corresponding Kubernetes validation: https://github.com/kubernetes/apimachinery/blob/02cfb53916346d085a6c6c7c66f882e3c6b0eca6/pkg/util/validation/validation.go#L208</p> <p>Valid values include:</p> <ul> <li>\"\" - empty string implies core Kubernetes API group</li> <li>\"gateway.networking.k8s.io\"</li> <li>\"foo.example.com\"</li> </ul> <p>Invalid values include:</p> <ul> <li>\"example.com/bar\" - \"/\" is an invalid character</li> </ul> <p>Validation: - MaxLength: 253 - Pattern: <code>^$|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code></p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#inferencemodel","title":"InferenceModel","text":"<p>InferenceModel is the Schema for the InferenceModels API.</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha2</code> <code>kind</code> string <code>InferenceModel</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferenceModelSpec <code>status</code> InferenceModelStatus"},{"location":"reference/spec/#inferencemodelspec","title":"InferenceModelSpec","text":"<p>InferenceModelSpec represents the desired state of a specific model use case. This resource is managed by the \"Inference Workload Owner\" persona.</p> <p>The Inference Workload Owner persona is someone that trains, verifies, and leverages a large language model from a model frontend, drives the lifecycle and rollout of new versions of those models, and defines the specific performance and latency goals for the model. These workloads are expected to operate within an InferencePool sharing compute capacity with other InferenceModels, defined by the Inference Platform Admin.</p> <p>InferenceModel's modelName (not the ObjectMeta name) is unique for a given InferencePool, if the name is reused, an error will be shown on the status of a InferenceModel that attempted to reuse. The oldest InferenceModel, based on creation timestamp, will be selected to remain valid. In the event of a race condition, one will be selected at random.</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>modelName</code> string ModelName is the name of the model as it will be set in the \"model\" parameter for an incoming request.ModelNames must be unique for a referencing InferencePool(names can be reused for a different pool in the same cluster).The modelName with the oldest creation timestamp is retained, and the incomingInferenceModel is sets the Ready status to false with a corresponding reason.In the rare case of a race condition, one Model will be selected randomly to be considered valid, and the other rejected.Names can be reserved without an underlying model configured in the pool.This can be done by specifying a target model and setting the weight to zero,an error will be returned specifying that no valid target model is found. MaxLength: 256 Required: {}  <code>criticality</code> Criticality Criticality defines how important it is to serve the model compared to other models referencing the same pool.Criticality impacts how traffic is handled in resource constrained situations. It handles this byqueuing or rejecting requests of lower criticality. InferenceModels of an equivalent Criticality willfairly share resources over throughput of tokens. In the future, the metric used to calculate fairness,and the proportionality of fairness will be configurable.Default values for this field will not be set, to allow for future additions of new field that may 'one of' with this field.Any implementations that may consume this field may treat an unset value as the 'Standard' range. Enum: [Critical Standard Sheddable]  <code>targetModels</code> TargetModel array TargetModels allow multiple versions of a model for traffic splitting.If not specified, the target model name is defaulted to the modelName parameter.modelName is often in reference to a LoRA adapter. MaxItems: 10  <code>poolRef</code> PoolObjectReference PoolRef is a reference to the inference pool, the pool must exist in the same namespace. Required: {}"},{"location":"reference/spec/#inferencemodelstatus","title":"InferenceModelStatus","text":"<p>InferenceModelStatus defines the observed state of InferenceModel</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>conditions</code> Condition array Conditions track the state of the InferenceModel.Known condition types are:* \"Accepted\" [map[lastTransitionTime:1970-01-01T00:00:00Z message:Waiting for controller reason:Pending status:Unknown type:Ready]] MaxItems: 8"},{"location":"reference/spec/#inferencepool","title":"InferencePool","text":"<p>InferencePool is the Schema for the InferencePools API.</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha2</code> <code>kind</code> string <code>InferencePool</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferencePoolSpec <code>status</code> InferencePoolStatus"},{"location":"reference/spec/#inferencepoolspec","title":"InferencePoolSpec","text":"<p>InferencePoolSpec defines the desired state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>selector</code> object (keys:LabelKey, values:LabelValue) Selector defines a map of labels to watch model server podsthat should be included in the InferencePool.In some cases, implementations may translate this field to a Service selector, so this matches the simplemap used for Service selectors instead of the full Kubernetes LabelSelector type.If sepecified, it will be applied to match the model server pods in the same namespace as the InferencePool.Cross namesoace selector is not supported. Required: {}  <code>targetPortNumber</code> integer TargetPortNumber defines the port number to access the selected model servers.The number must be in the range 1 to 65535. Maximum: 65535 Minimum: 1 Required: {}  <code>extensionRef</code> Extension Extension configures an endpoint picker as an extension service. Required: {}"},{"location":"reference/spec/#inferencepoolstatus","title":"InferencePoolStatus","text":"<p>InferencePoolStatus defines the observed state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>parent</code> PoolStatus array Parents is a list of parent resources (usually Gateways) that areassociated with the route, and the status of the InferencePool with respect toeach parent.A maximum of 32 Gateways will be represented in this list. An empty listmeans the route has not been attached to any Gateway. MaxItems: 32"},{"location":"reference/spec/#kind","title":"Kind","text":"<p>Underlying type: string</p> <p>Kind refers to a Kubernetes Kind.</p> <p>Valid values include:</p> <ul> <li>\"Service\"</li> <li>\"HTTPRoute\"</li> </ul> <p>Invalid values include:</p> <ul> <li>\"invalid/kind\" - \"/\" is an invalid character</li> </ul> <p>Validation: - MaxLength: 63 - MinLength: 1 - Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code></p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#labelkey","title":"LabelKey","text":"<p>Underlying type: string</p> <p>LabelKey was originally copied from: https://github.com/kubernetes-sigs/gateway-api/blob/99a3934c6bc1ce0874f3a4c5f20cafd8977ffcb4/apis/v1/shared_types.go#L694-L731 Duplicated as to not take an unexpected dependency on gw's API.</p> <p>LabelKey is the key of a label. This is used for validation of maps. This matches the Kubernetes \"qualified name\" validation that is used for labels. Labels are case sensitive, so: my-label and My-Label are considered distinct.</p> <p>Valid values include:</p> <ul> <li>example</li> <li>example.com</li> <li>example.com/path</li> <li>example.com/path.html</li> </ul> <p>Invalid values include:</p> <ul> <li>example~ - \"~\" is an invalid character</li> <li>example.com. - can not start or end with \".\"</li> </ul> <p>Validation: - MaxLength: 253 - MinLength: 1 - Pattern: <code>^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?([A-Za-z0-9][-A-Za-z0-9_.]{0,61})?[A-Za-z0-9]$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#labelvalue","title":"LabelValue","text":"<p>Underlying type: string</p> <p>LabelValue is the value of a label. This is used for validation of maps. This matches the Kubernetes label validation rules: * must be 63 characters or less (can be empty), * unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]), * could contain dashes (-), underscores (_), dots (.), and alphanumerics between.</p> <p>Valid values include:</p> <ul> <li>MyValue</li> <li>my.name</li> <li>123-my-value</li> </ul> <p>Validation: - MaxLength: 63 - MinLength: 0 - Pattern: <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#objectname","title":"ObjectName","text":"<p>Underlying type: string</p> <p>ObjectName refers to the name of a Kubernetes object. Object names can have a variety of forms, including RFC 1123 subdomains, RFC 1123 labels, or RFC 1035 labels.</p> <p>Validation: - MaxLength: 253 - MinLength: 1</p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#poolobjectreference","title":"PoolObjectReference","text":"<p>PoolObjectReference identifies an API object within the namespace of the referrer.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent. inference.networking.x-k8s.io MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is kind of the referent. For example \"InferencePool\". InferencePool MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}"},{"location":"reference/spec/#poolstatus","title":"PoolStatus","text":"<p>PoolStatus defines the observed state of InferencePool from a Gateway.</p> <p>Appears in: - InferencePoolStatus</p> Field Description Default Validation <code>parentRef</code> ObjectReference GatewayRef indicates the gateway that observed state of InferencePool. <code>conditions</code> Condition array Conditions track the state of the InferencePool.Known condition types are: \"Accepted\" \"ResolvedRefs\" [map[lastTransitionTime:1970-01-01T00:00:00Z message:Waiting for controller reason:Pending status:Unknown type:Accepted]] MaxItems: 8"},{"location":"reference/spec/#portnumber","title":"PortNumber","text":"<p>Underlying type: integer</p> <p>PortNumber defines a network port.</p> <p>Validation: - Maximum: 65535 - Minimum: 1</p> <p>Appears in: - Extension - ExtensionReference</p>"},{"location":"reference/spec/#targetmodel","title":"TargetModel","text":"<p>TargetModel represents a deployed model or a LoRA adapter. The Name field is expected to match the name of the LoRA adapter (or base model) as it is registered within the model server. Inference Gateway assumes that the model exists on the model server and it's the responsibility of the user to validate a correct match. Should a model fail to exist at request time, the error is processed by the Inference Gateway and emitted on the appropriate InferenceModel object.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>name</code> string Name is the name of the adapter or base model, as expected by the ModelServer. MaxLength: 253 Required: {}  <code>weight</code> integer Weight is used to determine the proportion of traffic that should besent to this model when multiple target models are specified.Weight defines the proportion of requests forwarded to the specifiedmodel. This is computed as weight/(sum of all weights in thisTargetModels list). For non-zero values, there may be some epsilon fromthe exact proportion defined here depending on the precision animplementation supports. Weight is not a percentage and the sum ofweights does not need to equal 100.If a weight is set for any targetModel, it must be set for all targetModels.Conversely weights are optional, so long as ALL targetModels do not specify a weight. Maximum: 1e+06 Minimum: 1"}]}