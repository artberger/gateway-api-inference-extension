{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>v0.3.0 manual test</p> <p>Gateway API Inference Extension is an official Kubernetes project that optimizes self-hosting Generative Models on Kubernetes.</p> <p>The overall resource model focuses on 2 new inference-focused personas and corresponding resources that they are expected to manage:</p> <p></p>"},{"location":"#concepts-and-definitions","title":"Concepts and Definitions","text":"<p>The following specific terms to this project:</p> <ul> <li>Inference Gateway: A proxy/load-balancer that has been coupled with the   EndPointer Picker extension. It provides optimized routing and load balancing for   serving Kubernetes self-hosted generative Artificial Intelligence (AI)   workloads. It simplifies the deployment, management, and observability of AI   inference workloads.</li> <li>Inference Scheduler: An extendable component that makes decisions about which endpoint is optimal (best cost /   best performance) for an inference request based on <code>Metrics and Capabilities</code>   from Model Serving.</li> <li>Metrics and Capabilities: Data provided by model serving platforms about   performance, availability and capabilities to optimize routing. Includes   things like [Prefix Cache] status or [LoRA Adapters] availability.</li> <li>Endpoint Picker(EPP): An implementation of an <code>Inference Scheduler</code> with additional Routing, Flow, and Request Control layers to allow for sophisticated routing strategies. Additional info on the architecture of the EPP here.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>Gateway API Inference Extension optimizes self-hosting Generative AI Models on Kubernetes. It provides optimized load-balancing for self-hosted Generative AI Models on Kubernetes. The project\u2019s goal is to improve and standardize routing to inference workloads across the ecosystem.</p> <p>This is achieved by leveraging Envoy's External Processing to extend any gateway that supports both ext-proc and Gateway API into an inference gateway. This extension extends popular gateways like Envoy Gateway, kgateway, and GKE Gateway - to become Inference Gateway - supporting inference platform teams self-hosting Generative Models (with a current focus on large language models) on Kubernetes. This integration makes it easy to expose and control access to your local OpenAI-compatible chat completion endpoints to other workloads on or off cluster, or to integrate your self-hosted models alongside model-as-a-service providers in a higher level AI Gateways like LiteLLM, Gloo AI Gateway, or Apigee.</p> <ul> <li> <p>Model-aware routing: Instead of simply routing based on the path of the request, an inference gateway allows you to route to models based on the model names. This is enabled by support for GenAI Inference API specifications (such as OpenAI API) in the gateway implementations such as in Envoy Proxy. This model-aware routing also extends to Low-Rank Adaptation (LoRA) fine-tuned models.</p> </li> <li> <p>Serving priority: an inference gateway allows you to specify the serving priority of your models. For example, you can specify that your models for online inference of chat tasks (which is more latency sensitive) have a higher Criticality than a model for latency tolerant tasks such as a summarization. </p> </li> <li> <p>Model rollouts:  an inference gateway allows you to incrementally roll out new model versions by traffic splitting definitions based on the model names. </p> </li> <li> <p>Extensibility for Inference Services: an inference gateway defines extensibility pattern for additional Inference services to create bespoke routing capabilities should out of the box solutions not fit your needs.</p> </li> <li> <p>Customizable Load Balancing for Inference: an inference gateway defines a pattern for customizable load balancing and request routing that is optimized for Inference. An inference gateway provides a reference implementation of model endpoint picking leveraging metrics emitted from the model servers. This endpoint picking mechanism can be used in lieu of traditional load balancing mechanisms. Model Server-aware load balancing (\"smart\" load balancing as its sometimes referred to in this repo) has been proven to reduce the serving latency and improve utilization of accelerators in your clusters.</p> </li> </ul> <p>By achieving these, the project aims to reduce latency and improve accelerator (GPU) utilization for AI workloads.</p>"},{"location":"#api-resources","title":"API Resources","text":"<p>Head to our API overview to start exploring our APIs!</p>"},{"location":"#composable-layers","title":"Composable Layers","text":"<p>This project aims to define specifications to enable a compatible ecosystem for extending the Gateway API with custom endpoint selection algorithms. This project defines a set of patterns across three distinct layers of components that are relevant to this project:</p>"},{"location":"#gateway-api-implementations","title":"Gateway API Implementations","text":"<p>Gateway API has more than 25 implementations. As this pattern stabilizes, we expect a wide set of these implementations to support this project to become an inference gateway</p>"},{"location":"#endpoint-picker","title":"Endpoint Picker","text":"<p>As part of this project, we've built the Endpoint Picker. A pluggable &amp; extensible ext-proc deployment that implements this architecture.</p>"},{"location":"#model-server-frameworks","title":"Model Server Frameworks","text":"<p>This project will work closely with model server frameworks to establish a shared standard for interacting with these extensions, particularly focused on metrics and observability so extensions will be able to make informed routing decisions. The project is currently focused on integrations with vLLM and Triton, and will be open to other integrations as they are requested.</p>"},{"location":"#request-flow","title":"Request Flow","text":"<p>To illustrate how this all comes together, it may be helpful to walk through a sample request.</p> <ol> <li> <p>The first step involves the Gateway selecting the correct InferencePool (set of endpoints running a model server framework) or Service to route to. This logic is based on the existing Gateway and HTTPRoute APIs, and will be familiar to any Gateway API users or implementers.</p> </li> <li> <p>If the request should be routed to an InferencePool, the Gateway will forward the request information to the endpoint selection extension for that pool.</p> </li> <li> <p>The inference gateway will fetch metrics from whichever portion of the InferencePool endpoints can best achieve the configured objectives. Note that this kind of metrics probing may happen asynchronously, depending on the inference gateway.</p> </li> <li> <p>The inference gateway will instruct the Gateway which endpoint the request should be routed to.</p> </li> <li> <p>The Gateway will route the request to the desired endpoint.</p> </li> </ol> <p></p>"},{"location":"#who-is-working-on-gateway-api-inference-extension","title":"Who is working on Gateway API Inference Extension?","text":"<p>This project is being driven by WG-Serving SIG-Network to improve and standardize routing to inference workloads in Kubernetes. Check out the implementations reference to see the latest projects &amp; products that support this project. If you are interested in contributing to or building an implementation using Gateway API then don\u2019t hesitate to get involved!</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-can-i-get-involved-with-this-project","title":"How can I get involved with this project?","text":"<p>The contributing page keeps track of how to get involved with the project.</p>"},{"location":"faq/#why-isnt-this-project-in-the-main-gateway-api-repo","title":"Why isn't this project in the main Gateway API repo?","text":"<p>This project is an extension of Gateway API, and may eventually be merged into the main Gateway API repo. As we're starting, this project represents a close collaboration between WG-Serving, SIG-Network, and the Gateway API subproject. These groups are all well represented within the ownership of this project, and the separate repo enables this group to iterate more quickly as this project is getting started. As the project stabilizes, we'll revisit if it should become part of the main Gateway API project.</p>"},{"location":"faq/#will-there-be-a-default-controller-implementation","title":"Will there be a default controller implementation?","text":"<p>No. Although this project will provide a default/reference implementation of an extension, each individual Gateway controller can support this pattern. The scope of this project is to define the API extension model, a reference extension, conformance tests, and overall documentation.</p>"},{"location":"faq/#can-you-add-support-for-my-use-case-to-the-reference-extension","title":"Can you add support for my use case to the reference extension?","text":"<p>Maybe. We're trying to keep the scope of the reference extension fairly narrow and instead hoping to see an ecosystem of compatible extensions developed in this space. Unless a use case fits neatly into the existing scope of our reference extension, it would likely be better to develop a separate extension focused on your use case.</p>"},{"location":"api-types/inferencemodel/","title":"Inference Model","text":"Alpha since v0.1.0 <p>The <code>InferenceModel</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencemodel/#background","title":"Background","text":"<p>An InferenceModel allows the Inference Workload Owner to define:</p> <ul> <li>Which Model/LoRA adapter(s) to consume.</li> <li>Mapping from a client facing model name to the target model name in the InferencePool.</li> <li>InferenceModel allows for traffic splitting between adapters in the same InferencePool to allow for new LoRA adapter versions to be easily rolled out.</li> <li>Criticality of the requests to the InferenceModel.</li> </ul>"},{"location":"api-types/inferencemodel/#spec","title":"Spec","text":"<p>The full spec of the InferenceModel is defined here.</p>"},{"location":"api-types/inferencepool/","title":"Inference Pool","text":"Alpha since v0.1.0 <p>The <code>InferencePool</code> resource is alpha and may have breaking changes in future releases of the API.</p>"},{"location":"api-types/inferencepool/#background","title":"Background","text":"<p>The InferencePool API defines a group of Pods (containers) dedicated to serving AI models. Pods within an InferencePool share the same compute configuration, accelerator type, base language model, and model server. This abstraction simplifies the management of AI model serving resources, providing a centralized point of administrative configuration for Platform Admins.</p> <p>An InferencePool is expected to be bundled with an Endpoint Picker extension. This extension is responsible for tracking key metrics on each model server (i.e. the KV-cache utilization, queue length of pending requests, active LoRA adapters, etc.) and routing incoming inference requests to the optimal model server replica based on these metrics. An EPP can only be associated with a single InferencePool. The associated InferencePool is specified by the poolName and poolNamespace flags. An HTTPRoute can have multiple backendRefs that reference the same InferencePool and therefore routes to the same EPP. An HTTPRoute can have multiple backendRefs that reference different InferencePools and therefore routes to different EPPs.</p> <p>Additionally, any Pod that seeks to join an InferencePool would need to support the model server protocol, defined by this project, to ensure the Endpoint Picker has adequate information to intelligently route requests.</p>"},{"location":"api-types/inferencepool/#how-to-configure-an-inferencepool","title":"How to Configure an InferencePool","text":"<p>The full spec of the InferencePool is defined here.</p> <p>In summary, the InferencePoolSpec consists of 3 major parts:</p> <ul> <li>The <code>selector</code> field specifies which Pods belong to this pool. The labels in this selector must exactly match the labels applied to your model server Pods. </li> <li>The <code>targetPortNumber</code> field defines the port number that the Inference Gateway should route to on model server Pods that belong to this pool. </li> <li>The <code>extensionRef</code> field references the endpoint picker extension (EPP) service that monitors key metrics from model servers within the InferencePool and provides intelligent routing decisions.</li> </ul>"},{"location":"api-types/inferencepool/#example-configuration","title":"Example Configuration","text":"<p>Here is an example InferencePool configuration:</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp\n    port: 9002\n    failureMode: FailClose\n</code></pre> <p>In this example: </p> <ul> <li>An InferencePool named <code>vllm-llama3-8b-instruct</code> is created in the <code>default</code> namespace.</li> <li>It will select Pods that have the label <code>app: vllm-llama3-8b-instruct</code>.</li> <li>Traffic routed to this InferencePool will call out to the EPP service <code>vllm-llama3-8b-instruct-epp</code> on port <code>9002</code> for making routing decisions. If EPP fails to pick an endpoint, or is not responsive, the request will be dropped.</li> <li>Traffic routed to this InferencePool will be forwarded to the port <code>8000</code> on the selected Pods.</li> </ul>"},{"location":"api-types/inferencepool/#overlap-with-service","title":"Overlap with Service","text":"<p>InferencePool has some small overlap with Service, displayed here:</p> <p></p> <p>The InferencePool is not intended to be a mask of the Service object. It provides a specialized abstraction tailored for managing and routing traffic to groups of LLM model servers, allowing Platform Admins to focus on pool-level management rather than low-level networking details.</p>"},{"location":"api-types/inferencepool/#replacing-an-inferencepool","title":"Replacing an InferencePool","text":"<p>Please refer to the Replacing an InferencePool guide for details on uses cases and how to replace an InferencePool.</p>"},{"location":"concepts/api-overview/","title":"API Overview","text":""},{"location":"concepts/api-overview/#background","title":"Background","text":"<p>Gateway API Inference Extension optimizes self-hosting Generative AI Models on Kubernetes.  It provides optimized load-balancing for self-hosted Generative AI Models on Kubernetes. The project\u2019s goal is to improve and standardize routing to inference workloads across the ecosystem.</p> <p>This is achieved by leveraging Envoy's External Processing to extend any gateway that supports both ext-proc and Gateway API into an inference gateway. This extension extends popular gateways like Envoy Gateway, kgateway, and GKE Gateway - to become Inference Gateway - supporting inference platform teams self-hosting Generative Models (with a current focus on large language models) on Kubernetes. This integration makes it easy to expose and control access to your local OpenAI-compatible chat completion endpoints to other workloads on or off cluster, or to integrate your self-hosted models alongside model-as-a-service providers  in a higher level AI Gateways like LiteLLM, Gloo AI Gateway, or Apigee.</p>"},{"location":"concepts/api-overview/#api-resources","title":"API Resources","text":"<p>Gateway API Inference Extension introduces two inference-focused API resources with distinct responsibilities,  each aligning with a specific user persona in the Generative AI serving workflow.</p> <p></p>"},{"location":"concepts/api-overview/#inferencepool","title":"InferencePool","text":"<p>InferencePool represents a set of Inference-focused Pods and an extension that will be used to route to them. Within the broader Gateway API resource model, this resource is considered a \"backend\". In practice, that means that you'd replace a Kubernetes Service with an InferencePool. This resource has some similarities to Service (a way to select Pods and specify a port), but has some unique capabilities. With InferencePool, you can configure a routing extension as well as inference-specific routing optimizations. For more information on this resource, refer to our InferencePool documentation or go directly to the InferencePool spec.</p>"},{"location":"concepts/api-overview/#inferencemodel","title":"InferenceModel","text":"<p>An InferenceModel represents a model or adapter, and configuration associated with that model. This resource enables you to configure the relative criticality of a model, and allows you to seamlessly translate the requested model name to one or more backend model names. Multiple InferenceModels can be attached to an InferencePool. For more information on this resource, refer to our InferenceModel documentation or go directly to the InferenceModel spec.</p>"},{"location":"concepts/conformance/","title":"Conformance","text":"<p>Similar to Gateway API, this project will rely on conformance tests to ensure compatibility across implementations. This will be focused on three different layers:</p>"},{"location":"concepts/conformance/#1-gateway-api-implementations","title":"1. Gateway API Implementations","text":"<p>Conformance tests will verify that:</p> <ul> <li>InferencePool is supported as a backend type</li> <li>Implementations forward requests to the configured extension for an   InferencePool following the specification defined by this project</li> <li>Implementations honor the routing guidance provided by the extension</li> <li>Implementations behave appropriately when an extension is either not present   or fails to respond</li> </ul>"},{"location":"concepts/conformance/#2-inference-routing-extensions","title":"2. Inference Routing Extensions","text":"<p>Conformance tests will verify that:</p> <ul> <li>Extensions accept requests that match the protocol specified by this project</li> <li>Extensions respond with routing guidance that matches the protocol specified   by this project</li> </ul>"},{"location":"concepts/conformance/#3-model-server-frameworks","title":"3. Model Server Frameworks","text":"<p>Conformance tests will verify that:</p> <ul> <li>Frameworks serve the expected set of metrics using a format and path specified   by this project</li> </ul>"},{"location":"concepts/design-principles/","title":"Design Principles","text":"<p>These principles guide our efforts to build flexible Gateway API extensions that empower the development of high-performance AI Inference routing technologies\u2014balancing rapid delivery with long-term growth.</p> <p>Inference Gateways</p> <p>For simplicity, we'll refer to Gateway API Gateways which are composed together with AI Inference extensions as \"Inference Gateways\" throughout this document.</p>"},{"location":"concepts/design-principles/#prioritize-stability-of-the-core-interfaces","title":"Prioritize stability of the core interfaces","text":"<p>The most critical part of this project is the interfaces between components. To encourage both controller and extension developers to integrate with this project, we need to prioritize the stability of these interfaces. Although we can extend these interfaces in the future, it\u2019s critical the core is stable as soon as possible.</p> <p>When describing \"core interfaces\", we are referring to both of the following:</p>"},{"location":"concepts/design-principles/#1-gateway-endpoint-picker","title":"1. Gateway -&gt; Endpoint Picker","text":"<p>At a high level, this defines how a Gateway provides information to an Endpoint Picker, and how the Endpoint Picker selects endpoint(s) that the Gateway should route to.</p>"},{"location":"concepts/design-principles/#2-endpoint-picker-model-server-framework","title":"2. Endpoint Picker -&gt; Model Server Framework","text":"<p>This defines what an Endpoint Picker should expect from a compatible Model Server Framework with a focus on health checks and metrics.</p>"},{"location":"concepts/design-principles/#our-presets-are-finely-tuned","title":"Our presets are finely tuned","text":"<p>We provide APIs and reference implementations for the most common inference requirements. Our defaults for those APIs and implementations\u2014shaped by extensive experience with leading model serving platforms and APIs\u2014are designed to provide the majority of Inference Gateway users with a great default experience without the need for extensive configuration or customization. If you take all of our default extensions and attach them to a compatible <code>Gateway</code>, it just \"works out of the box\".</p>"},{"location":"concepts/design-principles/#encourage-innovation-via-extensibility","title":"Encourage innovation via extensibility","text":"<p>This project is largely based on the idea that extensibility will enable innovation. With that in mind, we should make it as easy as possible for AI researchers to experiment with custom scheduling and routing logic. They should not need to know how to build a Kubernetes controller, or replicate a full networking stack. Instead, all the information needed to make a routing decision should be provided in an accessible format, with clear guidelines and examples of how to customize routing logic.</p>"},{"location":"concepts/design-principles/#objectives-over-instructions","title":"Objectives over instructions","text":"<p>The pace of innovation in this ecosystem has been rapid. Focusing too heavily on the specifics of current techniques could result in the API becoming outdated quickly. Instead of making the API too descriptive about how an objective should be achieved, this API should focus on the objectives that a Gateway and/or Endpoint Picker should strive to attain. Overly specific instructions or configuration can start as implementation specific APIs and grow into standards as the concepts become more stable and widespread.</p>"},{"location":"concepts/design-principles/#composable-components-and-reducing-reinvention","title":"Composable components and reducing reinvention","text":"<p>While it may be tempting to develop an entirely new AI-focused Gateway, many essential routing capabilities are already well established by Kubernetes. Our focus is on creating a layer of composable components that can be assembled together with other Kubernetes components. This approach empowers engineers to use our solution as a building block\u2014combining established technologies like Gateway API with our extensible model to build higher level solutions. Should you encounter a limitation, consider how existing tooling may be extended or improved first.</p>"},{"location":"concepts/design-principles/#additions-to-the-api-should-be-carefully-prioritized","title":"Additions to the API should be carefully prioritized","text":"<p>Every addition to the API should take the principles described above into account. Given that the goal of the API is to encourage a highly extensible ecosystem, each additional feature in the API is raising the barrier for entry to any new controller or extension. Our top priority should be to focus on concepts that we expect to be broadly implementable and useful. The extensible nature of this API will allow each individual implementation to experiment with new features via custom flags or APIs before they become part of the core API surface.</p>"},{"location":"concepts/roles-and-personas/","title":"Roles and Personas","text":"<p>Before diving into the details of the API, descriptions of the personas these APIs were designed for will help convey the thought process of the API design.</p>"},{"location":"concepts/roles-and-personas/#inference-platform-admin","title":"Inference Platform Admin","text":"<p>The Inference Platform Admin creates and manages the infrastructure necessary to run LLM workloads, including handling Ops for:</p> <ul> <li>Hardware</li> <li>Model Server</li> <li>Base Model</li> <li>Resource Allocation for Workloads</li> <li>Gateway configuration</li> <li>etc</li> </ul>"},{"location":"concepts/roles-and-personas/#inference-workload-owner","title":"Inference Workload Owner","text":"<p>An Inference Workload Owner persona owns and manages one or many Generative AI Workloads (LLM focused currently). This includes:</p> <ul> <li>Defining criticality</li> <li>Managing fine-tunes</li> <li>LoRA Adapters</li> <li>System Prompts</li> <li>Prompt Cache</li> <li>etc.</li> <li>Managing rollout of adapters</li> </ul>"},{"location":"contributing/","title":"How to Get Involved","text":"<p>This page contains links to all of the meeting notes, design docs and related discussions around the APIs.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Bug reports should be filed as GitHub Issues on this repo.</p> <p>NOTE: If you're reporting a bug that applies to a specific implementation of this project, please check our implementations page to find links to the repositories where you can get help with your specific implementation.</p>"},{"location":"contributing/#communications","title":"Communications","text":"<p>Major discussions and notifications will be sent on both the WG-Serving and SIG-Network mailing lists.</p> <p>Although we may end up creating a new Slack channel in the future, our conversations are currently split between the following Kubernetes Slack channels:</p> <ul> <li>#sig-network-gateway-api</li> <li>#wg-serving</li> </ul>"},{"location":"contributing/#meetings","title":"Meetings","text":"<p>Gateway API community meetings happen every Thursday at 10am Pacific Time (convert to your timezone). To receive an invite to this and other WG-Serving community meetings, join the WG-Serving mailing list.</p> <ul> <li>Zoom link (passcode in meeting notes doc)</li> </ul>"},{"location":"contributing/#meeting-notes-and-recordings","title":"Meeting Notes and Recordings","text":"<p>Meeting agendas and notes are maintained in the meeting notes doc. Feel free to add topics for discussion at an upcoming meeting.</p> <p>All meetings are recorded and automatically uploaded to the WG-Serving meetings YouTube playlist.</p>"},{"location":"contributing/devguide/","title":"Developer Guide","text":"<p>TODO</p>"},{"location":"gieps/overview/","title":"Gateway Inference Enhancement Proposal (GIEP)","text":"<p>Gateway Inference Enhancement Proposals (GIEPs) serve a similar purpose to the GIEP process for the main Gateway API project:</p> <ol> <li>Ensure that changes to the API follow a known process and discussion in the   OSS community.</li> <li>Make changes and proposals discoverable (current and future).</li> <li>Document design ideas, tradeoffs, decisions that were made for historical   reference.</li> <li>Record the results of larger community discussions.</li> <li>Record changes to the GIEP process itself.</li> </ol>"},{"location":"gieps/overview/#process","title":"Process","text":"<p>This diagram shows the state diagram of the GIEP process at a high level, but the details are below.</p> <pre><code>flowchart TD\n    D([Discuss with&lt;br /&gt;the community]) --&gt; C\n    C([Issue Created]) -------&gt; Memorandum\n    C([Issue Created]) --&gt; Provisional\n    Provisional --&gt;|If practical &lt;br /&gt; work needed| Prototyping\n    Provisional --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Prototyping --&gt;|GIEP Doc PR&lt;br /&gt;done| Implementable\n    Implementable --&gt;|Gateway API&lt;br /&gt;work completed| Experimental\n    Experimental --&gt;|Supported in&lt;br /&gt;multiple implementations&lt;br /&gt;+ Conformance tests| Standard\n    Standard --&gt;|Entire change is GA or implemented| Completed</code></pre>"},{"location":"gieps/overview/#giep-definitions","title":"GIEP Definitions","text":""},{"location":"gieps/overview/#giep-states","title":"GIEP States","text":"<p>Each GIEP has a state, which tracks where it is in the GIEP process.</p> <p>GIEPs can move to some states from any other state:</p> <ul> <li>Declined: The GIEP has been declined and further work will not occur.</li> <li>Deferred: We do not currently have bandwidth to handle this GIEP, it may     be revisited in the future.</li> <li>Declined: This proposal was considered by the community but ultimately   rejected.</li> <li>Withdrawn: This proposal was considered by the community but ultimately   withdrawn by the author.</li> </ul> <p>There is a special state to cover Memorandum GIEPs:</p> <ul> <li>Memorandum: These GIEPs either:<ul> <li>Document an agreement for further work, creating no spec changes   themselves, or</li> <li>Update the GIEP process.</li> </ul> </li> </ul> <p>API GIEPs flow through a number of states, which generally correspond to the level of stability of the change described in the GIEP:</p> <ul> <li>Provisional: The goals described by this GIEP have consensus but     implementation details have not been agreed to yet.</li> <li>Prototyping: An extension of <code>Provisional</code> which can be opted in to in     order to indicate to the community that there are some active practical     tests and experiments going on which are intended to be a part of the     development of this GIEP. This may include APIs or code, but that content     must not be distributed with releases.</li> <li>Implementable: The goals and implementation details described by this     GIEP have consensus but have not been fully implemented yet.</li> <li>Experimental: This GIEP has been implemented and is part of the     \"Experimental\" release channel. Breaking changes are still possible, up to     and including complete removal and moving to <code>Rejected</code>.</li> <li>Standard: This GIEP has been implemented and is part of the \"Standard\"     release channel. It should be quite stable.</li> <li>Completed: All implementation work on this API GIEP has been completed.</li> </ul>"},{"location":"gieps/overview/#relationships-between-gieps","title":"Relationships between GIEPs","text":"<p>GIEPs can have relationships between them. At this time, there are three possible relationships:</p> <ul> <li>Obsoletes and its backreference ObsoletedBy: when a GIEP is made   obsolete by another GIEP, and has its functionality completely replaced. The   Obsoleted GIEP is moved to the Declined state.</li> <li>Extends and its backreference ExtendedBy: when a GIEP has additional   details or implementation added in another GIEP.</li> <li>SeeAlso: when a GIEP is relevant to another GIEP, but is not affected in   any other defined way.</li> </ul> <p>Relationships are tracked in the YAML metadata files accompanying each GIEP.</p>"},{"location":"gieps/overview/#giep-metadata-file","title":"GIEP metadata file","text":"<p>Each GIEP has a YAML file containing metadata alongside it, please keep it up to date as changes to the GIEP occur.</p> <p>In particular, note the <code>authors</code>, and <code>changelog</code> fields, please keep those up to date.</p>"},{"location":"gieps/overview/#process_1","title":"Process","text":""},{"location":"gieps/overview/#1-discuss-with-the-community","title":"1. Discuss with the community","text":"<p>Before creating a GIEP, share your high level idea with the community. There are several places this may be done:</p> <ul> <li>A new GitHub   Discussion</li> <li>On our Slack Channel</li> <li>On one of our community   meetings</li> </ul> <p>Please default to GitHub discussions: they work a lot like GitHub issues which makes them easy to search.</p>"},{"location":"gieps/overview/#2-create-an-issue","title":"2. Create an Issue","text":"<p>Create a GIEP issue in the repo describing your change. At this point, you should copy the outcome of any other conversations or documents into this document.</p>"},{"location":"gieps/overview/#3-agree-on-the-goals","title":"3. Agree on the Goals","text":"<p>Although it can be tempting to start writing out all the details of your proposal, it's important to first ensure we all agree on the goals.</p> <p>For API GIEPs, the first version of your GIEP should aim for a \"Provisional\" status and leave out any implementation details, focusing primarily on \"Goals\" and \"Non-Goals\".</p> <p>For Memorandum GIEPs, the first version of your GIEP will be the only one, as Memorandums have only a single stage - <code>Accepted</code>.</p>"},{"location":"gieps/overview/#3-document-implementation-details","title":"3. Document Implementation Details","text":"<p>Now that everyone agrees on the goals, it is time to start writing out your proposed implementation details. These implementation details should be very thorough, including the proposed API spec, and covering any relevant edge cases. Note that it may be helpful to use a shared doc for part of this phase to enable faster iteration on potential designs.</p> <p>It is likely that throughout this process, you will discuss a variety of alternatives. Be sure to document all of these in the GIEP, and why we decided against them. At this stage, the GIEP should be targeting the \"Implementable\" stage.</p>"},{"location":"gieps/overview/#4-implement-the-giep-as-experimental","title":"4. Implement the GIEP as \"Experimental\"","text":"<p>With the GIEP marked as \"Implementable\", it is time to actually make those proposed changes in our API. In some cases, these changes will be documentation only, but in most cases, some API changes will also be required. It is important that every new feature of the API is marked as \"Experimental\" when it is introduced. Within the API, we use <code>&lt;gateway:experimental&gt;</code> tags to denote experimental fields. Within Golang packages (conformance tests, CLIs, e.t.c.) we use the <code>experimental</code> Golang build tag to denote experimental functionality.</p> <p>Some other requirements must be met before marking a GIEP <code>Experimental</code>:</p> <ul> <li>the graduation criteria to reach <code>Standard</code> MUST be filled out</li> <li>a proposed probationary period (see next section) must be included in the GIEP   and approved by maintainers.</li> </ul> <p>Before changes are released they MUST be documented. GIEPs that have not been both implemented and documented before a release cut off will be excluded from the release.</p>"},{"location":"gieps/overview/#probationary-period","title":"Probationary Period","text":"<p>Any GIEP in the <code>Experimental</code> phase is automatically under a \"probationary period\" where it will come up for re-assessment if its graduation criteria are not met within a given time period. GIEPs that wish to move into <code>Experimental</code> status MUST document a proposed period (6 months is the suggested default) that MUST be approved by maintainers. Maintainers MAY select an alternative time duration for a probationary period if deemed appropriate, and will document their reasoning.</p> <p>Rationale: This probationary period exists to avoid GIEPs getting \"stale\" and to provide guidance to implementations about how relevant features should be used, given that they are not guaranteed to become supported.</p> <p>At the end of a probationary period if the GIEP has not been able to resolve its graduation criteria it will move to \"Rejected\" status. In extenuating circumstances an extension of that period may be accepted by approval from maintainers. GIEPs which are <code>Rejected</code> in this way are removed from the experimental CRDs and more or less put on hold. GIEPs may be allowed to move back into <code>Experimental</code> status from <code>Rejected</code> for another probationary period if a new strategy for achieving their graduation criteria can be established. Any such plan to take a GIEP \"off the shelf\" must be reviewed and accepted by the maintainers.</p> <p>Warning: It is extremely important** that projects which implement <code>Experimental</code> features clearly document that these features may be removed in future releases.</p>"},{"location":"gieps/overview/#5-graduate-the-giep-to-standard","title":"5. Graduate the GIEP to \"Standard\"","text":"<p>Once this feature has met the graduation criteria, it is time to graduate it to the \"Standard\" channel of the API. Depending on the feature, this may include any of the following:</p> <ol> <li>Graduating the resource to beta</li> <li>Graduating fields to \"standard\" by removing <code>&lt;gateway:experimental&gt;</code> tags</li> <li>Graduating a concept to \"standard\" by updating documentation</li> </ol>"},{"location":"gieps/overview/#6-close-out-the-giep-issue","title":"6. Close out the GIEP issue","text":"<p>The GIEP issue should only be closed once the feature has: - Moved to the standard channel for distribution (if necessary) - Moved to a \"v1\" <code>apiVersion</code> for CRDs - been completely implemented and has wide acceptance (for process changes).</p> <p>In short, the GIEP issue should only be closed when the work is \"done\" (whatever that means for that GIEP).</p>"},{"location":"gieps/overview/#format","title":"Format","text":"<p>GIEPs should match the format of the template found in GIEP-696.</p>"},{"location":"gieps/overview/#out-of-scope","title":"Out of scope","text":"<p>What is out of scope: see text from KEP. Examples:</p> <ul> <li>Bug fixes</li> <li>Small changes (API validation, documentation, fixups). It is always possible   that the reviewers will determine a \"small\" change ends up requiring a GIEP.</li> </ul>"},{"location":"gieps/overview/#faq","title":"FAQ","text":""},{"location":"gieps/overview/#why-is-it-named-giep","title":"Why is it named GIEP?","text":"<p>To avoid potential confusion if people start following the cross references to the full GEP or KEP process.</p>"},{"location":"gieps/overview/#why-have-a-different-process-than-mainline","title":"Why have a different process than mainline?","text":"<p>Gateway API has some differences with most upstream KEPs. Notably Gateway API intentionally avoids including any implementation with the project, so this process is focused entirely on the substance of the API. As this project is based on CRDs it also has an entirely separately release process, and has developed concepts like \"release channels\" that do not exist in upstream.</p>"},{"location":"gieps/overview/#is-it-ok-to-discuss-using-shared-docs-scratch-docs-etc","title":"Is it ok to discuss using shared docs, scratch docs etc?","text":"<p>Yes, this can be a helpful intermediate step when iterating on design details. It is important that all major feedback, discussions, and alternatives considered in that step are represented in the GIEP though. A key goal of GIEPs is to show why we made a decision and which alternatives were considered. If separate docs are used, it's important that we can still see all relevant context and decisions in the final GIEP.</p>"},{"location":"gieps/overview/#when-should-i-mark-a-giep-as-prototyping-as-opposed-to-provisional","title":"When should I mark a GIEP as <code>Prototyping</code> as opposed to <code>Provisional</code>?","text":"<p>The <code>Prototyping</code> status carries the same base meaning as <code>Provisional</code> in that consensus is not complete between stakeholders and we're not ready to move toward releasing content yet. You should use <code>Prototyping</code> to indicate to your fellow community members that we're in a state of active practical tests and experiments which are intended to help us learn and iterate on the GIEP. These can include distributing content, but not under any release channel.</p>"},{"location":"gieps/overview/#should-i-implement-support-for-experimental-channel-features","title":"Should I implement support for <code>Experimental</code> channel features?","text":"<p>Ultimately one of the main ways to get something into <code>Standard</code> is for it to mature through the <code>Experimental</code> phase, so we really need people to implement these features and provide feedback in order to have progress. That said, the graduation of a feature past <code>Experimental</code> is not a forgone conclusion. Before implementing an experimental feature, you should:</p> <ul> <li>Clearly document that support for the feature is experimental and may   disappear in the future.</li> <li>Have a plan in place for how you would handle the removal of this feature from   the API.</li> </ul>"},{"location":"gieps/giep-116/","title":"GIEP-116: GIEP template","text":"<ul> <li>Issue: #0</li> <li>Status: Provisional|Implementable|Experimental|Standard|Deferred|Rejected|Withdrawn|Replaced</li> </ul> <p>(See status definitions here.)</p>"},{"location":"gieps/giep-116/#tldr","title":"TLDR","text":"<p>(1-2 sentence summary of the proposal)</p>"},{"location":"gieps/giep-116/#goals","title":"Goals","text":"<p>(Primary goals of this proposal.)</p>"},{"location":"gieps/giep-116/#non-goals","title":"Non-Goals","text":"<p>(What is out of scope for this proposal.)</p>"},{"location":"gieps/giep-116/#introduction","title":"Introduction","text":"<p>(Can link to external doc -- but we should bias towards copying the content into the GEP as online documents are easier to lose -- e.g. owner messes up the permissions, accidental deletion)</p>"},{"location":"gieps/giep-116/#api","title":"API","text":"<p>(... details, can point to PR with changes)</p>"},{"location":"gieps/giep-116/#conformance-details","title":"Conformance Details","text":"<p>(This section describes the names to be used for the feature or features in conformance tests and profiles.</p> <p>These should be <code>CamelCase</code> names that specify the feature as precisely as possible, and are particularly important for Extended features, since they may be surfaced to users.)</p>"},{"location":"gieps/giep-116/#alternatives","title":"Alternatives","text":"<p>(List other design alternatives and why we did not go in that direction)</p>"},{"location":"gieps/giep-116/#references","title":"References","text":"<p>(Add any additional document links. Again, we should try to avoid too much content not in version control to avoid broken links)</p>"},{"location":"guides/","title":"Getting started with an Inference Gateway","text":"Experimental <p>This project is still in an alpha state and breaking changes may occur in the future.</p> <p>This quickstart guide is intended for engineers familiar with k8s and model servers (vLLM in this instance). The goal of this guide is to get an Inference Gateway up and running!</p>"},{"location":"guides/#prerequisites","title":"Prerequisites","text":"<ul> <li>A cluster with:</li> <li>Support for services of type <code>LoadBalancer</code>. For kind clusters, follow this guide   to get services of type LoadBalancer working.</li> <li>Support for sidecar containers (enabled by default since Kubernetes v1.29)   to run the model server deployment.</li> </ul>"},{"location":"guides/#steps","title":"Steps","text":""},{"location":"guides/#deploy-sample-model-server","title":"Deploy Sample Model Server","text":"<p>Three options are supported for running the model server:</p> <ol> <li> <p>GPU-based model server.       Requirements: a Hugging Face access token that grants access to the model meta-llama/Llama-3.1-8B-Instruct.</p> </li> <li> <p>CPU-based model server (not using GPUs).       The sample uses the model Qwen/Qwen2.5-1.5B-Instruct.</p> </li> <li> <p>vLLM Simulator model server (not using GPUs).       The sample is configured to simulate the meta-llama/Llama-3.1-8B-Instruct model.</p> </li> </ol> <p>Choose one of these options and follow the steps below. Please do not deploy more than one, as the deployments have the same name and will override each other.</p> GPU-Based Model ServerCPU-Based Model ServervLLM Simulator Model Server <p>For this setup, you will need 3 GPUs to run the sample model server. Adjust the number of replicas in <code>./config/manifests/vllm/gpu-deployment.yaml</code> as needed.   Create a Hugging Face secret to download the model meta-llama/Llama-3.1-8B-Instruct. Ensure that the token grants access to this model.</p> <p>Deploy a sample vLLM deployment with the proper protocol to work with the LLM Instance Gateway.</p> <pre><code>kubectl create secret generic hf-token --from-literal=token=$HF_TOKEN # Your Hugging Face Token with access to the set of Llama models\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml\n</code></pre> <p>This setup is using the formal <code>vllm-cpu</code> image, which according to the documentation can run vLLM on x86 CPU platform.   For this setup, we use approximately 9.5GB of memory and 12 CPUs for each replica.</p> <p>While it is possible to deploy the model server with less resources, this is not recommended. For example, in our tests, loading the model using 8GB of memory and 1 CPU was possible but took almost 3.5 minutes and inference requests took unreasonable time. In general, there is a tradeoff between the memory and CPU we allocate to our pods and the performance. The more memory and CPU we allocate the better performance we can get.</p> <p>After running multiple configurations of these values we decided in this sample to use 9.5GB of memory and 12 CPUs for each replica, which gives reasonable response times. You can increase those numbers and potentially may even get better response times. For modifying the allocated resources, adjust the numbers in cpu-deployment.yaml as needed.</p> <p>Deploy a sample vLLM deployment with the proper protocol to work with the LLM Instance Gateway.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml\n</code></pre> <p>This option uses the vLLM simulator to simulate a backend model server.   This setup uses the least amount of compute resources, does not require GPU's, and is ideal for test/dev environments.</p> <p>To deploy the vLLM simulator, run the following command.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/sim-deployment.yaml\n</code></pre>"},{"location":"guides/#install-the-inference-extension-crds","title":"Install the Inference Extension CRDs","text":"Latest ReleaseDev Version <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/latest/download/manifests.yaml\n</code></pre> <pre><code>kubectl apply -k https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd\n</code></pre>"},{"location":"guides/#deploy-inferencemodel","title":"Deploy InferenceModel","text":"<p>Deploy the sample InferenceModel which is configured to forward traffic to the <code>food-review-1</code> LoRA adapter of the sample model server.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml\n</code></pre>"},{"location":"guides/#deploy-the-inferencepool-and-endpoint-picker-extension","title":"Deploy the InferencePool and Endpoint Picker Extension","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml\n</code></pre>"},{"location":"guides/#deploy-an-inference-gateway","title":"Deploy an Inference Gateway","text":"<p>Choose one of the following options to deploy an Inference Gateway.</p> GKEIstioKgateway <ol> <li> <p>Enable the Gateway API and configure proxy-only subnets when necessary. See Deploy Gateways   for detailed instructions.</p> </li> <li> <p>Deploy Gateway and HealthCheckPolicy resources</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gateway.yaml\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   inference-gateway   &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> <li> <p>Given that the default connection timeout may be insufficient for most inference workloads, it is recommended to configure a timeout appropriate for your intended use case.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml\n</code></pre> </li> </ol> <p>Please note that this feature is currently in an experimental phase and is not intended for production use.   The implementation and user experience are subject to changes as we continue to iterate on this project.</p> <ol> <li> <p>Requirements</p> <ul> <li>Gateway API CRDs installed.</li> </ul> </li> <li> <p>Install Istio</p> <pre><code>TAG=1.26-alpha.665da00e1e5392c31cf44cd4dedecd354dd660d5\n# on Linux\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-linux-amd64.tar.gz\ntar -xvf istioctl-$TAG-linux-amd64.tar.gz\n# on macOS\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-osx.tar.gz\ntar -xvf istioctl-$TAG-osx.tar.gz\n# on Windows\nwget https://storage.googleapis.com/istio-build/dev/$TAG/istioctl-$TAG-win.zip\nunzip istioctl-$TAG-win.zip\n\n./istioctl install --set tag=$TAG --set hub=gcr.io/istio-testing\n</code></pre> </li> <li> <p>If you run the Endpoint Picker (EPP) with the <code>--secureServing</code> flag set to <code>true</code> (the default mode), it is currently using a self-signed certificate. As a security measure, Istio does not trust self-signed certificates by default. As a temporary workaround, you can apply the destination rule to bypass TLS verification for EPP. A more secure TLS implementation in EPP is being discussed in Issue 582.</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/destination-rule.yaml\n</code></pre> </li> <li> <p>Deploy Gateway</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/gateway.yaml\n</code></pre> </li> <li> <p>Label the gateway</p> <pre><code>kubectl label gateway inference-gateway istio.io/enable-inference-extproc=true\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   inference-gateway   &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> </ol> <p>Kgateway recently added support for inference extension as a technical preview. This means do not   run Kgateway with inference extension in production environments. Refer to Issue 10411   for the list of caveats, supported features, etc.</p> <ol> <li> <p>Requirements</p> <ul> <li>Helm installed.</li> <li>Gateway API CRDs installed.</li> </ul> </li> <li> <p>Set the Kgateway version and install the Kgateway CRDs.</p> <pre><code>KGTW_VERSION=v2.0.2\nhelm upgrade -i --create-namespace --namespace kgateway-system --version $KGTW_VERSION kgateway-crds oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds\n</code></pre> </li> <li> <p>Install Kgateway</p> <pre><code>helm upgrade -i --namespace kgateway-system --version $KGTW_VERSION kgateway oci://cr.kgateway.dev/kgateway-dev/charts/kgateway --set inferenceExtension.enabled=true\n</code></pre> </li> <li> <p>Deploy the Gateway</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/gateway.yaml\n</code></pre> <p>Confirm that the Gateway was assigned an IP address and reports a <code>Programmed=True</code> status:  <pre><code>$ kubectl get gateway inference-gateway\nNAME                CLASS               ADDRESS         PROGRAMMED   AGE\ninference-gateway   kgateway            &lt;MY_ADDRESS&gt;    True         22s\n</code></pre></p> </li> <li> <p>Deploy the HTTPRoute</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/httproute.yaml\n</code></pre> </li> <li> <p>Confirm that the HTTPRoute status conditions include <code>Accepted=True</code> and <code>ResolvedRefs=True</code>:</p> <pre><code>kubectl get httproute llm-route -o yaml\n</code></pre> </li> </ol>"},{"location":"guides/#try-it-out","title":"Try it out","text":"<p>Wait until the gateway is ready.</p> GPU-Based Model ServerCPU-Based Model Server <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\n\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre> <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\n\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre>"},{"location":"guides/#cleanup","title":"Cleanup","text":"<p>The following instructions assume you would like to cleanup ALL resources that were created in this quickstart guide.    Please be careful not to delete resources you'd like to keep.</p> <ol> <li> <p>Uninstall the InferencePool, InferenceModel, and model server resources</p> <pre><code>kubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml --ignore-not-found\nkubectl delete secret hf-token --ignore-not-found\n</code></pre> </li> <li> <p>Uninstall the Gateway API resources</p> <pre><code>kubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/httproute.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/destination-rule.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/istio/httproute.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/gateway.yaml --ignore-not-found\nkubectl delete -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/httproute.yaml --ignore-not-found\n</code></pre> </li> <li> <p>Uninstall the Gateway API Inference Extension CRDs</p> <pre><code>kubectl delete -k https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd --ignore-not-found\n</code></pre> </li> <li> <p>Choose one of the following options to cleanup the Inference Gateway.</p> </li> </ol> GKEIstioKgateway <p>No further clean up is needed.</p> <p>The following instructions assume you would like to clean up ALL Istio resources that were created in this quickstart guide.</p> <ol> <li> <p>Uninstall All Istio resources</p> <pre><code>istioctl uninstall -y --purge\n</code></pre> </li> <li> <p>Remove the Istio namespace</p> <pre><code>kubectl delete ns istio-system\n</code></pre> </li> </ol> <p>The following instructions assume you would like to cleanup ALL Kgateway resources that were created in this quickstart guide.</p> <ol> <li> <p>Uninstall Kgateway</p> <pre><code>helm uninstall kgateway -n kgateway-system\n</code></pre> </li> <li> <p>Uninstall the Kgateway CRDs.</p> <pre><code>helm uninstall kgateway-crds -n kgateway-system\n</code></pre> </li> <li> <p>Remove the Kgateway namespace.</p> <pre><code>kubectl delete ns kgateway-system\n</code></pre> </li> </ol>"},{"location":"guides/adapter-rollout/","title":"Lora Adapter Rollout","text":"<p>The goal of this guide is to show you how to perform incremental roll out operations, which gradually deploy new versions of your inference infrastructure. You can update LoRA adapters and Inference Pool with minimal service disruption. This page also provides guidance on traffic splitting and rollbacks to help ensure reliable deployments for LoRA adapters rollout.</p> <p>LoRA adapter rollouts let you deploy new versions of LoRA adapters in phases, without altering the underlying base model or infrastructure. Use LoRA adapter rollouts to test improvements, bug fixes, or new features in your LoRA adapters.</p>"},{"location":"guides/adapter-rollout/#example","title":"Example","text":""},{"location":"guides/adapter-rollout/#prerequisites","title":"Prerequisites","text":"<p>Follow the steps in the main guide</p>"},{"location":"guides/adapter-rollout/#load-the-new-adapter-version-to-the-model-servers","title":"Load the new adapter version to the model servers","text":"<p>This guide leverages the LoRA syncer sidecar to dynamically manage adapters within a vLLM deployment, enabling users to add or remove them through a shared ConfigMap.</p> <p>Modify the LoRA syncer ConfigMap to initiate loading of the new adapter version.</p> <pre><code>kubectl edit configmap vllm-llama3-8b-instruct-adapters\n</code></pre> <p>Change the ConfigMap to match the following (note the new entry under models):</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n</code></pre> <p>The new adapter version is applied to the model servers live, without requiring a restart.</p>"},{"location":"guides/adapter-rollout/#direct-traffic-to-the-new-adapter-version","title":"Direct traffic to the new adapter version","text":"<p>Modify the InferenceModel to configure a canary rollout with traffic splitting. In this example, 10% of traffic for food-review model will be sent to the new food-review-2 adapter.</p> <pre><code>kubectl edit inferencemodel food-review\n</code></pre> <p>Change the targetModels list in InferenceModel to match the following:</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-1\n    weight: 90\n  - name: food-review-2\n    weight: 10\n</code></pre> <p>The above configuration means one in every ten requests should be sent to the new version. Try it out:</p> <ol> <li> <p>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\n</code></pre></p> </li> <li> <p>Send a few requests as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></p> </li> </ol>"},{"location":"guides/adapter-rollout/#finish-the-rollout","title":"Finish the rollout","text":"<p>Modify the InferenceModel to direct 100% of the traffic to the latest version of the adapter.</p> <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-2\n    weight: 100\n</code></pre> <p>Unload the older versions from the servers by updating the LoRA syncer ConfigMap to list the older version under the <code>ensureNotExist</code> list:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n      ensureNotExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n</code></pre> <p>With this, all requests should be served by the new adapter version.</p>"},{"location":"guides/conformance-tests/","title":"Test Setup and Execution","text":"<p>This document provides steps to run the Gateway API Inference Extension conformance tests.</p>"},{"location":"guides/conformance-tests/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>You need a Kubernetes cluster with LoadBalancer support.</p> </li> <li> <p>Choose an Implementation - Install an existing implementation. For setup instructions, refer to the The Quickstart Guide.  Alternatively run tests against your implementation after completing the implementer's guide.</p> </li> </ol> <p>Note: Since the EPP (EndPoint Picker) takes the <code>InferencePool</code> name as an environment variable, each conformance test creates a corresponding EPP deployment for each <code>InferencePool</code> it defines. For conformance testing, the EPP is configured with the <code>HeaderBasedTestingFilter</code>. This is enabled by setting the <code>ENABLE_REQ_HEADER_BASED_SCHEDULER_FOR_TESTING=true</code> environment variable in the EPP deployment manifest. </p>"},{"location":"guides/conformance-tests/#running-conformance-tests","title":"Running Conformance Tests","text":"<ol> <li> <p>Clone the Repository:     Create a local copy of the Gateway API Inference Extension repository:     <pre><code>git clone https://github.com/kubernetes-sigs/gateway-api-inference-extension.git\ncd gateway-api-inference-extension\n</code></pre></p> </li> <li> <p>Execute Tests:     Run the following command to execute all available tests. Replace <code>&lt;your_gatewayclass_name&gt;</code> with the GatewayClass used by the implementation under test.</p> <pre><code>go test ./conformance -args -gateway-class &lt;your_gatewayclass_name&gt;\n</code></pre> </li> </ol>"},{"location":"guides/conformance-tests/#test-execution-options","title":"Test Execution Options","text":"<ul> <li> <p>Speeding up Reruns: For repeated runs, you can add the flag <code>-cleanup-base-resources=false</code>. This will preserve resources such as namespaces and gateways between test runs, speeding up the process.     <pre><code>go test ./conformance -args -gateway-class &lt;your_gatewayclass_name&gt; -cleanup-base-resources=false\n</code></pre></p> </li> <li> <p>Running Specific Tests: To run a specific test, you can reference the test name by using the <code>-run-test</code> flag. For example:     <pre><code>go test ./conformance -args -gateway-class &lt;your_gatewayclass_name&gt; -run-test HTTPRouteMultipleGatewaysDifferentPools\n</code></pre></p> </li> <li> <p>Detailed Logging: To view detailed logs, you can enable logging mode by adding the <code>-v</code> as well as <code>-debug</code> flags.     <pre><code>go test -v ./conformance -args -debug -gateway-class &lt;your_gatewayclass_name&gt; -cleanup-base-resources=false -run-test HTTPRouteMultipleGatewaysDifferentPools\n</code></pre></p> </li> </ul>"},{"location":"guides/implementers/","title":"Implementer's Guide","text":"<p>This guide is intended for developers looking to implement support for the InferencePool custom resources within their Gateway API controller. It outlines how InferencePool fits into the existing resource model, discusses implementation options, explains how to interact with extensions, and provides guidance on testing.</p>"},{"location":"guides/implementers/#inferencepool-as-a-gateway-backend","title":"InferencePool as a Gateway Backend","text":"<p>Before we dive into the implementation, let\u2019s recap how an InferencePool works. </p> <p></p> <p>InferencePool represents a set of Inference-focused Pods and an extension that will be used to route to them. The InferencePool introduces a new type of backend within the Gateway API resource model. Instead of targeting Services, a Gateway can route traffic to an InferencePool. This InferencePool then becomes responsible for intelligent routing to the underlying model server pods based on the associated InferenceModel configurations. </p> <p>Here is an example of how to route traffic to an InferencePool using an HTTPRoute: <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    name: inference-gateway\n  rules:\n  - backendRefs:\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: base-model\n    matches:\n    - path:\n        type: PathPrefix\n        value: /\n</code></pre></p> <p>Note that the <code>rules.backendRefs</code> describes which InferencePool should receive the forwarded traffic when the path matches the corresponding path prefix. This is very similar to how we configure a Gateway with an HTTPRoute that directs traffic to a Service (a way to select Pods and specify a port). By using the InferencePool, it provides an abstraction over a set of compute resources (model server pods), and allows the controller to implement specialized routing strategies for these inference workloads.</p>"},{"location":"guides/implementers/#building-the-gateway-controller","title":"Building the Gateway controller","text":"<p>The general idea of implementing a Gateway controller supporting the InferencePool involves two major steps: </p> <ol> <li>Tracking the endpoints for InferencePool backends </li> <li>Callout to an extension to make intelligent routing decisions</li> </ol>"},{"location":"guides/implementers/#endpoint-tracking","title":"Endpoint Tracking","text":"<p>Consider a simple inference pool like this: <pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp\n</code></pre></p> <p>There are mainly two options for how to treat the Inference Pool in your controller.</p> <p>Option 1: Shadow Service Creation</p> <p>If your Gateway controller already handles Service as a backend, you can choose to create a headless Service that mirrors the endpoints defined by the InferencePool, like this:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata: \n  name: vllm-llama3-8b-instruct-shadow-service\nspec:\n  ports:\n  - port: 54321\n    protocol: TCP\n    targetPort: 8000\n  selector:\n    app:  vllm-llama3-8b-instruct\n  type: ClusterIP\n  clusterIP: None\n</code></pre> <p>The gateway controller would then treat this shadow service just like any other backend service it routes traffic to. </p> <p>This approach likely allows you to leverage existing service discovery, healthcheck infrastructure, and load balancing mechanisms that your controller already supports. However, it does come with the overhead of managing additional Service objects, and hence may affect the overall latency of the reconciliation of the Gateways.</p> <p>Option 2: Tracking InferencePool Endpoints Separately</p> <p>You can also choose to directly select and monitor the endpoints belonging to the InferencePool. For the simple inference pool example we have above, the controller would use the label <code>app: vllm-llama3-8b-instruct</code> to discover the pods matching the criteria, and get their endpoints (i.e. IP and port number). It would then need to monitor these pods for health and availability. </p> <p>With this approach, you can tailor the endpoint tracking and routing logic specifically to the characteristics and requirements of your InferencePool.</p>"},{"location":"guides/implementers/#callout-extension","title":"Callout Extension","text":"<p>The Endpoint Picker, or EPP, is a core component of the inference extension. The primary interaction for routing requests is defined between the proxy (e.g., Envoy) and the EPP using the Envoy external processing service protocol. See the Endpoint Picker Protocol for more information.</p>"},{"location":"guides/implementers/#how-to-callout-to-epp","title":"How to Callout to EPP","text":"<p>For each HTTP request, the proxy CAN communicate the subset of endpoints the EPP MUST pick from by setting <code>x-gateway-destination-endpoint-subset</code> key in the filter metadata field of the ext-proc request. If this key is set, the EPP must select from this endpoint list. If the list is empty or no endpoints are eligible, it should return a 503 error. If the key isn't set, the EPP selects from the endpoints defined by the InferencePool selector.</p>"},{"location":"guides/implementers/#response-from-the-extension","title":"Response from the extension","text":"<p>The EPP communicates the chosen endpoint to the proxy via the <code>x-gateway-destination-endpoint</code> HTTP header and the <code>dynamic_metadata</code> field of the ext-proc response. Failure to communicate the endpoint using both methods results in a 503 error if no endpoints are ready, or a 429 error if the request should be dropped. The header and metadata values must match. In addition to the chosen endpoint, a single fallback endpoint CAN be set using the key <code>x-gateway-destination-endpoint-fallback</code> in the same metadata namespace as one used for <code>x-gateway-destination-endpoint</code>.</p>"},{"location":"guides/implementers/#testing-tips","title":"Testing Tips","text":"<p>Here are some tips for testing your controller end-to-end:</p> <ul> <li>Focus on Key Scenarios: Add common scenarios like creating, updating, and deleting InferencePool resources, as well as different routing rules that target InferencePool backends.</li> <li>Verify Routing Behaviors: Design more complex routing scenarios and verify that requests are correctly routed to the appropriate model server pods within the InferencePool based on the InferenceModel configuration.</li> <li>Test Error Handling: Verify that the controller correctly handles scenarios like unsupported model names or resource constraints (if criticality-based shedding is implemented). Test with state transitions (such as constant requests while Pods behind EPP are being replaced and Pods behind InferencePool are being replaced) to ensure that the system is resilient to failures and can automatically recover by redirecting traffic to healthy Pods.</li> <li>Using Reference EPP Implementation + Echoserver: You can use the reference EPP implementation for testing your controller end-to-end. Instead of a full-fledged model server, a simple mock server (like the echoserver) can be very useful for verifying routing to ensure the correct pod received the request. </li> <li>Performance Test: Run end-to-end benchmarks to make sure that your inference gateway can achieve the latency target that is desired.</li> </ul>"},{"location":"guides/implementers/#conformance-tests","title":"Conformance Tests","text":"<p>See Conformance Test Setup and Execution.</p>"},{"location":"guides/inferencepool-rollout/","title":"InferencePool Rollout","text":"<p>The goal of this guide is to show you how to perform incremental roll out operations, which gradually deploy new versions of your inference infrastructure. You can update Inference Pool with minimal service disruption. This page also provides guidance on traffic splitting and rollbacks to help ensure reliable deployments for InferencePool rollout.</p> <p>InferencePool rollout is a powerful technique for performing various infrastructure and model updates with minimal disruption and built-in rollback capabilities. This method allows you to introduce changes incrementally, monitor their impact, and revert to the previous state if necessary.</p>"},{"location":"guides/inferencepool-rollout/#use-cases","title":"Use Cases","text":"<p>Use Cases for InferencePool Rollout:</p> <ul> <li>Node(compute, accelerator) update roll out</li> <li>Base model roll out</li> <li>Model server framework rollout</li> </ul>"},{"location":"guides/inferencepool-rollout/#nodecompute-accelerator-update-roll-out","title":"Node(compute, accelerator) update roll out","text":"<p>Node update roll outs safely migrate inference workloads to new node hardware or accelerator configurations. This process happens in a controlled manner without interrupting model service. Use node update roll outs to minimize service disruption during hardware upgrades, driver updates, or security issue resolution.</p>"},{"location":"guides/inferencepool-rollout/#base-model-roll-out","title":"Base model roll out","text":"<p>Base model updates roll out in phases to a new base LLM, retaining compatibility with existing LoRA adapters. You can use base model update roll outs to upgrade to improved model architectures or to address model-specific issues.</p>"},{"location":"guides/inferencepool-rollout/#model-server-framework-rollout","title":"Model server framework rollout","text":"<p>Model server framework rollouts enable the seamless deployment of new versions or entirely different serving frameworks, like updating from an older vLLM version to a newer one, or even migrating from a custom serving solution to a managed one. This type of rollout is critical for introducing performance enhancements, new features, or security patches within the serving layer itself, without requiring changes to the underlying base models or application logic. By incrementally rolling out framework updates, teams can ensure stability and performance, quickly identifying and reverting any regressions before they impact the entire inference workload.</p>"},{"location":"guides/inferencepool-rollout/#how-to-do-inferencepool-rollout","title":"How to do InferencePool rollout","text":"<ol> <li>Deploy new infrastructure: Create a new InferencePool configured with the new node(compute/accelerator) / model server / base model that you chose.</li> <li>Configure traffic splitting: Use an HTTPRoute to split traffic between the existing InferencePool and the new InferencePool. The <code>backendRefs.weight</code> field controls the traffic percentage allocated to each pool.</li> <li>Maintain InferenceModel integrity: Retain the existing InferenceModel configuration to ensure uniform model behavior across both node configurations or base model versions or model server versions.</li> <li>Preserve rollback capability: Retain the original nodes and InferencePool during the roll out to facilitate a rollback if necessary.</li> </ol>"},{"location":"guides/inferencepool-rollout/#example","title":"Example","text":"<p>This is an example of InferencePool rollout with node(compute, accelerator) update roll out</p>"},{"location":"guides/inferencepool-rollout/#prerequisites","title":"Prerequisites","text":"<p>Follow the steps in the main guide</p>"},{"location":"guides/inferencepool-rollout/#deploy-new-infrastructure","title":"Deploy new infrastructure","text":"<p>You start with an existing InferencePool named vllm-llama3-8b-instruct. To replace the original InferencePool, you create a new InferencePool named vllm-llama3-8b-instruct-new along with InferenceModels and Endpoint Picker Extension configured with the updated node specifications of <code>nvidia-h100-80gb</code> accelerator type,</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review-new\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct-new\n  targetModels:\n    - name: food-review-1\n      weight: 100\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct-new\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct-new\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct-new\n    spec:\n      containers:\n        - name: vllm\n          image: \"vllm/vllm-openai:latest\"\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n            - \"--model\"\n            - \"meta-llama/Llama-3.1-8B-Instruct\"\n            - \"--tensor-parallel-size\"\n            - \"1\"\n            - \"--port\"\n            - \"8000\"\n            - \"--max-num-seq\"\n            - \"1024\"\n            - \"--compilation-config\"\n            - \"3\"\n            - \"--enable-lora\"\n            - \"--max-loras\"\n            - \"2\"\n            - \"--max-lora-rank\"\n            - \"8\"\n            - \"--max-cpu-loras\"\n            - \"12\"\n          env:\n            - name: VLLM_USE_V1\n              value: \"1\"\n            - name: PORT\n              value: \"8000\"\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-token\n                  key: token\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          lifecycle:\n            preStop:\n              sleep:\n                seconds: 30\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            timeoutSeconds: 1\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 1\n            timeoutSeconds: 1\n          startupProbe:\n            failureThreshold: 600\n            initialDelaySeconds: 2\n            periodSeconds: 1\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              nvidia.com/gpu: 1\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts: # DO NOT USE subPath, dynamic configmap updates don't work on subPaths\n            - name: config-volume\n              mountPath:  /config\n      restartPolicy: Always\n      enableServiceLinks: false\n      terminationGracePeriodSeconds: 130\n      nodeSelector:\n        cloud.google.com/gke-accelerator: \"nvidia-h100-80gb\"\n\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-llama3-8b-instruct-adapters-new\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters-new\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters-new\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct-new\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct-new\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp-new\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-llama3-8b-instruct-epp-new\n  namespace: default\nspec:\n  selector:\n    app: vllm-llama3-8b-instruct-epp-new\n  ports:\n    - protocol: TCP\n      port: 9002\n      targetPort: 9002\n      appProtocol: http2\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct-epp-new\n  namespace: default\n  labels:\n    app: vllm-llama3-8b-instruct-epp-new\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct-epp-new\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct-epp-new\n    spec:\n      terminationGracePeriodSeconds: 130\n      containers:\n        - name: epp\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:main\n          imagePullPolicy: Always\n          args:\n            - -poolName\n            - \"vllm-llama3-8b-instruct-new\"\n            - \"-poolNamespace\"\n            - \"default\"\n            - -v\n            - \"4\"\n            - --zap-encoder\n            - \"json\"\n            - -grpcPort\n            - \"9002\"\n            - -grpcHealthPort\n            - \"9003\"\n          ports:\n            - containerPort: 9002\n            - containerPort: 9003\n            - name: metrics\n              containerPort: 9090\n          livenessProbe:\n            grpc:\n              port: 9003\n              service: inference-extension\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          readinessProbe:\n            grpc:\n              port: 9003\n              service: inference-extension\n            initialDelaySeconds: 5\n            periodSeconds: 10\n  EOF\n</code></pre>"},{"location":"guides/inferencepool-rollout/#direct-traffic-to-the-new-inference-pool","title":"Direct traffic to the new inference pool","text":"<p>By configuring an HTTPRoute, as shown below, you can incrementally split traffic between the original <code>vllm-llama3-8b-instruct</code> and new <code>vllm-llama3-8b-instruct-new</code>.</p> <pre><code>kubectl edit httproute llm-route\n</code></pre> <p>Change the backendRefs list in HTTPRoute to match the following:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      name: inference-gateway\n  rules:\n    - backendRefs:\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct\n          weight: 90\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct-new\n          weight: 10\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n</code></pre> <p>The above configuration means one in every ten requests should be sent to the new version. Try it out:</p> <ol> <li> <p>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\n</code></pre></p> </li> <li> <p>Send a few requests as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"Write as if you were a critic: San Francisco\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></p> </li> </ol>"},{"location":"guides/inferencepool-rollout/#finish-the-rollout","title":"Finish the rollout","text":"<p>Modify the HTTPRoute to direct 100% of the traffic to the latest version of the InferencePool.</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      name: inference-gateway\n  rules:\n    - backendRefs:\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct-new\n          weight: 100\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n</code></pre>"},{"location":"guides/inferencepool-rollout/#delete-old-version-of-inferencepool-inferencemodel-and-endpoint-picker-extension","title":"Delete old version of InferencePool, InferenceModel and Endpoint Picker Extension","text":"<pre><code>kubectl delete InferenceModel food-review --ignore-not-found\nkubectl delete Deployment vllm-llama3-8b-instruct --ignore-not-found\nkubectl delete ConfigMap vllm-llama3-8b-instruct-adapters --ignore-not-found\nkubectl delete InferencePool vllm-llama3-8b-instruct --ignore-not-found\nkubectl delete Deployment vllm-llama3-8b-instruct-epp --ignore-not-found\nkubectl delete Service vllm-llama3-8b-instruct-epp --ignore-not-found\n</code></pre> <p>With this, all requests should be served by the new Inference Pool.</p>"},{"location":"guides/metrics/","title":"Metrics","text":"<p>This guide describes the current state of exposed metrics and how to scrape them.</p>"},{"location":"guides/metrics/#requirements","title":"Requirements","text":"EPPDynamic LoRA Adapter Sidecar <p>To have response metrics, ensure the body mode is set to <code>Buffered</code> or <code>Streamed</code> (this should be the default behavior for all implementations).</p> <p>If you want to include usage metrics for vLLM model server streaming request, send the request with <code>include_usage</code>:</p> <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"food-review\",\n\"prompt\": \"whats your fav movie?\",\n\"max_tokens\": 10,\n\"temperature\": 0,\n\"stream\": true,\n\"stream_options\": {\"include_usage\": \"true\"}\n}'\n</code></pre> <p>To have response metrics, ensure the vLLM model server is configured with the dynamic LoRA adapter as a sidecar container and a ConfigMap to configure which models to load/unload. See this doc for an example.</p>"},{"location":"guides/metrics/#exposed-metrics","title":"Exposed metrics","text":""},{"location":"guides/metrics/#epp","title":"EPP","text":"Metric name Metric Type Description Labels Status inference_model_request_total Counter The counter of requests broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_error_total Counter The counter of requests errors broken out for each model. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_duration_seconds Distribution Distribution of response latency. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA normalized_time_per_output_token_seconds Distribution Distribution of ntpot (response latency per output token) <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_request_sizes Distribution Distribution of request size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_response_sizes Distribution Distribution of response size in bytes. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_input_tokens Distribution Distribution of input token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_output_tokens Distribution Distribution of output token count. <code>model_name</code>=&lt;model-name&gt;  <code>target_model_name</code>=&lt;target-model-name&gt; ALPHA inference_model_running_requests Gauge Number of running requests for each model. <code>model_name</code>=&lt;model-name&gt; ALPHA inference_pool_average_kv_cache_utilization Gauge The average kv cache utilization for an inference server pool. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_average_queue_size Gauge The average number of requests pending in the model server queue. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_per_pod_queue_size Gauge The total number of queue for each model server pod under the inference pool <code>model_server_pod</code>=&lt;model-server-pod-name&gt;  <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_pool_ready_pods Gauge The number of ready pods for an inference server pool. <code>name</code>=&lt;inference-pool-name&gt; ALPHA inference_extension_info Gauge The general information of the current build. <code>commit</code>=&lt;hash-of-the-build&gt;  <code>build_ref</code>=&lt;ref-to-the-build&gt; ALPHA"},{"location":"guides/metrics/#dynamic-lora-adapter-sidecar","title":"Dynamic LoRA Adapter Sidecar","text":"Metric name Metric Type Description Labels Status lora_syncer_adapter_status Gauge Status of LoRA adapters (1=loaded, 0=not_loaded) <code>adapter_name</code>=&lt;adapter-id&gt; ALPHA"},{"location":"guides/metrics/#scrape-metrics","title":"Scrape Metrics","text":"<p>The metrics endpoints are exposed on different ports by default:</p> <ul> <li>EPP exposes the metrics endpoint at port 9090</li> <li>Dynamic LoRA adapter sidecar exposes the metrics endpoint at port 8080</li> </ul> <p>To scrape metrics, the client needs a ClusterRole with the following rule: <code>nonResourceURLs: \"/metrics\", verbs: get</code>.</p> <p>Here is one example if the client needs to mound the secret to act as the service account <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: inference-gateway-metrics-reader\nrules:\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: inference-gateway-sa-metrics-reader-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: inference-gateway-metrics-reader\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: inference-gateway-sa-metrics-reader-secret\n  namespace: default\n  annotations:\n    kubernetes.io/service-account.name: inference-gateway-sa-metrics-reader\ntype: kubernetes.io/service-account-token\n</code></pre></p> <p>Then, you can curl the appropriate port as follows. For EPP (port 9090)</p> <pre><code>TOKEN=$(kubectl -n default get secret inference-gateway-sa-metrics-reader-secret  -o jsonpath='{.secrets[0].name}' -o jsonpath='{.data.token}' | base64 --decode)\n\nkubectl -n default port-forward inference-gateway-ext-proc-pod-name  9090\n\ncurl -H \"Authorization: Bearer $TOKEN\" localhost:9090/metrics\n</code></pre>"},{"location":"guides/metrics/#prometheus-alerts","title":"Prometheus Alerts","text":"<p>The section instructs how to configure prometheus alerts using collected metrics.</p>"},{"location":"guides/metrics/#configure-alerts","title":"Configure alerts","text":"<p>You can follow this blog post for instruction of setting up alerts in your monitoring stacks with Prometheus.</p> <p>A template alert rule is available at alert.yaml. You can modify and append these rules to your existing Prometheus deployment.</p>"},{"location":"guides/metrics/#high-inference-request-latency-p99","title":"High Inference Request Latency P99","text":"<pre><code>alert: HighInferenceRequestLatencyP99\nexpr: histogram_quantile(0.99, rate(inference_model_request_duration_seconds_bucket[5m])) &gt; 10.0 # Adjust threshold as needed (e.g., 10.0 seconds)\nfor: 5m\nannotations:\n  title: 'High latency (P99) for model {{ $labels.model_name }}'\n  description: 'The 99th percentile request duration for model {{ $labels.model_name }} and target model {{ $labels.target_model_name }} has been consistently above 10.0 seconds for 5 minutes.'\nlabels:\n  severity: 'warning'\n</code></pre>"},{"location":"guides/metrics/#high-inference-error-rate","title":"High Inference Error Rate","text":"<pre><code>alert: HighInferenceErrorRate\nexpr: sum by (model_name) (rate(inference_model_request_error_total[5m])) / sum by (model_name) (rate(inference_model_request_total[5m])) &gt; 0.05 # Adjust threshold as needed (e.g., 5% error rate)\nfor: 5m\nannotations:\n  title: 'High error rate for model {{ $labels.model_name }}'\n  description: 'The error rate for model {{ $labels.model_name }} and target model {{ $labels.target_model_name }} has been consistently above 5% for 5 minutes.'\nlabels:\n  severity: 'critical'\n  impact: 'availability'\n</code></pre>"},{"location":"guides/metrics/#high-inference-pool-queue-average-size","title":"High Inference Pool Queue Average Size","text":"<pre><code>alert: HighInferencePoolAvgQueueSize\nexpr: inference_pool_average_queue_size &gt; 50 # Adjust threshold based on expected queue size\nfor: 5m\nannotations:\n  title: 'High average queue size for inference pool {{ $labels.name }}'\n  description: 'The average number of requests pending in the queue for inference pool {{ $labels.name }} has been consistently above 50 for 5 minutes.'\nlabels:\n  severity: 'critical'\n  impact: 'performance'\n</code></pre>"},{"location":"guides/metrics/#high-inference-pool-average-kv-cache","title":"High Inference Pool Average KV Cache","text":"<pre><code>alert: HighInferencePoolAvgKVCacheUtilization\nexpr: inference_pool_average_kv_cache_utilization &gt; 0.9 # 90% utilization\nfor: 5m\nannotations:\n  title: 'High KV cache utilization for inference pool {{ $labels.name }}'\n  description: 'The average KV cache utilization for inference pool {{ $labels.name }} has been consistently above 90% for 5 minutes, indicating potential resource exhaustion.'\nlabels:\n  severity: 'critical'\n  impact: 'resource_exhaustion'\n</code></pre>"},{"location":"guides/serve-multiple-genai-models/","title":"Serve multiple generative AI models","text":"<p>A company wants to deploy multiple large language models (LLMs) to serve different workloads.  For example, they might want to deploy a Gemma3 model for a chatbot interface and a Deepseek model for a recommendation application.  The company needs to ensure optimal serving performance for these LLMs. By using an Inference Gateway, you can deploy these LLMs on your cluster with your chosen accelerator configuration in an <code>InferencePool</code>.  You can then route requests based on the model name (such as \"chatbot\" and \"recommender\") and the <code>Criticality</code> property.</p>"},{"location":"guides/serve-multiple-genai-models/#how","title":"How","text":"<p>The following diagram illustrates how an Inference Gateway routes requests to different models based on the model name. The model name is extracted by Body-Based routing  from the request body to the header. The header is then matched to dispatch  requests to different <code>InferencePool</code> (and their EPPs) instances. </p> <p>This example illustrates a conceptual example regarding how to use the <code>HTTPRoute</code> object to route based on model name like \u201cchatbot\u201d or \u201crecommender\u201d to <code>InferencePool</code>. <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: routes-to-llms\nspec:\n  parentRefs:\n  - name: inference-gateway\n  rules:\n  - matches:\n    - headers:\n      - type: Exact\n        #Body-Based routing(https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/pkg/bbr/README.md) is being used to copy the model name from the request body to the header.\n        name: X-Gateway-Model-Name\n        value: chatbot\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: gemma3\n      kind: InferencePool\n  - matches:\n    - headers:\n      - type: Exact\n        #Body-Based routing(https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/pkg/bbr/README.md) is being used to copy the model name from the request body to the header.\n        name: X-Gateway-Model-Name\n        value: recommender\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: deepseek-r1\n      kind: InferencePool     \n</code></pre></p>"},{"location":"guides/serve-multiple-genai-models/#try-it-out","title":"Try it out","text":"<ol> <li>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\n</code></pre></li> <li>Send a few requests to model \"chatbot\" as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"chatbot\",\n\"prompt\": \"What is the color of the sky\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></li> <li>Send a few requests to model \"recommender\" as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"recommender\",\n\"prompt\": \"Give me restaurant recommendations in Paris\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></li> </ol>"},{"location":"guides/serve-multiple-lora-adapters/","title":"Serve LoRA adapters on a shared pool","text":"<p>A company wants to serve LLMs for document analysis and focuses on audiences in multiple languages, such as English and Spanish. They have a fine-tuned LoRA adapter for each language, but need to efficiently use their GPU and TPU capacity. You can use an Inference Gateway to deploy dynamic LoRA fine-tuned adapters for each language (for example, <code>english-bot</code> and <code>spanish-bot</code>) on a common base model and accelerator. This lets you reduce the number of required accelerators by densely packing multiple models in a shared pool.</p>"},{"location":"guides/serve-multiple-lora-adapters/#how","title":"How","text":"<p>The following diagram illustrates how Inference Gateway serves multiple LoRA adapters on a shared pool.  This example illustrates how you can densely serve multiple LoRA adapters with distinct workload performance objectives on a common InferencePool. <pre><code>apiVersion: gateway.networking.x-k8s.io/v1alpha1\nkind: InferencePool\nmetadata:\n  name: gemma3\nspec:\n  selector:\n    pool: gemma3\n</code></pre> Let us say we have a couple of LoRA adapters named \u201cenglish-bot\u201d and \u201cspanish-bot\u201d for the Gemma3 base model. You can create an <code>InferenceModel</code> resource and associate these LoRA adapters to the relevant InferencePool resource. In this case, we associate these LoRA adapters to the gemma3 InferencePool resource created above.</p> <p><pre><code>apiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: english-bot\nspec:\n  modelName: english-bot\n  criticality: Standard\n  poolRef:\n    name: gemma3\n\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: spanish-bot\nspec:\n  modelName: spanish-bot\n  criticality: Critical\n  poolRef:\n    name: gemma3\n</code></pre> Now, you can route your requests from the gateway using the <code>HTTPRoute</code> object. <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  listeners:\n  - protocol: HTTP\n    port: 80\n    name: http\n\n---\nkind: HTTPRoute\napiVersion: gateway.networking.k8s.io/v1\nmetadata:\n  name: routes-to-llms\nspec:\n  parentRefs:\n    - name: inference-gateway\n  rules:\n  - matches:\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: gemma3\n      kind: InferencePool\n</code></pre></p>"},{"location":"guides/serve-multiple-lora-adapters/#try-it-out","title":"Try it out","text":"<ol> <li>Get the gateway IP: <pre><code>IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\n</code></pre></li> <li>Send a few requests to model \"english-bot\" as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"english-bot\",\n\"prompt\": \"What is the color of the sky\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></li> <li>Send a few requests to model \"spanish-bot\" as follows: <pre><code>curl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"spanish-bot\",\n\"prompt\": \"\u00bfDe qu\u00e9 color es...?\",\n\"max_tokens\": 100,\n\"temperature\": 0\n}'\n</code></pre></li> </ol>"},{"location":"guides/epp-configuration/prefix-aware/","title":"Prefix Cache Aware Plugin Configuration","text":"<p>The prefix cache plugin takes advantage of the prefix caching (e.g., vllm APC) feature of model servers, and optimizes request scheduling by placing requests sharing the longest prefixes to the same server as much as possible, while balancing the server load by considering kv-cache and queue depth.</p>"},{"location":"guides/epp-configuration/prefix-aware/#enable-the-prefix-cache-plugin","title":"Enable the prefix cache plugin","text":"<p>Currently prefix cache aware plugin is implemented in the V2 scheduler as an experimental feature. To enable it, set the following environment variables when starting the EndpointPicker(EPP).</p> <pre><code>EXPERIMENTAL_USE_SCHEDULER_V2: true\nENABLE_PREFIX_CACHE_SCHEDULING: true\n</code></pre> <p>See the Use Helm section to install an inferencepool with the environment variables.</p>"},{"location":"guides/epp-configuration/prefix-aware/#customize-the-prefix-cache-plugin","title":"Customize the prefix cache plugin","text":"<p>The prefix cache plugin exposes the following advanced configuration options via environment variables:</p> <ul> <li> <p><code>PREFIX_CACHE_HASH_BLOCK_SIZE</code>: The plugin matches prefixes in the unit of blocks. This is the size of each block in number of bytes. vLLM default block size is 16 tokens. Assume 4 characters per token, the default is set to 64 in EPP. The default is recommended unless performance is critical for use cases with extremely long inputs.</p> </li> <li> <p><code>PREFIX_CACHE_MAX_PREFIX_BLOCKS</code>: The maximum number of blocks to find prefix match. The default is 128 (or 128*64=8192 characters, or roughly 2048 tokens). This is useful to tradeoff prefix match accuracy for performance.</p> </li> <li> <p><code>PREFIX_CACHE_LRU_CAPACITY_PER_SERVER</code>: Maximum capacity the prefix LRU cache in number of block hashes per server (pod). Below shows a detailed analysis on how to estimate this.</p> <p>The prefix cache plugin estimates the prefix cache indexes in model server HBMs.  In the perfect scenario, EPP has the exact same prefix cache entries per model server as their HBM cache entries. If the EPP cache is smaller than HBM cache, a positive EPP cache match is more accurate, but there are more false cache misses. If the EPP cache is larger than the HBM cache, then there are more false cache hits. Therefore the EPP prefix cache indexer size should be as close as possible to the HBM cache size.</p> <p>NOTE: EPP builds prefix cache based on characters, while model server maintains prefix cache entries in tokens, a conversion between character &lt;-&gt; token is needed.</p> <p>Below are the formulas to estimate the EPP prefix indexer size:</p> <pre><code>max_kv_tokens_per_server = (HBM_size - model_size)/ kv_size_per_token\nlru_indexer_capacity_per_server = (max_kv_tokens_per_server * avg_chars_per_token)/prefix_indexer_hash_block_size\nlru_indexer_capacity_total = max_num_servers * lru_indexer_capacity_per_server\n</code></pre> <p>Let's take an example:</p> <ul> <li>Model: llama3 8B</li> <li>Accelerator: Nvidia H100 80GB</li> <li>Num replicas: 3</li> <li>Estimated # characters per token: 4 (source)</li> </ul> <pre><code>max_kv_tokens_per_server = (80GB - 16GB) / 128KB = 500,000\n# assume avg_chars_per_token = 4, prefix_indexer_hash_block_size = 64 (default)\n# each entry is about 358KB, so the memory footrpint is abut 11 MB per server\nlru_indexer_capacity_per_server = 500,000*4/64 = 31250\n</code></pre> </li> </ul> <p>See the Use Helm section to install an inferencepool with the environment variables.</p> <p></p>"},{"location":"guides/epp-configuration/prefix-aware/#use-helm","title":"Use Helm","text":"<p>Use the following reference command to install an inferencepool with the prefix cache plugin environment variable configurations:</p> <pre><code>$ helm install triton-llama3-8b-instruct \\\n  --set inferencePool.modelServers.matchLabels.app=triton-llama3-8b-instruct \\\n  --set inferencePool.modelServerType=triton-tensorrt-llm \\\n  --set provider.name=[none|gke] \\\n  --set inferenceExtension.env.EXPERIMENTAL_USE_SCHEDULER_V2=true \\\n  --set inferenceExtension.env.ENABLE_PREFIX_CACHE_SCHEDULING=true \\\n  --set inferenceExtension.env.PREFIX_CACHE_LRU_CAPACITY_PER_SERVER=31250 \\\n  --set inferenceExtension.env.PREFIX_CACHE_MAX_PREFIX_BLOCKS=1024 \\\n  oci://us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/charts/inferencepool --version v0\n</code></pre>"},{"location":"implementations/gateways/","title":"Gateway Implementations","text":"<p>This project has several implementations that are planned or in progress:</p> <ul> <li>Envoy AI Gateway</li> <li>Kgateway</li> <li>Google Kubernetes Engine</li> <li>Istio</li> <li>Alibaba Cloud Container Service for Kubernetes</li> </ul>"},{"location":"implementations/gateways/#envoy-ai-gateway","title":"Envoy AI Gateway","text":"<p>Envoy AI Gateway is an open source project built on top of  Envoy and Envoy Gateway to handle request traffic  from application clients to GenAI services. The features and capabilities are outlined here. Use the quickstart to get Envoy AI Gateway running with Gateway API in a few simple steps.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#kgateway","title":"Kgateway","text":"<p>Kgateway is a feature-rich, Kubernetes-native ingress controller and next-generation API gateway. Kgateway brings the full power and community support of Gateway API to its existing control-plane implementation.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>Google Kubernetes Engine (GKE) is a managed Kubernetes platform offered by Google Cloud. GKE's implementation of the Gateway API is through the GKE Gateway controller which provisions Google Cloud Load Balancers for Pods in GKE clusters.</p> <p>The GKE Gateway controller supports weighted traffic splitting, mirroring, advanced routing, multi-cluster load balancing and more. See the docs to deploy private or public Gateways and also multi-cluster Gateways.</p> <p>Progress towards supporting this project is tracked with a GitHub Issue.</p>"},{"location":"implementations/gateways/#istio","title":"Istio","text":"<p>Istio is an open source service mesh and gateway implementation. It provides a fully compliant implementation of the Kubernetes Gateway API for cluster ingress traffic control.  For service mesh users, Istio also fully supports east-west (including GAMMA) traffic management within the mesh.</p> <p>Gateway API Inference Extension support is being tracked by this GitHub Issue.</p>"},{"location":"implementations/gateways/#alibaba-cloud-container-service-for-kubernetes","title":"Alibaba Cloud Container Service for Kubernetes","text":"<p>Alibaba Cloud Container Service for Kubernetes (ACK) is a managed Kubernetes platform  offered by Alibaba Cloud. The implementation of the Gateway API in ACK is through the  ACK Gateway with Inference Extension component, which introduces model-aware,  GPU-efficient load balancing for AI workloads beyond basic HTTP routing.</p> <p>The ACK Gateway with Inference Extension implements the Gateway API Inference Extension  and provides optimized routing for serving generative AI workloads,  including weighted traffic splitting, mirroring, advanced routing, etc.  See the docs for the usage.</p> <p>Progress towards supporting Gateway API Inference Extension is being tracked  by this Issue.</p>"},{"location":"implementations/model-servers/","title":"Supported Model Servers","text":"<p>Any model server that conform to the model server protocol are supported by the inference extension.</p>"},{"location":"implementations/model-servers/#compatible-model-server-versions","title":"Compatible Model Server Versions","text":"Model Server Version Commit Notes vLLM V0 v0.6.4 and above commit 0ad216f vLLM V1 v0.8.0 and above commit bc32bc7 Triton(TensorRT-LLM) 25.03 and above commit 15cb989. LoRA affinity feature is not available as the required LoRA metrics haven't been implemented in Triton yet. Feature request"},{"location":"implementations/model-servers/#vllm","title":"vLLM","text":"<p>vLLM is configured as the default in the endpoint picker extension. No further configuration is required.</p>"},{"location":"implementations/model-servers/#triton-with-tensorrt-llm-backend","title":"Triton with TensorRT-LLM Backend","text":"<p>Triton specific metric names need to be specified when starting the EPP.</p>"},{"location":"implementations/model-servers/#option-1-use-helm","title":"Option 1: Use Helm","text":"<p>Use <code>--set inferencePool.modelServerType=triton-tensorrt-llm</code> to install the <code>inferencepool</code> via helm. See the <code>inferencepool</code> helm guide for more details.</p>"},{"location":"implementations/model-servers/#option-2-edit-epp-deployment-yaml","title":"Option 2: Edit EPP deployment yaml","text":"<p>Add the following to the <code>args</code> of the EPP deployment</p> <p><code>- -totalQueuedRequestsMetric - \"nv_trt_llm_request_metrics{request_type=waiting}\" - -kvCacheUsagePercentageMetric - \"nv_trt_llm_kv_cache_block_metrics{kv_cache_block_type=fraction}\" - -loraInfoMetric - \"\" # Set an empty metric to disable LoRA metric scraping as they are not supported by Triton yet.</code></p>"},{"location":"performance/benchmark/","title":"Benchmark","text":"<p>This user guide shows how to run benchmarks against a vLLM model server deployment by using both Gateway API Inference Extension, and a Kubernetes service as the load balancing strategy. The benchmark uses the Latency Profile Generator (LPG) tool to generate load and collect results.</p>"},{"location":"performance/benchmark/#prerequisites","title":"Prerequisites","text":""},{"location":"performance/benchmark/#deploy-the-inference-extension-and-sample-model-server","title":"Deploy the inference extension and sample model server","text":"<p>Follow the getting started guide to deploy the vLLM model server, CRDs, etc.</p> <p>Note: Only the GPU-based model server deployment option is supported for benchmark testing.</p>"},{"location":"performance/benchmark/#optional-scale-the-sample-vllm-deployment","title":"[Optional] Scale the sample vLLM deployment","text":"<p>You are more likely to see the benefits of the inference extension when there are a decent number of replicas to make the optimal routing decision.</p> <pre><code>kubectl scale deployment vllm-llama3-8b-instruct --replicas=8\n</code></pre>"},{"location":"performance/benchmark/#expose-the-model-server-via-a-k8s-service","title":"Expose the model server via a k8s service","text":"<p>To establish a baseline, expose the vLLM deployment as a k8s service:</p> <pre><code>kubectl expose deployment vllm-llama3-8b-instruct --port=80 --target-port=8000 --type=LoadBalancer\n</code></pre>"},{"location":"performance/benchmark/#run-benchmark","title":"Run benchmark","text":"<p>The LPG benchmark tool works by sending traffic to the specified target IP and port, and collecting the results. Follow the steps below to run a single benchmark. Multiple LPG instances can be deployed to run benchmarks in parallel against different targets.</p> <ol> <li> <p>Check out the repo.</p> <pre><code>git clone https://github.com/kubernetes-sigs/gateway-api-inference-extension\ncd gateway-api-inference-extension\n</code></pre> </li> <li> <p>Get the target IP. The examples below shows how to get the IP of a gateway or a k8s service.</p> <pre><code># Get gateway IP\nGW_IP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\n# Get LoadBalancer k8s service IP\nSVC_IP=$(kubectl get service/vllm-llama2-7b -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n\necho $GW_IP\necho $SVC_IP\n</code></pre> </li> <li> <p>Then update the <code>&lt;target-ip&gt;</code> in <code>./config/manifests/benchmark/benchmark.yaml</code> to the value of <code>$SVC_IP</code> or <code>$GW_IP</code>.    Feel free to adjust other parameters such as <code>request_rates</code> as well. For a complete list of LPG configurations, refer to the    LPG user guide.</p> </li> <li> <p>Start the benchmark tool.</p> <pre><code>kubectl apply -f ./config/manifests/benchmark/benchmark.yaml\n</code></pre> </li> <li> <p>Wait for benchmark to finish and download the results. Use the <code>benchmark_id</code> environment variable to specify what this    benchmark is for. For instance, <code>inference-extension</code> or <code>k8s-svc</code>. When the LPG tool finishes benchmarking, it will print    a log line <code>LPG_FINISHED</code>. The script below will watch for that log line and then start downloading results.</p> <pre><code>benchmark_id='k8s-svc' ./tools/benchmark/download-benchmark-results.bash\n</code></pre> <p>After the script finishes, you should see benchmark results under <code>./tools/benchmark/output/default-run/k8s-svc/results/json</code> folder. Here is a sample json file. Replace <code>k8s-svc</code> with <code>inference-extension</code> when running an inference extension benchmark.</p> </li> </ol>"},{"location":"performance/benchmark/#tips","title":"Tips","text":"<ul> <li>When using a <code>benchmark_id</code> other than <code>k8s-svc</code> or <code>inference-extension</code>, the labels in <code>./tools/benchmark/benchmark.ipynb</code> must be   updated accordingly to analyze the results.</li> <li>You can specify <code>run_id=\"runX\"</code> environment variable when running the <code>./download-benchmark-results.bash</code> script. This is useful when you run benchmarks multiple times to get a more statistically meaningful results and group the results accordingly.</li> <li>Update the <code>request_rates</code> that best suit your benchmark environment.</li> </ul>"},{"location":"performance/benchmark/#advanced-benchmark-configurations","title":"Advanced Benchmark Configurations","text":"<p>Refer to the LPG user guide for a detailed list of configuration knobs.</p>"},{"location":"performance/benchmark/#analyze-the-results","title":"Analyze the results","text":"<p>This guide shows how to run the jupyter notebook using vscode after completing k8s service and inference extension benchmarks.</p> <ol> <li> <p>Create a python virtual environment.</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> </li> <li> <p>Install the dependencies.</p> <pre><code>pip install -r ./tools/benchmark/requirements.txt\n</code></pre> </li> <li> <p>Open the notebook <code>./tools/benchmark/benchmark.ipynb</code>, and run each cell. In the last cell update the benchmark ids with<code>inference-extension</code> and <code>k8s-svc</code>. At the end you should     see a bar chart like below where \"ie\" represents inference extension. This chart is generated using this benchmarking tool with 6 vLLM (v1) model servers (H100 80 GB), llama2-7b and the ShareGPT dataset.</p> <p></p> </li> </ol>"},{"location":"performance/regression-testing/","title":"Regression Testing","text":"<p>Regression testing verifies that recent code changes have not adversely affected the performance or stability of the Inference Gateway.</p> <p>This guide explains how to run regression tests against the Gateway API inference extension using the Latency Profile Generator (LPG) to simulate traffic and collect performance metrics.</p>"},{"location":"performance/regression-testing/#prerequisites","title":"Prerequisites","text":"<p>Refer to the benchmark guide for common setup steps, including deployment of the inference extension, model server setup, scaling the vLLM deployment, and obtaining the Gateway IP.</p>"},{"location":"performance/regression-testing/#create-the-lpg-docker-image","title":"Create the LPG Docker Image","text":"<p>Follow the detailed instructions here to build the LPG Docker image:</p> <ul> <li>Create an artifact repository:</li> </ul> <pre><code>gcloud artifacts repositories create ai-benchmark --location=us-central1 --repository-format=docker\n</code></pre> <ul> <li>Prepare datasets for Infinity-Instruct and billsum:</li> </ul> <pre><code>pip install datasets transformers numpy pandas tqdm matplotlib\npython datasets/import_dataset.py --hf_token YOUR_TOKEN\n</code></pre> <ul> <li>Build the benchmark Docker image:</li> </ul> <pre><code>docker build -t inference-benchmark .\n</code></pre> <ul> <li>Push the Docker image to your artifact registry:</li> </ul> <pre><code>docker tag inference-benchmark us-central1-docker.pkg.dev/{project-name}/ai-benchmark/inference-benchmark\ndocker push us-central1-docker.pkg.dev/{project-name}/ai-benchmark/inference-benchmark\n</code></pre>"},{"location":"performance/regression-testing/#conduct-regression-tests","title":"Conduct Regression Tests","text":"<p>Run benchmarks using the configurations below, which are optimized for NVIDIA H100 GPUs (80 GB). Adjust configurations for other hardware as necessary.</p>"},{"location":"performance/regression-testing/#test-case-1-single-workload","title":"Test Case 1: Single Workload","text":"<ul> <li>Dataset: <code>billsum_conversations.json</code> (created from HuggingFace billsum dataset).<ul> <li>This dataset features long prompts, making it prefill-heavy and ideal for testing scenarios that emphasize initial token generation.</li> </ul> </li> <li>Model: Llama 3 (8B) (critical)</li> <li>Replicas: 10 (vLLM)</li> <li>Request Rates: 300\u2013350 (increments of 10)</li> </ul> <p>Refer to example manifest: <code>./config/manifests/regression-testing/single-workload-regression.yaml</code></p>"},{"location":"performance/regression-testing/#test-case-2-multi-lora","title":"Test Case 2: Multi-LoRA","text":"<ul> <li>Dataset: <code>Infinity-Instruct_conversations.json</code> (created from HuggingFace Infinity-Instruct dataset).<ul> <li>This dataset has long outputs, making it decode-heavy and useful for testing scenarios focusing on sustained token generation.</li> </ul> </li> <li>Model: Llama 3 (8B)</li> <li>LoRA Adapters: 15 adapters (<code>nvidia/llama-3.1-nemoguard-8b-topic-control</code>, rank 8, critical)</li> <li>Hardware: NVIDIA H100 GPUs (80 GB)</li> <li>Traffic Distribution: 60% (first 5 adapters, each 12%), 30% (next 5, each 6%), 10% (last 5, each 2%) simulating prod/dev/test tiers</li> <li>Max LoRA: 3</li> <li>Replicas: 10 (vLLM)</li> <li>Request Rates: 20\u2013200 (increments of 20)</li> </ul> <p>Optionally, you can also run benchmarks using the <code>ShareGPT</code> dataset for additional coverage.</p> <p>Update deployments for multi-LoRA support: - vLLM Deployment: <code>./config/manifests/regression-testing/vllm/multi-lora-deployment.yaml</code> - InferenceModel: <code>./config/manifests/inferencemodel.yaml</code></p> <p>Refer to example manifest: <code>./config/manifests/regression-testing/multi-lora-regression.yaml</code></p>"},{"location":"performance/regression-testing/#execute-benchmarks","title":"Execute Benchmarks","text":"<p>Benchmark in two phases: before and after applying your changes:</p> <ul> <li>Before changes:</li> </ul> <pre><code>benchmark_id='regression-before' ./tools/benchmark/download-benchmark-results.bash\n</code></pre> <ul> <li>After changes:</li> </ul> <pre><code>benchmark_id='regression-after' ./tools/benchmark/download-benchmark-results.bash\n</code></pre>"},{"location":"performance/regression-testing/#analyze-benchmark-results","title":"Analyze Benchmark Results","text":"<p>Use the provided Jupyter notebook (<code>./tools/benchmark/benchmark.ipynb</code>) to analyze results:</p> <ul> <li>Update benchmark IDs to <code>regression-before</code> and <code>regression-after</code>.</li> <li>Compare latency and throughput metrics, performing regression analysis.</li> <li>Check R\u00b2 values specifically:</li> <li>Prompts Attempted/Succeeded: Expect R\u00b2 \u2248 1</li> <li>Output Tokens per Minute, P90 per Output Token Latency, P90 Latency: Expect R\u00b2 close to 1 (allow minor variance).</li> </ul> <p>Identify significant deviations, investigate causes, and confirm performance meets expected standards.</p>"},{"location":"reference/spec/","title":"API Reference","text":""},{"location":"reference/spec/#packages","title":"Packages","text":"<ul> <li>inference.networking.x-k8s.io/v1alpha2</li> </ul>"},{"location":"reference/spec/#inferencenetworkingx-k8siov1alpha2","title":"inference.networking.x-k8s.io/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the inference.networking.x-k8s.io API group.</p>"},{"location":"reference/spec/#resource-types","title":"Resource Types","text":"<ul> <li>InferenceModel</li> <li>InferencePool</li> </ul>"},{"location":"reference/spec/#criticality","title":"Criticality","text":"<p>Underlying type: string</p> <p>Criticality defines how important it is to serve the model compared to other models. Criticality is intentionally a bounded enum to contain the possibilities that need to be supported by the load balancing algorithm. Any reference to the Criticality field must be optional(use a pointer), and set no default. This allows us to union this with a oneOf field in the future should we wish to adjust/extend this behavior.</p> <p>Validation: - Enum: [Critical Standard Sheddable]</p> <p>Appears in: - InferenceModelSpec</p> Field Description <code>Critical</code> Critical defines the highest level of criticality. Requests to this band will be shed last. <code>Standard</code> Standard defines the base criticality level and is more important than Sheddable but lessimportant than Critical. Requests in this band will be shed before critical traffic.Most models are expected to fall within this band. <code>Sheddable</code> Sheddable defines the lowest level of criticality. Requests to this band will be shed beforeall other bands."},{"location":"reference/spec/#endpointpickerconfig","title":"EndpointPickerConfig","text":"<p>EndpointPickerConfig specifies the configuration needed by the proxy to discover and connect to the endpoint picker extension. This type is intended to be a union of mutually exclusive configuration options that we may add in the future.</p> <p>Appears in: - InferencePoolSpec</p> Field Description Default Validation <code>extensionRef</code> Extension Extension configures an endpoint picker as an extension service. Required: {}"},{"location":"reference/spec/#extension","title":"Extension","text":"<p>Extension specifies how to configure an extension that runs the endpoint picker.</p> <p>Appears in: - EndpointPickerConfig - InferencePoolSpec</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent.The default value is \"\", representing the Core API group. MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is the Kubernetes resource kind of the referent. For example\"Service\".Defaults to \"Service\" when not specified.ExternalName services can refer to CNAME DNS records that may liveoutside of the cluster and as such are difficult to reason about interms of conformance. They also may not be safe to forward to (seeCVE-2021-25740 for more information). Implementations MUST NOTsupport ExternalName Services. Service MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}  <code>portNumber</code> PortNumber The port number on the service running the extension. When unspecified,implementations SHOULD infer a default value of 9002 when the Kind isService. Maximum: 65535 Minimum: 1  <code>failureMode</code> ExtensionFailureMode Configures how the gateway handles the case when the extension is not responsive.Defaults to failClose. FailClose Enum: [FailOpen FailClose]"},{"location":"reference/spec/#extensionconnection","title":"ExtensionConnection","text":"<p>ExtensionConnection encapsulates options that configures the connection to the extension.</p> <p>Appears in: - Extension</p> Field Description Default Validation <code>failureMode</code> ExtensionFailureMode Configures how the gateway handles the case when the extension is not responsive.Defaults to failClose. FailClose Enum: [FailOpen FailClose]"},{"location":"reference/spec/#extensionfailuremode","title":"ExtensionFailureMode","text":"<p>Underlying type: string</p> <p>ExtensionFailureMode defines the options for how the gateway handles the case when the extension is not responsive.</p> <p>Validation: - Enum: [FailOpen FailClose]</p> <p>Appears in: - Extension - ExtensionConnection</p> Field Description <code>FailOpen</code> FailOpen specifies that the proxy should not drop the request and forward the request to and endpoint of its picking. <code>FailClose</code> FailClose specifies that the proxy should drop the request."},{"location":"reference/spec/#extensionreference","title":"ExtensionReference","text":"<p>ExtensionReference is a reference to the extension deployment.</p> <p>Appears in: - Extension</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent.The default value is \"\", representing the Core API group. MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is the Kubernetes resource kind of the referent. For example\"Service\".Defaults to \"Service\" when not specified.ExternalName services can refer to CNAME DNS records that may liveoutside of the cluster and as such are difficult to reason about interms of conformance. They also may not be safe to forward to (seeCVE-2021-25740 for more information). Implementations MUST NOTsupport ExternalName Services. Service MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}  <code>portNumber</code> PortNumber The port number on the service running the extension. When unspecified,implementations SHOULD infer a default value of 9002 when the Kind isService. Maximum: 65535 Minimum: 1"},{"location":"reference/spec/#group","title":"Group","text":"<p>Underlying type: string</p> <p>Group refers to a Kubernetes Group. It must either be an empty string or a RFC 1123 subdomain.</p> <p>This validation is based off of the corresponding Kubernetes validation: https://github.com/kubernetes/apimachinery/blob/02cfb53916346d085a6c6c7c66f882e3c6b0eca6/pkg/util/validation/validation.go#L208</p> <p>Valid values include:</p> <ul> <li>\"\" - empty string implies core Kubernetes API group</li> <li>\"gateway.networking.k8s.io\"</li> <li>\"foo.example.com\"</li> </ul> <p>Invalid values include:</p> <ul> <li>\"example.com/bar\" - \"/\" is an invalid character</li> </ul> <p>Validation: - MaxLength: 253 - Pattern: <code>^$|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code></p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#inferencemodel","title":"InferenceModel","text":"<p>InferenceModel is the Schema for the InferenceModels API.</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha2</code> <code>kind</code> string <code>InferenceModel</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferenceModelSpec <code>status</code> InferenceModelStatus"},{"location":"reference/spec/#inferencemodelspec","title":"InferenceModelSpec","text":"<p>InferenceModelSpec represents the desired state of a specific model use case. This resource is managed by the \"Inference Workload Owner\" persona.</p> <p>The Inference Workload Owner persona is someone that trains, verifies, and leverages a large language model from a model frontend, drives the lifecycle and rollout of new versions of those models, and defines the specific performance and latency goals for the model. These workloads are expected to operate within an InferencePool sharing compute capacity with other InferenceModels, defined by the Inference Platform Admin.</p> <p>InferenceModel's modelName (not the ObjectMeta name) is unique for a given InferencePool, if the name is reused, an error will be shown on the status of a InferenceModel that attempted to reuse. The oldest InferenceModel, based on creation timestamp, will be selected to remain valid. In the event of a race condition, one will be selected at random.</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>modelName</code> string ModelName is the name of the model as it will be set in the \"model\" parameter for an incoming request.ModelNames must be unique for a referencing InferencePool(names can be reused for a different pool in the same cluster).The modelName with the oldest creation timestamp is retained, and the incomingInferenceModel's Ready status is set to false with a corresponding reason.In the rare case of a race condition, one Model will be selected randomly to be considered valid, and the other rejected.Names can be reserved without an underlying model configured in the pool.This can be done by specifying a target model and setting the weight to zero,an error will be returned specifying that no valid target model is found. MaxLength: 256 Required: {}  <code>criticality</code> Criticality Criticality defines how important it is to serve the model compared to other models referencing the same pool.Criticality impacts how traffic is handled in resource constrained situations. It handles this byqueuing or rejecting requests of lower criticality. InferenceModels of an equivalent Criticality willfairly share resources over throughput of tokens. In the future, the metric used to calculate fairness,and the proportionality of fairness will be configurable.Default values for this field will not be set, to allow for future additions of new field that may 'one of' with this field.Any implementations that may consume this field may treat an unset value as the 'Standard' range. Enum: [Critical Standard Sheddable]  <code>targetModels</code> TargetModel array TargetModels allow multiple versions of a model for traffic splitting.If not specified, the target model name is defaulted to the modelName parameter.modelName is often in reference to a LoRA adapter. MaxItems: 10  <code>poolRef</code> PoolObjectReference PoolRef is a reference to the inference pool, the pool must exist in the same namespace. Required: {}"},{"location":"reference/spec/#inferencemodelstatus","title":"InferenceModelStatus","text":"<p>InferenceModelStatus defines the observed state of InferenceModel</p> <p>Appears in: - InferenceModel</p> Field Description Default Validation <code>conditions</code> Condition array Conditions track the state of the InferenceModel.Known condition types are:* \"Accepted\" [map[lastTransitionTime:1970-01-01T00:00:00Z message:Waiting for controller reason:Pending status:Unknown type:Ready]] MaxItems: 8"},{"location":"reference/spec/#inferencepool","title":"InferencePool","text":"<p>InferencePool is the Schema for the InferencePools API.</p> Field Description Default Validation <code>apiVersion</code> string <code>inference.networking.x-k8s.io/v1alpha2</code> <code>kind</code> string <code>InferencePool</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> InferencePoolSpec <code>status</code> InferencePoolStatus"},{"location":"reference/spec/#inferencepoolspec","title":"InferencePoolSpec","text":"<p>InferencePoolSpec defines the desired state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>selector</code> object (keys:LabelKey, values:LabelValue) Selector defines a map of labels to watch model server podsthat should be included in the InferencePool.In some cases, implementations may translate this field to a Service selector, so this matches the simplemap used for Service selectors instead of the full Kubernetes LabelSelector type.If sepecified, it will be applied to match the model server pods in the same namespace as the InferencePool.Cross namesoace selector is not supported. Required: {}  <code>targetPortNumber</code> integer TargetPortNumber defines the port number to access the selected model servers.The number must be in the range 1 to 65535. Maximum: 65535 Minimum: 1 Required: {}  <code>extensionRef</code> Extension Extension configures an endpoint picker as an extension service. Required: {}"},{"location":"reference/spec/#inferencepoolstatus","title":"InferencePoolStatus","text":"<p>InferencePoolStatus defines the observed state of InferencePool</p> <p>Appears in: - InferencePool</p> Field Description Default Validation <code>parent</code> PoolStatus array Parents is a list of parent resources (usually Gateways) that areassociated with the route, and the status of the InferencePool with respect toeach parent.A maximum of 32 Gateways will be represented in this list. An empty listmeans the route has not been attached to any Gateway. MaxItems: 32"},{"location":"reference/spec/#kind","title":"Kind","text":"<p>Underlying type: string</p> <p>Kind refers to a Kubernetes Kind.</p> <p>Valid values include:</p> <ul> <li>\"Service\"</li> <li>\"HTTPRoute\"</li> </ul> <p>Invalid values include:</p> <ul> <li>\"invalid/kind\" - \"/\" is an invalid character</li> </ul> <p>Validation: - MaxLength: 63 - MinLength: 1 - Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code></p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#labelkey","title":"LabelKey","text":"<p>Underlying type: string</p> <p>LabelKey was originally copied from: https://github.com/kubernetes-sigs/gateway-api/blob/99a3934c6bc1ce0874f3a4c5f20cafd8977ffcb4/apis/v1/shared_types.go#L694-L731 Duplicated as to not take an unexpected dependency on gw's API.</p> <p>LabelKey is the key of a label. This is used for validation of maps. This matches the Kubernetes \"qualified name\" validation that is used for labels. Labels are case sensitive, so: my-label and My-Label are considered distinct.</p> <p>Valid values include:</p> <ul> <li>example</li> <li>example.com</li> <li>example.com/path</li> <li>example.com/path.html</li> </ul> <p>Invalid values include:</p> <ul> <li>example~ - \"~\" is an invalid character</li> <li>example.com. - can not start or end with \".\"</li> </ul> <p>Validation: - MaxLength: 253 - MinLength: 1 - Pattern: <code>^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?([A-Za-z0-9][-A-Za-z0-9_.]{0,61})?[A-Za-z0-9]$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#labelvalue","title":"LabelValue","text":"<p>Underlying type: string</p> <p>LabelValue is the value of a label. This is used for validation of maps. This matches the Kubernetes label validation rules: * must be 63 characters or less (can be empty), * unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]), * could contain dashes (-), underscores (_), dots (.), and alphanumerics between.</p> <p>Valid values include:</p> <ul> <li>MyValue</li> <li>my.name</li> <li>123-my-value</li> </ul> <p>Validation: - MaxLength: 63 - MinLength: 0 - Pattern: <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$</code></p> <p>Appears in: - InferencePoolSpec</p>"},{"location":"reference/spec/#objectname","title":"ObjectName","text":"<p>Underlying type: string</p> <p>ObjectName refers to the name of a Kubernetes object. Object names can have a variety of forms, including RFC 1123 subdomains, RFC 1123 labels, or RFC 1035 labels.</p> <p>Validation: - MaxLength: 253 - MinLength: 1</p> <p>Appears in: - Extension - ExtensionReference - PoolObjectReference</p>"},{"location":"reference/spec/#poolobjectreference","title":"PoolObjectReference","text":"<p>PoolObjectReference identifies an API object within the namespace of the referrer.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>group</code> Group Group is the group of the referent. inference.networking.x-k8s.io MaxLength: 253 Pattern: <code>^$\\|^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$</code> <code>kind</code> Kind Kind is kind of the referent. For example \"InferencePool\". InferencePool MaxLength: 63 MinLength: 1 Pattern: <code>^[a-zA-Z]([-a-zA-Z0-9]*[a-zA-Z0-9])?$</code> <code>name</code> ObjectName Name is the name of the referent. MaxLength: 253 MinLength: 1 Required: {}"},{"location":"reference/spec/#poolstatus","title":"PoolStatus","text":"<p>PoolStatus defines the observed state of InferencePool from a Gateway.</p> <p>Appears in: - InferencePoolStatus</p> Field Description Default Validation <code>parentRef</code> ObjectReference GatewayRef indicates the gateway that observed state of InferencePool. <code>conditions</code> Condition array Conditions track the state of the InferencePool.Known condition types are: \"Accepted\" \"ResolvedRefs\" [map[lastTransitionTime:1970-01-01T00:00:00Z message:Waiting for controller reason:Pending status:Unknown type:Accepted]] MaxItems: 8"},{"location":"reference/spec/#portnumber","title":"PortNumber","text":"<p>Underlying type: integer</p> <p>PortNumber defines a network port.</p> <p>Validation: - Maximum: 65535 - Minimum: 1</p> <p>Appears in: - Extension - ExtensionReference</p>"},{"location":"reference/spec/#targetmodel","title":"TargetModel","text":"<p>TargetModel represents a deployed model or a LoRA adapter. The Name field is expected to match the name of the LoRA adapter (or base model) as it is registered within the model server. Inference Gateway assumes that the model exists on the model server and it's the responsibility of the user to validate a correct match. Should a model fail to exist at request time, the error is processed by the Inference Gateway and emitted on the appropriate InferenceModel object.</p> <p>Appears in: - InferenceModelSpec</p> Field Description Default Validation <code>name</code> string Name is the name of the adapter or base model, as expected by the ModelServer. MaxLength: 253 Required: {}  <code>weight</code> integer Weight is used to determine the proportion of traffic that should besent to this model when multiple target models are specified.Weight defines the proportion of requests forwarded to the specifiedmodel. This is computed as weight/(sum of all weights in thisTargetModels list). For non-zero values, there may be some epsilon fromthe exact proportion defined here depending on the precision animplementation supports. Weight is not a percentage and the sum ofweights does not need to equal 100.If a weight is set for any targetModel, it must be set for all targetModels.Conversely weights are optional, so long as ALL targetModels do not specify a weight. Maximum: 1e+06 Minimum: 1"}]}